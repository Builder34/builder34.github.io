{"meta":{"title":"抹布先生M","subtitle":"记录技术与生活,keep coding","description":"Builder34的个人博客...","author":"Builder Luo","url":"https://blog.monbuilder.top"},"pages":[{"title":"标签","date":"2018-12-16T10:26:27.591Z","updated":"2018-12-16T10:26:27.591Z","comments":false,"path":"index.html","permalink":"https://blog.monbuilder.top/index.html","excerpt":"","text":""},{"title":"关于","date":"2018-12-16T10:26:27.589Z","updated":"2018-12-16T10:26:27.588Z","comments":false,"path":"about/index.html","permalink":"https://blog.monbuilder.top/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2018-12-16T10:26:27.588Z","updated":"2018-12-16T10:26:27.588Z","comments":false,"path":"categories/index.html","permalink":"https://blog.monbuilder.top/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2018-12-16T10:26:27.589Z","updated":"2018-12-16T10:26:27.589Z","comments":false,"path":"books/index.html","permalink":"https://blog.monbuilder.top/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-01-10T14:44:50.009Z","updated":"2018-12-16T10:26:27.590Z","comments":true,"path":"links/index.html","permalink":"https://blog.monbuilder.top/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-12-16T10:26:27.590Z","updated":"2018-12-16T10:26:27.590Z","comments":false,"path":"repository/index.html","permalink":"https://blog.monbuilder.top/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-12-16T10:28:20.231Z","updated":"2018-12-16T10:28:20.231Z","comments":false,"path":"tags/index.html","permalink":"https://blog.monbuilder.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Elasticsearch学习教程系列(1)-命令学习(一) 集群健康、索引、文档操作","slug":"elasticsearch-learn01","date":"2019-03-14T04:15:22.000Z","updated":"2019-03-14T03:07:38.915Z","comments":true,"path":"2019/03/14/elasticsearch-learn01/","link":"","permalink":"https://blog.monbuilder.top/2019/03/14/elasticsearch-learn01/","excerpt":"","text":"本教程是基于Elasticsearch6.5版本编写在本系列教程上一篇文章中，我们介绍了Elasticsearch的一些基本概念、安装并运行起了一个Elasticsearch节点。现在我们已经启动并运行了节点（集群），下一步是了解如何与它进行通信。幸运的是，Elasticsearch提供了一个非常全面和强大的REST API，您可以使用它与集群进行交互。使用API 可以完成的一些事项如下： 检查集群运行情况，状态和统计信息 管理您的群集，节点和索引数据和元数据 对索引执行CRUD（创建，读取，更新和删除）和搜索操作 执行高级搜索操作，例如分页，排序，过滤，脚本编写，聚合等等 集群运行情况(Cluster Health)让我们从基本运行状况检查开始，我们可以使用它来查看集群的运行情况。我们将使用curl来执行此操作，但您可以使用任何允许您进行HTTP / REST调用的工具。假设我们仍然在我们启动Elasticsearch的同一节点上打开另一个命令shell窗口。‘’[builder@master ~]$ curl -X GET “http://localhost:9200/_cat/health?v&quot;‘’ epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent‘’ 1547394013 15:40:13 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0%‘’我们可以看到名为“elasticsearch”的群集处于green(绿色)状态。每当我们要求群集健康时，我们要么获得绿色，黄色或红色。 绿色 - 一切都很好（集群功能齐全）黄色 - 所有数据均可用，但尚未分配一些副本（群集功能齐全）红色 - 某些数据由于某种原因不可用（群集部分功能）注意：当群集为红色时，它将继续提供来自可用分片的搜索请求，但您可能需要尽快修复它，因为存在未分配的分片。同样从上面的响应中，我们可以看到总共1个节点，并且我们有0个分片，因为我们还没有数据。请注意，由于我们使用的是默认群集名称（elasticsearch），并且由于Elasticsearch默认使用单播网络发现来查找同一台计算机上的其他节点，因此您可能会意外启动计算机上的多个节点并将它们所有都加入一个集群。在这种情况下，您可能会在上面的响应中看到多个节点。查看集群中的节点列表：‘’[builder@master ]$ curl -X GET “http://localhost:9200/_cat/nodes?v&quot;‘’ ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name‘’ 127.0.0.1 20 98 8 1.90 mdi * siwsSwZ从上面的结果中，我们可以看到一个名为“siwsSwZ”的节点，它是我们集群中当前的单个节点。 索引(indices)的CRUD、查询操作查看索引列表： 12[builder@master ~]$ curl -X GET \"http://localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 因为我们的节点是刚刚建立的，还没数据，所以上面查询的结果只有一行字段名，没有索引数据 创建索引现在让我们创建一个名为“customer”的索引，然后再次列出所有索引（下面第一个命令使用PUT动词创建名为“customer”的索引。加?pretty表示返回的结果打印格式化过的JSON（如果有的话）。）： 123456789[builder@master ~]$ curl -X PUT \"localhost:9200/customer?pretty\" &#123; \"acknowledged\" : true, \"shards_acknowledged\" : true, \"index\" : \"customer\" &#125;[builder@master ~]$ curl -X GET \"localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open customer i7R-XRH8R0Kn7uLQRNC_yQ 5 1 0 0 1.1kb 1.1kb 第二个命令的结果告诉我们，我们现在有一个名为customer的索引，它有5个主分片和1个副本（默认值），并且它包含0个文档。您可能还注意到客户索引标记了黄色运行状况。回想一下我们之前的讨论，黄色表示某些副本尚未（尚未）分配。此索引发生这种情况的原因是因为默认情况下Elasticsearch为此索引创建了一个副本。由于我们目前只有一个节点在运行，因此在另一个节点加入集群的较晚时间点之前，尚无法分配一个副本（用于高可用性）。将该副本分配到第二个节点后，此索引的运行状况将变为绿色。 Tips: 上面命令中，-X PUT 后的路径不要带http哟，否则会出现如下错误的： 12345[builder@master ~]$ curl -X PUT \"http://localhost:9200/_cat/customer1?pretty\" &#123; \"error\" : \"Incorrect HTTP method for uri [/_cat/customer1?pretty] and method [PUT], allowed: [POST]\", \"status\" : 405 &#125; 查询文档现在让我们在customer索引中加入一些内容。我们将一个简单的客户文档索引到客户索引中，ID为1，如下所示： 123456789101112131415[builder@master ~]$ curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Builder Luo\"&#125;' &#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 0, \"_primary_term\" : 2 &#125; curl命令说明：-X为请求方式，-H 参数为设置请求头，-d参数为请求参数 。‘’ Tips: 关于：路径localhost:9200/customer/_doc/1?pretty‘’ localhost:9200/customer是之前创建的索引，后接/_doc/1表示为在customer索引上创建1个ID为1的文档(?pretty表示将返回的响应json格式化美观)提示：值得注意的是，Elasticsearch在你将文档编入索引之前不需要先显式创建索引。在前面的示例中，如果customer索引事先尚未存在，则Elasticsearch将自动创建customer索引。查看刚刚创建好的文档: 1234567891011[builder@master ~]$ curl -X GET \"localhost:9200/customer/_doc/1?pretty\"&#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"_source\" : &#123; \"name\" : \"Builder Luo\" &#125;&#125; 删除索引123456[builder@master ~]$ curl -X DELETE \"localhost:9200/customer?pretty\"&#123; \"acknowledged\" : true&#125;[builder@master ~]$ curl -X GET \"localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 上面意味着索引已成功删除，我们现在回到我们在集群中没有任何内容的地方。在我们继续之前，让我们再仔细看看到目前为止我们学到的一些API命令： 1234567curl -X PUT ''localhost:9200/customer''curl -X GET ''localhost:9200/customer''curl -X DELETE ''localhost:9200/customer''curl -X PUT ''localhost:9200/customer/_doc/1'' -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Builder Luo\"&#125;'curl -X GET ''localhost:9200/customer/_doc/1''curl -X DELETE ''localhost:9200/customer/_doc/1'' 如果我们仔细研究上述命令，我们实际上可以看到我们如何在Elasticsearch中访问数据的模式。该模式可归纳如下： 1&lt;HTTP Verb&gt; Node_address/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt; 这种REST访问模式在所有API命令中都非常普遍，如果你能记住它，你将在掌握Elasticsearch方面有一个良好的开端。 替换文档Elasticsearch几乎实时提供数据操作和搜索功能。默认情况下，从索引/更新/删除数据到搜索结果中显示的时间，您可能会有一秒钟的延迟（刷新间隔）。数据在事务完成后立即可用，这是Elasticsearch与SQL等其他平台的重要区别之前，我们执行过如下命令： 1234567891011121314151617''[builder@master ~]$ curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Builder Luo\"&#125;' 下面，我们执行一样的命令，文档ID还是指定为1，只是-d参数内容有变化：''[builder@master ~]$ curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Baby Mon\"&#125;' ''&#123;'' \"_index\" : \"customer\",'' \"_type\" : \"_doc\",'' \"_id\" : \"1\",'' \"_version\" : 2,'' \"result\" : \"updated\",'' \"_shards\" : &#123;'' \"total\" : 2,'' \"successful\" : 1,'' \"failed\" : 0'' &#125;,'' \"_seq_no\" : 1,'' \"_primary_term\" : 1'' &#125; 以上内容将ID为1的文档名称从“Builder Luo”更改为“Baby Mon”, result字段值为: updated。另一方面，如果我们使用不同的ID，则会对新文档编制索引，并且索引中已有的现有文档保持不变。索引时，ID部分是可选的。如果未指定，Elasticsearch将生成随机ID，然后使用它来索引文档。Elasticsearch生成的实际ID（或前面示例中显式指定的内容）将作为索引API调用的一部分返回。此示例显示如何在没有显式ID的情况下索引文档： 123456789101112131415[builder@master ~]$ curl -X POST \"localhost:9200/customer/_doc?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Moonlight Chen\"&#125;' &#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"UmGaVGgBol3Y-BIhbtd0\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 0, \"_primary_term\" : 1 &#125; 可以看到上面自动生成的ID是：UmGaVGgBol3Y-BIhbtd0请注意，在上面的情况中，我们使用POST动词而不是PUT，因为我们没有指定ID。 修改文档数据除了能够重新索引和替换文档，我们还可以更新文档数据。（请注意，Elasticsearch实际上并没有在内部进行就地更新。每当我们进行更新时，Elasticsearch都会删除旧文档，然后一次性对应用了更新的新文档编制索引。） 1234567891011121314151617181920212223242526272829303132333435# -d参数，需要封装成： &#123; \"doc\": &#123;dataJSON&#125;&#125;[builder@master ~]$ curl -X POST \"localhost:9200/customer/_doc/1/_update?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"doc\": &#123; \"name\": \"My Baby Mon\", \"age\": 20&#125; &#125;' &#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 4, \"result\" : \"updated\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 3, \"_primary_term\" : 1&#125;# 也可以使用简单脚本执行更新。此示例使用脚本将年龄增加5：[builder@master ~]$ curl -X POST \"localhost:9200/customer/_doc/1/_update?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"script\": \"ctx._source.age += 5\" &#125;'&#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 5, \"result\" : \"updated\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 4, \"_primary_term\" : 1 &#125;# 在上面的示例中，ctx._source指的是即将更新的当前源文档。 Elasticsearch提供了在给定查询条件（如SQL UPDATE-WHERE语句）的情况下更新多个文档的功能。请参阅[docs-update-by-queryAPI[https://www.elastic.co/guide/en/elasticsearch/reference/6.5/docs-update-by-query.html]]本文到这里，我们主要介绍了Elasticsearch的索引、文档的一些主要命令，我们将在下一篇文章中介绍Elasticsearch的批处理命令、以及探索操作数据的命令","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://blog.monbuilder.top/tags/Elasticsearch/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}]},{"title":"搭建大数据平台系列(4)-hive环境搭建","slug":"hive","date":"2019-03-06T14:19:33.000Z","updated":"2019-03-06T14:53:01.208Z","comments":true,"path":"2019/03/06/hive/","link":"","permalink":"https://blog.monbuilder.top/2019/03/06/hive/","excerpt":"","text":"0.准备步骤Hive 是依赖在Hadoop上的，所以他的安装不需要像Hadoop或者spark那样每个节点都安装一遍，只需在Hadoop的master节点上安装一个即可。Hive的安装前，需要Hadoop的环境，以及Mysql。 1.安装过程###1.1下载并解压安装包 1234567891011121314#下载hive-1.1.0-cdh5.5.0.tar.gz到master机器的~/bigdataspacce文件夹下#解压安装包的命令：[hadoop@master ~]$ cd ~/bigdataspacce[hadoop@master bigdataspace]$ tar -zxvf hive-1.1.0-cdh5.5.0.tar.gz#解压完成后删除压缩包：[hadoop@master bigdataspace]$ rm hive-1.1.0-cdh5.5.0.tar.gz#配置HIVE_HOME环境变量[hadoop@master ~]$ sudo vi /etc/profile(添加配置内容如下,红色为需要新增的配置)export HIVE_HOME=/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin:$PATH#让环境变量生效[hadoop@master ~]$ source /etc/profile ###1.2修改hive-env.sh配置文件 123456[hadoop@master ~]$ cd /home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf[hadoop@master conf]$ cp hive-env.sh.template hive-env.sh[hadoop@master conf]$ vi hive-env.sh#在hive-env.sh配置文件末尾加上:export HADOOP_HOME=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0export HIVE_CONF_DIR=/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf ###1.3新建hive-site.xml配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[hadoop@master conf]$ vi hive-env.sh##主要的配置内容如下：&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/hive-db/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/tmp/hive-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/tmp/$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/downloaded&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/queryLogs/$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://slave1:3306/hive?useUnicode=true&amp;amp;characterEncoding=utf8&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 1.4添加mysql-connector的jar包到hive安装路径下的lib文件夹12#$HIVE_HOME为前面hive安装的目录路径:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0[hadoop@master ~] mv mysql-connector-java-5.1.33.jar $HIVE_HOME/lib 1.5启动元数据服务12[hadoop@master ~]$ cd ~/bigdataspace/hive-1.1.0-cdh5.5.0[hadoop@master hive-1.1.0-cdh5.5.0]$ ./bin/hive --service metastore &amp; 1.6启动/停止hive (CTL)命令行123456#因为一开始配置了HIVE_HOME环境变量，可以直接在任何目录下执行hive命令了,进入hive控制台[hadoop@master bigdataspace]$ hive Logging initialized using configuration in jar:file:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/lib/hive-common-1.1.0-cdh5.5.0.jar!/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive (default)&gt; 上面报错了，解决Logging initialized using configuration in jar:file… （因为没log配置文件，直接从jar包查找） 1234567891011$ cd ~/bigdataspace/ /hive-1.1.0-cdh5.5.0/conf$ cp beeline-log4j.properties.template beeline-log4j.properties$ cp hive-log4j.properties.template hive-log4j.properties$ cp hive-exec-log4j.properties.template hive-exec-log4j.properties[hadoop@master bigdataspace]$ hiveLogging initialized using configuration in file:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive (default)&gt;hive&gt; quit; #(退出hive，使用exit也可以) 1.7启动/停止beeline命令行（CTL）1234#启动：[hadoop@master bigdataspace]$ beeline#停止：beeline&gt; !q 1.8HiveServer2的使用1234567891011121314151617181920212223242526[hadoop@master ~]$ cd ~/bigdataspace/hive-1.1.0-cdh5.5.0/bin/[hadoop@master bin]$ ./hiveserver2 &amp; #后面的&amp;表示改命名在系统后台执行(如果执行上面命令让界面无法回到命令行，可以按ctrl+C回到命令行，这里&amp;会让hiverserver2在后台继续执行)#查看HiveServer2的进程情况(如果无则hiverserver2启动失败或停止了):[hadoop@master bin]$ ps -ef |grep HiveServer2 hadoop 25545 14762 3 17:02 pts/1 00:00:21 /home/hadoop/bigdataspace/jdk1.8.0_60/bin/java -Xmx256m -Djava.library.path=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0/lib/native/ -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx512m -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/lib/hive-service-1.1.0-cdh5.5.0.jar org.apache.hive.service.server.HiveServer2hadoop 26038 14762 0 17:14 pts/1 00:00:00 grep HiveServer2(“kill -9 PID” 可以通过kill停止hiveserver2的后台服务) 使用beeline连接hiveserver2测试：(jdbc:hive2：表示连接到hiveserver2master:表示hiveserver2安装的机器host/IP10001:表示hiveserver2设置的端口号(hive-site.xml中可设置))[hadoop@master hive-1.1.0-cdh5.5.0]$ beeline -u jdbc:hive2://master:10001###这里可能会出现一些slf4j包有多个，引用异常，但是不是报错，如：SLF4J: Class path contains multiple SLF4J bindingsSLF4J: Found binding in [jar:file:/home/hadoop/bigdataspace/had…)Connecting to jdbc:hive2://master:10001Connected to: Apache Hive (version 1.1.0-cdh5.5.0)Driver: Hive JDBC (version 1.1.0-cdh5.5.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.5.0 by Apache Hive0: jdbc:hive2://master:10001&gt; 以上完成了Hive的基本安装配置。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"Elasticsearch学习教程系列(0)-入门与安装","slug":"elasticsearch-learn00","date":"2019-01-10T14:34:05.000Z","updated":"2019-03-06T14:23:56.929Z","comments":true,"path":"2019/01/10/elasticsearch-learn00/","link":"","permalink":"https://blog.monbuilder.top/2019/01/10/elasticsearch-learn00/","excerpt":"","text":"本教程是基于Elasticsearch6.5版本编写下面将介绍、安装并启动Elasticsearch，查看其中的内容以及执行索引，搜索和修改数据等基本操作的过程。在本教程结束时，您应该很好地了解Elasticsearch是什么，它是如何工作的，并希望能够获得灵感，看看如何使用它来构建复杂的搜索应用程序或从数据中挖掘智能。 0.1入门简介Elasticsearch是一个高度可扩展的开源全文搜索和分析引擎。Elasticsearch基于Apache Lucene，它允许您快速，近实时地存储，搜索和分析大量数据。它通常用作与底层引擎/技术，提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口；为具有复杂搜索功能和要求的应用程序提供支持。以下是Elasticsearch可用于的一些场景示例： 您运行在线网上商店，允许您的客户搜索您销售的产品。在这种情况下，您可以使用Elasticsearch存储整个产品目录和库存，并为它们提供搜索和自动填充建议。 您希望收集日志或交易数据，并且希望分析和挖掘此数据以查找趋势，统计信息，摘要或异常。在这种情况下，您可以使用Logstash（Elasticsearch / Logstash / Kibana堆栈的一部分）来收集，聚合和解析数据，然后让Logstash将此数据提供给Elasticsearch。一旦数据在Elasticsearch中，您就可以运行搜索和聚合来挖掘您感兴趣的任何信息。 您运行价格警报平台，允许精通价格的客户指定一条规则，例如“我有兴趣购买特定的电子产品，如果小工具的价格在下个月内从任何供应商降至X美元以下，我希望收到通知” 。在这种情况下，您可以刮取供应商价格，将其推入Elasticsearch并使用其反向搜索（Percolator）功能来匹配价格变动与客户查询，并最终在发现匹配后将警报推送给客户。 您有分析/业务智能需求，并希望快速调查，分析，可视化并询问有关大量数据的特定问题（想想数百万或数十亿条记录）。在这种情况下，您可以使用Elasticsearch存储数据，然后使用Kibana（Elasticsearch / Logstash / Kibana堆栈的一部分）构建自定义仪表板，以便可视化对您来说重要的数据方面。此外，您可以使用Elasticsearch聚合功能针对您的数据执行复杂的商业智能查询。0.2基本概念有一些概念是Elasticsearch的核心。从一开始就理解这些概念将极大地帮助简化学习过程。1.近实时(NRT)Elasticsearch是一个近实时搜索平台。这意味着从索引文档到可搜索文档的时间有一点延迟（通常是一秒）。2.集群(cluster)集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。确保不要在不同的环境中重用相同的群集名称，否则最终会导致节点加入错误的群集。例如，您可以使用logging-dev，logging-stage以及logging-prod 用于开发，登台和生产集群。请注意，如果群集中只有一个节点，那么它是完全正常的。此外，您还可以拥有多个独立的集群，每个集群都有自己唯一的集群名称。3.节点(node)节点是作为群集一部分的单个服务器，存储数据并参与群集的索引和搜索功能。就像集群一样，节点由名称标识，默认情况下，该名称是在启动时分配给节点的随机通用唯一标识符（UUID）。如果不需要默认值，可以定义所需的任何节点名称。此名称对于管理目的非常重要，您可以在其中识别网络中哪些服务器与Elasticsearch集群中的哪些节点相对应。可以将节点配置为按群集名称加入特定群集。默认情况下，每个节点都设置为加入一个名为cluster的集群elasticsearch，这意味着如果您在网络上启动了许多节点并且假设它们可以相互发现 - 它们将自动形成并加入一个名为的集群elasticsearch。在单个群集中，您可以拥有任意数量的节点。此外，如果您的网络上当前没有其他Elasticsearch节点正在运行，则默认情况下启动单个节点将形成一个名为的新单节点集群elasticsearch。4.索引(index)索引是具有某些类似特征的文档集合。例如，您可以拥有客户数据的索引，产品目录的另一个索引以及订单数据的另一个索引。索引由名称标识（必须全部小写），此名称用于在对其中的文档执行索引，搜索，更新和删除操作时引用索引。在单个群集中，您可以根据需要定义任意数量的索引。5.文档(document)文档是可以编制索引的基本信息单元。例如，您可以为单个客户提供文档，为单个产品提供另一个文档，为单个订单提供另一个文档。该文档以JSON（JavaScript Object Notation）表示，JSON是一种普遍存在的互联网数据交换格式。在索引/类型中，您可以根据需要存储任意数量的文档。请注意，尽管文档实际上驻留在索引中，但实际上必须将文档编入索引/分配给索引中的类型。6.分片和副本(Shards &amp; Replicas)索引可能存储大量可能超过单个节点的硬件限制的数据。例如，占用1TB磁盘空间的十亿个文档的单个索引可能不适合单个节点的磁盘，或者可能太慢而无法单独从单个节点提供搜索请求。为了解决这个问题，Elasticsearch提供了将索引细分为多个称为分片的功能。创建索引时，只需定义所需的分片数即可。每个分片本身都是一个功能齐全且独立的“索引”，可以托管在集群中的任何节点上。分片很重要，主要有两个原因： 它允许您水平拆分/缩放内容量 它允许您跨分片（可能在多个节点上）分布和并行化操作，从而提高性能/吞吐量分片的分布方式以及如何将其文档聚合回搜索请求的机制完全由Elasticsearch管理，对用户而言是透明的。在可以随时发生故障的网络/云环境中，非常有用，强烈建议使用故障转移机制，以防分片/节点以某种方式脱机或因任何原因消失。为此，Elasticsearch允许您将索引的分片的一个或多个副本制作成所谓的副本分片或简称副本。复制很重要，主要有两个原因： 它在碎片/节点发生故障时提供高可用性。因此，请务必注意，副本分片永远不会在与从中复制的原始/主分片相同的节点上分配。 它允许您扩展搜索量/吞吐量，因为可以在所有副本上并行执行搜索。总而言之，每个索引可以拆分为多个分片。索引也可以复制为零（表示没有副本）或更多次。复制后，每个索引都将具有主分片（从中复制的原始分片）和副本分片（主分片的副本）。可以在创建索引时为每个索引定义分片和副本的数量。创建索引后，您还可以随时动态更改副本数。您可以使用_shrink和_splitAPI 更改现有索引的分片数，但这不是一项简单的任务，并且预先计划正确数量的分片是最佳方法。默认情况下，Elasticsearch中的每个索引都分配了5个主分片和1个副本，这意味着如果群集中至少有两个节点，则索引将包含5个主分片和另外5个副本分片（1个完整副本），总计为每个索引10个分片。Tips: 每个Elasticsearch分片都是Lucene索引。单个Lucene索引中可以包含最大数量的文档。截止LUCENE-5843，限制是2,147,483,519（= Integer.MAX_VALUE - 128）文档。您可以使用_cat/shardsAPI 监控分片大小。0.3安装Elasticsearch是基于Java开发的，因此安装时，需要JDK环境。我们安装的版本是Elasticsearch6.5.4，至少需要JDK 8。我们假设您已经在您的Linux(例如CentOS6.5)环境下已经安装了JDK。 12345678910111213141516171819 [builder@master ~]$ java -version java version \"1.8.0_171\" Java(TM) SE Runtime Environment (build 1.8.0_171-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode) #下载安装包，并解压： [builder@master ~]$ cd ~ [builder@master ~]$ mdkir env [builder@master ~]$ cd env [builder@master ~]$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.5.4.tar.gz#等待下载完成后，解压 [builder@master ~]$ tar -zxvf elasticsearch-6.5.4.tar.gz [builder@master ~]$ cd elasticsearch-6.5.4 # 在elasticsearch命令后加 -d 参数，是指以守护线程启动程序，即是后台运行 [builder@master ~]$ ./bin/elasticsearch -d 到这里Elasticsearch的安装已经完成了，是不是非常简单呢？另外，我们启动的时候是可以通过 -E 参数指定集群名称(cluster.name)或者节点名称(node.name)的： [builder@master ~]$ ./bin/elasticsearch -Ecluster.name=esCluster -Enode.name=builder01 -d Tips: 默认情况下，Elasticsearch使用port 9200来提供对其REST API的访问。如有必要，可以配置此端口。","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://blog.monbuilder.top/tags/Elasticsearch/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}]},{"title":"搭建大数据平台系列(3)-hbase环境搭建","slug":"hbase","date":"2018-12-13T09:21:59.000Z","updated":"2018-12-14T06:14:20.392Z","comments":true,"path":"2018/12/13/hbase/","link":"","permalink":"https://blog.monbuilder.top/2018/12/13/hbase/","excerpt":"","text":"1.安装步骤Hbase的安装需要在hadoop和zookeeper安装完成的基础上进行安装部署，所以，需要在安装hbase前准备好hadoop和zookeeper的环境（请看本系列前几篇文章）123456789101112131415161718`1.下载hbase-1.0.0-cdh5.5.0.tar.gz到master机器的/bigdataspacce文件夹下2.解压安装包的命令：[hadoop@master ]()$ cd /bigdataspacce[hadoop@master bigdataspace]()$ tar -zxvf hbase-1.0.0-cdh5.5.0.tar.gz3.解压完成后删除压缩包：[hadoop@master bigdataspace]()$ rm hbase-1.0.0-cdh5.5.0.tar.gz4.修改hbase-env.sh、hbase-site.xml配置文件以及regionservers文件（配置dataNode节点）$ cd /home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0/conf$ vi hbase-env.sh# The java implementation to use. Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/(在上面这条注释下加上：)export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60……export HBASE_PID_DIR=/data/hbase-1.0.0-cdh5.5.0/pids# export HBASE_MANAGES_ZK=true #设置hbase是否管理zookeeperexport HBASE_MANAGES_ZK=trueexport HBASE_MANAGES_ZK=false #使用独立的ZooKeeper时需要修改HBASE_MANAGES_ZK值为false，为不使用默认自带的ZooKeeper实例。 12345678910111213141516171819202122232425262728293031`$ vi hbase-site.xml(修改配置文件内容为如下)\\&lt;configuration\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.rootdir\\&lt;/name\\&gt;\\&lt;value\\&gt;hdfs://master:8020/hbase\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.cluster.distributed\\&lt;/name\\&gt;\\&lt;value\\&gt;true\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.zookeeper.quorum\\&lt;/name\\&gt;\\&lt;value\\&gt;slave1,slave2,slave3\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.zookeeper.property.dataDir\\&lt;/name\\&gt;\\&lt;value\\&gt;/data/zookeeper-3.4.5-cdh5.5.0/var/data\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.tmp.dir\\&lt;/name\\&gt;\\&lt;value\\&gt;/data/hbase-1.0.0-cdh5.5.0/tmp\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;/configuration\\&gt;(hdfs://master:8020/hbase,这里的hbase目录未建好的话是需要hdfs dfs –mkdir 新建的目录)$ vi regionserversslave1slave2slave3(以上使用对应的ip配置也可以)``` `5.配置HBASE_HOME$ vi /etc/profile(加上如下配置)export HBASE_HOME=/home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH6.使用scp命令把hbase分发到各个节点$ scp -r hbase-1.0.0-cdh5.5.0/ hadoop@slave1:/bigdataspace/$ scp -r hbase-1.0.0-cdh5.5.0/ hadoop@slave2:/bigdataspace/$ scp -r hbase-1.0.0-cdh5.5.0/ hadoop@slave3:/bigdataspace/ 然后在各个节点上执行第5步：配置HBASE_HOME_1` `7.Hbase的启动与停止启动hbase时要确保hdfs已经启动，HBase的启动顺序为：HDFS->Zookeeper->HBase，启动Hbase的命令如下(在master机器上)：hadoop@master $ cd /home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0/bin(注意，如果设置了hbase管理zookeeper，则需要先关闭手动启动的各节点zookeeper)如slave1机器：hadoop@slave1 $ /bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/zkServer.sh stop在master机器： hadoop@master bin$ ./start-hbase.sh hadoop@master bin$ jps29385 HMaster19994 JobHistoryServer19068 NameNode29757 Jps19422 ResourceManager19263 SecondaryNameNode1` `如slave1机器：hadoop@slave1 bin$ jps12768 DataNode17971 HRegionServer12884 NodeManager22704 QuorumPeerMain18169 Jps17851 HQuorumPeer #hbase管理zookeeper的进程, 如果export HBASE_MANAGES_ZK=true，才会出现上面的进程如果HQuorumPeer不存在，而是QuorumPeerMain则表明需要手动关闭zookeeper，hbase才能接手管理。Hbase停止命令：hadoop@master bin$ ./stop-hbase.sh``## 2.验证启动成功访问HBase web 页面：http://master:60010/","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"搭建大数据平台系列(2)-zookeeper环境搭建","slug":"zookeeper","date":"2018-11-05T08:08:10.000Z","updated":"2018-12-14T06:14:20.400Z","comments":true,"path":"2018/11/05/zookeeper/","link":"","permalink":"https://blog.monbuilder.top/2018/11/05/zookeeper/","excerpt":"","text":"1.安装步骤Zookeeper集群一般配置奇数个，在本次测试机是部署到slave1，slave2，slave3这3台机器上。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541.下载zookeeper-3.4.5-cdh5.5.0.tar.gz到slave1机器的~/bigdataspacce文件夹下2.解压安装包的命令：[hadoop@slave1 bigdataspace]$ tar -zxvf zookeeper-3.4.5-cdh5.5.0.tar.gz3.解压完成后删除压缩包：[hadoop@slave1 bigdataspace]$ rm zookeeper-3.4.5-cdh5.5.0.tar.gz4.配置zoo.cfg文件 $ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/conf $ cp zoo_sample.cfg zoo.cfg $ vi zoo.cfg (修改文件中的dataDir配置) dataDir=/data/zookeeper-3.4.5-cdh5.5.0/var/data dataLogDir=/data/zookeeper-3.4.5-cdh5.5.0/var/dataLog/（并在clientPort下面新增如下配置）server.1=slave1:2888:3888server.2=slave2:2888:3888server.3=slave3:2888:38885.建立dataDir对应路径的文件夹，并进入该data文件夹下新建一个文件myid： $ mkdir -p /data/zookeeper-3.4.5-cdh5.5.0/var/dataLog$ mkdir -p /data/zookeeper-3.4.5-cdh5.5.0/var/data $ cd /data/zookeeper-3.4.5-cdh5.5.0/var/data $ vi myid (myid文件内容为zoo.cfg中配的server号码，如server.1则myid文件中只保存1，每台机器都配自己对应的号码) $ cat myid 1 $6.以上对zookeeper的配置基本完成，下面使用scp把zookeeper发到各个节点：$ scp -r zookeeper-3.4.5-cdh5.5.0/ hadoop@slave2:~/bigdataspace/$ scp -r zookeeper-3.4.5-cdh5.5.0/ hadoop@slave3:~/bigdataspace/7.通过scp把myid传到各个节点，并修改其zoo.cfg配置文件对应的server号码(如server.2=slave1:52888:53888则myid文件存入2) $ scp -r /data/zookeeper-3.4.5-cdh5.5.0/ hadoop@slave2:/data/ $ scp -r /data/zookeeper-3.4.5-cdh5.5.0/ hadoop@slave3:/data/ (然后到到各个节点上修改/data/zookeeper-3.4.5-cdh5.5.0/var/data/myid文件),如： [hadoop@slave2 ~]$ vi /data/zookeeper-3.4.5-cdh5.5.0/var/data/myid 2 [hadoop@slave2 ~]$8.zookeeper.out以及log4j日志文件的设置[hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/conf[hadoop@slave1 conf]$ vi log4j.properties# Define some default values that can be overridden by system propertieszookeeper.root.logger=INFO, ROLLINGFILE……log4j.appender.ROLLINGFILE=org.apache.log4j.DailyRollingFileAppender查看zkServer.sh脚本，发现运行时会先加载zookeeper-3.4.5-cdh5.5.0/libexec/zkEnv.sh，不存在会加载zookeeper-3.4.5-cdh5.5.0/bin/zkEnv.sh[hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/libexec[hadoop@slave1 libexec]$ vi zkEnv.shif [ \"x$&#123;ZOO_LOG_DIR&#125;\" = \"x\" ]then ZOO_LOG_DIR=\"/data/zookeeper-3.4.5-cdh5.5.0/logs\"fiif [ \"x$&#123;ZOO_LOG4J_PROP&#125;\" = \"x\" ]thenZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\"也可以把zookeeper-3.4.5-cdh5.5.0/bin/zkEnv.sh文件的配置修改成上面一样. 1239.启动zookeeper服务： [hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin [hadoop@slave1 bin]$ ./ zkServer.sh start #启动zookeeper（每台机器都要执行此命令） 2.验证1234567 [hadoop@slave1 bin]$ jps #使用jps命令 25906 Jps20536 QuorumPeerMain #zookeeper的进程19994 JobHistoryServer19068 NameNode19422 ResourceManager19263 SecondaryNameNode 如上，含有QuorumPeerMain表明安装成功12345678910111213[hadoop@master bin]$ ./ zkServer.sh stop #停止zookeeper（每台机器都要执行此命令）[hadoop@master bin]$ ./zkServer.sh status #查看角色状态命令JMX enabled by defaultUsing config: /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/../conf/zoo.cfgMode: follower（Mode: follower/leader，leader这个角色只有一台机器，是通过zookeeper的选举算法产生）如果出现如下错误，[hadoop@master bin]$ ./zkServer.sh statusJMX enabled by defaultUsing config: /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/../conf/zoo.cfgError contacting service. It is probably not running.极大可能是因为防火墙端口被限制了，可以打开这些被用到的端口(注意：只启用一台zookeeper也是会 出现这个错误，需要启动2台以上的节点) 12#进入zookeeper客户端的命令[hadoop@master bin]$ bin/zkCli.sh -server master:2181","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"rabbitMQ的介绍与安装使用","slug":"rabbitmq","date":"2018-10-24T01:47:37.000Z","updated":"2018-12-14T06:14:20.387Z","comments":true,"path":"2018/10/24/rabbitmq/","link":"","permalink":"https://blog.monbuilder.top/2018/10/24/rabbitmq/","excerpt":"","text":"0. 消息中间件的介绍RabbitMQ是一个“传统”消息代理，可以实现各种消息传递协议。它是首批实现合理级别功能，客户端库，开发工具和质量文档的开源消息代理之一。RabbitMQ最初是为实现AMQP而开发的，AMQP是一种开放式线路协议，具有强大的路由功能。虽然Java具有像JMS这样的消息传递标准，但它对于需要分布式消息传递的非Java应用程序没有帮助，因为它严重限制了任何集成场景，微服务或单片机。随着AMQP的出现，跨语言的灵活性成为开源消息代理的真实存在。RabbitMQ被设计为通用消息代理，采用点对点，请求/回复和pub-sub通信样式模式的多种变体。它使用智能代理/哑消费者模型，专注于向消费者提供一致的消息传递，消费者的消费速度与经纪人跟踪消费者状态的速度大致相似。它是成熟的，在正确配置时表现良好，得到很好的支持（客户端库Java，.NET，node.js，Ruby，PHP和更多语言），并且有许多可用的插件可以将它扩展到更多的用例和集成场景。 1.kafka,rabbitMQ的对比RabbitMQ中的通信可以根据需要同步或异步。发布者向交换发送消息，消费者从队列中检索消息。通过交换将生产者与队列分离可确保生产者不会受到硬编码路由决策的影响。RabbitMQ还提供了许多分布式部署方案（并且确实要求所有节点都能够解析主机名）。可以将多节点群集设置为群集联合，并且不依赖于外部服务（但某些群集形成插件可以使用AWS API，DNS，Consul等）。Apache Kafka专为高容量发布 - 订阅消息和流而设计，旨在持久，快速和可扩展。从本质上讲，Kafka提供了一个持久的消息存储，类似于日志，在服务器集群中运行，它存储称为主题的类别中的记录流。 2.rabbitMQ的安装与使用2.1在基于RPM的Linux上安装（RHEL，CentOS，Fedora，openSUSE），RabbitMQ RPM包需要sudo权限(或使用root用户)才能安装和管理。123456789101112131415161718192021222324252627282930313233343536373839####1.安装Erlang[root@slave1 ~]# yum install erlang[root@slave1 ~]# yum update erlang####2.安装RabbitMQ服务器 ##下载rabbitMQ rpm安装包[root@slave1 ~]# wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.8/rabbitmq-server-3.7.8-1.el6.noarch.rpm[root@slave1 ~]# yum install rabbitmq-server-3.7.8-1.el6.noarch.rpm ## 开机启动rabbitmq-server[root@slave1 ~]# chkconfig rabbitmq-server on #启动rabbitMQ服务[root@slave1 ~]# service rabbitmq-server start #停止rabbitMQ服务[root@slave1 ~]# service rabbitmq-server stop #重启rabbitMQ服务[root@slave1 ~]# service rabbitmq-server restart #修改配置文件[root@slave1 ~]# cd /etc/rabbitmq/[root@slave1 rabbitmq]# cp /usr/share/doc/rabbitmq-server-3.7.8/rabbitmq.config.example ./[root@slave1 rabbitmq]# mv rabbitmq.config.example rabbitmq.config[root@slave1 rabbitmq]# vim rabbitmq.config[&#123;rabbit, [&#123;tcp_listeners, [5672]&#125;, &#123;loopback_users, [\"test\"]&#125;]&#125;]. #开启Web管理插件，这样我们就可以通过浏览器来进行管理了[root@slave1 ~]# rabbitmq-plugins enable rabbitmq_management[root@slave1 ~]# service rabbitmq-server restart[root@slave1 ~]# vim /etc/rabbitmq/rabbitmq.config[root@slave1 ~]# service rabbitmq-server restart ##增加rabbitmq ui的登录用户[root@slave1 ~]# rabbitmqctl add_user test 123456[root@slave1 ~]# rabbitmqctl set_user_tags test administrator[root@slave1 ~]# rabbitmqctl set_permissions -p \"/\" test \".*\" \".*\" \".*\"[root@slave1 ~]# rabbitmqctl list_usersListing userstest [administrator]guest [administrator] 2.2 在macOS中安装rabbitMQ1234567891011#使用homebrew安装rabbitMQ,若安装过homebrew可以忽略下面第一句命令$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"$ brew update$ brew install rabbitmq#启动命令$ brew services start rabbitmq#重启命令$ brew services restart rabbitmq#停止命令$ brew services stop rabbitmq 使用浏览器访问ip:15672(rabbitMQ的UI界面)，可使用上面设置的test用户密码进行登录","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://blog.monbuilder.top/categories/消息中间件/"}],"tags":[{"name":"rabbitMQ 分布式","slug":"rabbitMQ-分布式","permalink":"https://blog.monbuilder.top/tags/rabbitMQ-分布式/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://blog.monbuilder.top/categories/消息中间件/"}]},{"title":"activiti6-tutorial00","slug":"activiti6-tutorial00","date":"2018-10-16T03:02:23.000Z","updated":"2018-12-14T06:14:20.396Z","comments":true,"path":"2018/10/16/activiti6-tutorial00/","link":"","permalink":"https://blog.monbuilder.top/2018/10/16/activiti6-tutorial00/","excerpt":"","text":"","categories":[],"tags":[],"keywords":[]},{"title":"webSphere性能监控(JMX)","slug":"websphere-monitor","date":"2018-09-21T08:12:32.000Z","updated":"2018-12-14T06:14:20.399Z","comments":true,"path":"2018/09/21/websphere-monitor/","link":"","permalink":"https://blog.monbuilder.top/2018/09/21/websphere-monitor/","excerpt":"","text":"1.监控方式1.1 部署自带perfServletApp监控项目以下步骤为支持JMX组件采集，websphere所需做的一些配置操作：1&gt;访问websphere的web console页面，例如：https://localhost:9043/ibm/console/2&gt;点击左侧树形菜单：应用程序-&gt;应用程序类型-&gt;WebSphere企业应用程序；进入页面，查看有无安装了perfServletApp，若无再点击：安装 按钮3&gt;在新新页面中选择：本地路径；找到并上传perfServletApp.ear部署包(此包路径在$WEBSPHERE_INSTALL_HOME/installableApps/PerfServletApp.ear)，可以copy到本地在上传4&gt;点击下一步，开始安装，一路点“下一步”，不用做任何修改填空，直到点击“完成”，最后保存配置5&gt;启动perfServletApp程序6&gt;配置perfServletApp访问的安全校验,点击：应用程序-&gt;perfServletApp,进入安全用户配置,按照下列步骤，最后确认并保存到主配置 7&gt;需要以上配置生效，需要重启webSphere服务 1.2 配置新增ssl证书,通过soap协议访问mbean以下步骤为支持JMX组件采集，websphere所需做的一些配置操作：","categories":[{"name":"webSphere","slug":"webSphere","permalink":"https://blog.monbuilder.top/categories/webSphere/"}],"tags":[{"name":"性能监控","slug":"性能监控","permalink":"https://blog.monbuilder.top/tags/性能监控/"}],"keywords":[{"name":"webSphere","slug":"webSphere","permalink":"https://blog.monbuilder.top/categories/webSphere/"}]},{"title":"2018版Java面试题汇总","slug":"interview-java2018","date":"2018-08-16T02:39:09.000Z","updated":"2018-12-14T06:14:20.397Z","comments":true,"path":"2018/08/16/interview-java2018/","link":"","permalink":"https://blog.monbuilder.top/2018/08/16/interview-java2018/","excerpt":"","text":"1.Java集合框架1.1 Hashtable与Hashmap的区别1234答：继承不同，hashtable继承自dictionary，hashmap继承自abstractmap。Hashtable: 生成一个新的，空的hashtable，使用默认的capacity容量为11，factor增长因子为0.75, 其实就是因为这个put方法是synchronized的，所以可以保证其线程安全。Hashmap：允许null key/value，线程不安全另外，ConcurrentHashMap所谓线程安全是如果没有哈希冲突使用compareAndSwapObject方式新增节点，如果哈希冲突的时候锁住哈希冲突的节点，这样新增的节点是线程安全的，而 ConcurrentHashMap又不像hashtable那样整个put方法被锁定，所以性能比hashtable要好，因为这样不影响其他节点的插入和读取。（concurrenthashmap也不允许空键值，抛空指针异常，CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。） 1.2 arraylist linkedlist vector 区别123456答：1. 对ArrayList和LinkedList而言，在列表末尾增加一个元素所花的开销都是固定的。对ArrayList而言，主要是在内部数组中增加一项，指向所添加的元素，偶尔可能会导致对数组重新进行分配；而对LinkedList而言，这个开销是统一的，分配一个内部Entry对象。 2. 在ArrayList的中间插入或删除一个元素意味着这个列表中剩余的元素都会被移动；而在LinkedList的中间插入或删除一个元素的开销是固定的。 3. LinkedList不支持高效的随机元素访问。 4. ArrayList的空间浪费主要体现在在list列表的结尾预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗相当的空间可以这样说：当操作是在一列数据的后面添加数据而不是在前面或中间,并且需要随机地访问其中的元素时,使用ArrayList会提供比较好的性能；当你的操作是在一列数据的前面或中间添加或删除数据,并且按照顺序访问其中的元素时,就应该使用LinkedList了。 (遗留问题：在长度为10000的链表和数组中分别随机插入一个元素，哪个效率更高？【58到家面试题】答案是：数组。为啥？) 1.3 Java 集合部分都有哪些接口，主要体现了哪些设计模式？1234567答：Java 集合部分主要有Collection、List、Set、Map、Comparator、Iterator 等，主要体现的设计模式是策略模式和迭代模式策略模式主要体现在每个接口有不同的实现，可以完成互换，如List 接口下有ArrayList 和LinkedList,在不同的场景下可以互换。迭代模式主要体现在Iterator 的实现，为不同的数据存储方式(数组、链表、散列表等)提供了统一的访问方式。Comparator 体现的设计模式是什么？ -- 策略模式，即不改变对象自身，而使用一个策略对象去改变它的行为。注：策略模式的优缺点是什么：优点：（1）将具体算法逻辑与客户类分离，（2）避免了大量的if else 判断缺点：（1）每个算法一个类，产生了太多的类，（2）客户端要知道所有的策略类，以便决定使用哪一个。 2.线程2.1 线程五状态123456789101112新建、就绪、执行、阻塞、死亡其中，阻塞状态(Blocked)线程运行过程中，可能由于各种原因进入阻塞状态: 1&gt;线程通过调用sleep方法进入睡眠状态； 2&gt;线程调用一个在I/O上被阻塞的操作，即该操作在输入输出操作完成之前不会返回到它的调用者； 3&gt;线程试图得到一个锁，而该锁正被其他线程持有； 4&gt;线程在等待某个触发条件；所谓阻塞状态是正在运行的线程没有运行结束，暂时让出CPU，这时其他处于就绪状态的线程就可以获得CPU时间，进入运行状态。 死亡状态(Dead)有两个原因会导致线程死亡： 1) run方法正常退出而自然死亡， 2) 一个未捕获的异常终止了run方法而使线程猝死。 2.2 线程安全性的五种类别12345答：①. 不可变 -- 不可变的对象一定是线程安全的，并且永远也不需要额外的同步。Java 类库中大多数基本数值类如 Integer 、 String 和 BigInteger 都是不可变的。 ②. 线程安全 -- 线程安全的对象，由类的规格说明所规定的约束在对象被多个线程访问时仍然有效，不管运行时环境如何排列，线程都不需要任何额外的同步。这种线程安全性保证是很严格的 -- 许多类，如 Hashtable 或者 Vector都不能满足这种严格的定义。 ③. 有条件的线程安全 -- 有条件的线程安全类对于单独的操作可以是线程安全的，但是某些操作序列可能需要外部同步。条件线程安全的最常见的例子是遍历由 Hashtable 或者 Vector 或者返回的迭代器。 ④. 线程兼容 -- 线程兼容类不是线程安全的，但是可以通过正确使用同步而在并发环境中安全地使用。这可能意味着用一个 synchronized 块包围每一个方法调用，或者创建一个包装器对象，其中每一个方法都是同步的(就像 Collections.synchronizedList() 一样)。许多常见的类是线程兼容的，如集合类 ArrayList 和 HashMap 、 java.text.SimpleDateFormat 、或者 JDBC 类 Connection 和 ResultSet。 ⑤. 线程对立 -- 线程对立类是那些不管是否调用了外部同步都不能在并发使用时安全地呈现的类。线程对立很少见，当类修改静态数据，而静态数据会影响在其他线程中执行的其他类的行为，这时通常会出现线程对立。线程对立类的一个例子是调用 System.setOut() 的类。 2.3 简单理解线程池技术12345678910111213141516171819202122232425262728293031323334353637383940多线程技术主要解决处理器单元内多个线程执行的问题，它可以显著减少处理器单元的闲置时间，增加处理器单元的吞吐能力。 假设一个服务器完成一项任务所需时间为：T1 创建线程时间，T2 在线程中执行任务的时间，T3 销毁线程时间。 如果：T1 + T3 远大于 T2，则可以采用线程池，以提高服务器性能。 一个线程池包括以下四个基本组成部分： 1、线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务； 2、工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务； 3、任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等； 4、任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。 线程池技术正是关注如何缩短或调整T1,T3时间的技术，从而提高服务器程序性能的。它把T1，T3分别安排在服务器程序的启动和结束的时间段或者一些空闲的时间段，这样在服务器程序处理客户请求时，不会有T1，T3的开销了。 线程池不仅调整T1,T3产生的时间段，而且它还显著减少了创建线程的数目，看一个例子： 假设一个服务器一天要处理50000个请求，并且每个请求需要一个单独的线程完成。在线程池中，线程数一般是固定的，所以产生线程总数不会超过线程池中线程的数目，而如果服务器不利用线程池来处理这些请求则线程总数为50000。一般线程池大小是远小于50000。所以利用线程池的服务器程序不会为了创建50000而在处理请求时浪费时间，从而提高效率。代码实现中并没有实现任务接口，而是把Runnable对象加入到线程池管理器（ThreadPool），然后剩下的事情就由线程池管理器（ThreadPool）来完成了。Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。// 执行任务,其实只是把任务加入任务队列，什么时候执行有线程池管理器觉定 public void execute(Runnable task) &#123; synchronized (taskQueue) &#123; taskQueue.add(task); taskQueue.notify(); &#125; &#125; // 批量执行任务,其实只是把任务加入任务队列，什么时候执行有线程池管理器觉定 public void execute(Runnable[] task) &#123; synchronized (taskQueue) &#123; for (Runnable t : task) taskQueue.add(t); taskQueue.notify(); &#125; &#125; // 批量执行任务,其实只是把任务加入任务队列，什么时候执行有线程池管理器觉定 public void execute(List&lt;Runnable&gt; task) &#123; synchronized (taskQueue) &#123; for (Runnable t : task) taskQueue.add(t); taskQueue.notify(); &#125; &#125; 2.3 说一下synchronized原理12345678910111213141516答：①.synchronized的字节码表示： 在java语言中存在两种内建的synchronized语法：1、synchronized语句；2、synchronized方法。对于synchronized语句当Java源代码被javac编译成bytecode的时候，会在同步块的入口位置和退出位置分别插入monitorenter和monitorexit字节码指令。而synchronized方法则会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的access_flags字段中的synchronized标志位置1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示Class做为锁对象。 ②.JVM中锁的优化： 简单来说在JVM中monitorenter和monitorexit字节码依赖于底层的操作系统的Mutex Lock来实现的，但是由于使用Mutex Lock需要将当前线程挂起并从用户态切换到内核态来执行，这种切换的代价是非常昂贵的；然而在现实中的大部分情况下，同步方法是运行在单线程环境（无锁竞争环境）如果每次都调用Mutex Lock那么将严重的影响程序的性能。不过在jdk1.6中对锁的实现引入了大量的优化，如锁粗化（Lock Coarsening）、锁消除（Lock Elimination）、轻量级锁（Lightweight Locking）、偏向锁（Biased Locking）、适应性自旋（Adaptive Spinning）等技术来减少锁操作的开销。 ③.在JVM中创建对象时会在对象前面加上两个字大小的对象头（Object header），在32位机器上一个字为32bit，根据不同的状态位Mark World中存放不同的内容，在轻量级锁中，Mark Word被分成两部分，刚开始时LockWord为被设置为HashCode、最低三位表示LockWord所处的状态，初始状态为001表示无锁状态。 ④.Monitor Record是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表；那么这些monitor record有什么用呢？每一个被锁住的对象都会和一个monitor record关联（对象头中的LockWord指向monitor record的起始地址，由于这个地址是8byte对齐的所以LockWord的最低三位可以用来作为状态），同时monitor record中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。 ⑤.获取锁（monitorenter）的大概过程如下： 当对象处于无锁状态时（RecordWord值为HashCode，状态位为001），线程首先从自己的可用moniter record列表中取得一个空闲的moniter record，初始Nest和Owner值分别被预先设置为1和该线程自己的标识，一旦monitor record准备好然后我们通过CAS原子指令安装该monitor record的起始地址到对象头的LockWord字段来膨胀. 对象已经被膨胀同时Owner中保存的线程标识为获取锁的线程自己，这就是重入（reentrant）锁的情况，只需要简单的将Nest加1即可。不需要任何原子操作，效率非常高 对象已膨胀但Owner的值为NULL，当一个锁上存在阻塞或等待的线程同时锁的前一个拥有者刚释放锁时会出现这种状态，此时多个线程通过CAS原子指令在多线程竞争状态下试图将Owner设置为自己的标识来获得锁，竞争失败的线程在则会进入到第四种情况的执行路径。 对象处于膨胀状态同时Owner不为NULL(被锁住)，在调用操作系统的重量级的互斥锁之前先自旋一定的次数，当达到一定的次数时如果仍然没有成功获得锁，则开始准备进入阻塞状态，首先将rfThis的值原子性的加1，由于在加1的过程中可能会被其他线程破坏Object和monitor record之间的关联，所以在原子性加1后需要再进行一次比较以确保LockWord的值没有被改变，当发现被改变后则要重新进行monitorenter过程。同时再一次观察Owner是否为NULL，如果是则调用CAS参与竞争锁，锁竞争失败则进入到阻塞状态。 ⑥.释放锁（monitorexit）的大概过程如下： 首先检查该对象是否处于膨胀状态并且该线程是这个锁的拥有者，如果发现不对则抛出异常； 检查Nest字段是否大于1，如果大于1则简单的将Nest减1并继续拥有锁，如果等于1，则进入到下一步； 检查rfThis是否大于0，设置Owner为NULL然后唤醒一个正在阻塞或等待的线程再一次试图获取锁，如果等于0则进入到下一步； 缩小（deflate）一个对象，通过将对象的LockWord置换回原来的HashCode值来解除和monitor record之间的关联来释放锁，同时将monitor record放回到线程是有的可用monitor record列表。","categories":[{"name":"面试","slug":"面试","permalink":"https://blog.monbuilder.top/categories/面试/"}],"tags":[{"name":"Java面试","slug":"Java面试","permalink":"https://blog.monbuilder.top/tags/Java面试/"}],"keywords":[{"name":"面试","slug":"面试","permalink":"https://blog.monbuilder.top/categories/面试/"}]},{"title":"搭建大数据平台系列(1)-Hadoop环境搭建[hdfs,yarn,mapreduce]","slug":"hadoop","date":"2018-08-09T15:00:08.000Z","updated":"2018-12-14T06:14:20.398Z","comments":true,"path":"2018/08/09/hadoop/","link":"","permalink":"https://blog.monbuilder.top/2018/08/09/hadoop/","excerpt":"","text":"1.ssh免密码登录设置123[hadoop@master ~]$ ssh -versionOpenSSH_5.3p1, OpenSSL 1.0.1e-fips 11 Feb 2013Bad escape character 'rsion'. 查看ssh的版本后，如果ssh未安装则需要执行如下安装命令：1[hadoop@master ~]$ sudo yum install openssh-server 在每台机器上都执行一次下面的命令：123456$ ssh-keygen –t rsa #一路回车，提示要填的都默认不填，按回车上面执行完成后，每台机器上都会生成一个~/.ssh文件夹$ ll ~/.ssh #查看.ssh文件下的文件列表-rw-------. 1 hadoop hadoop 1580 Apr 18 16:53 authorized_keys-rw-------. 1 hadoop hadoop 1675 Apr 15 16:01 id_rsa-rw-r--r--. 1 hadoop hadoop 395 Apr 15 16:01 id_rsa.pub 把slave1，slave2，slave3上生成的公钥id_rsa.pub发给master机器：在slave1机器上：1[hadoop@slave1 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave1 在slave2机器上：1[hadoop@slave2 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave2 在slave3机器上：1[hadoop@slave3 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave3 在master机器上，将所有公钥加到新增的用于认证的公钥文件authorized_keys中：1[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub* &gt;&gt; ~/.ssh/authorized_keys 需要修改文件authorized_keys的权限（权限的设置非常重要，因为不安全的设置安全设置,会让你不能使用RSA功能 ）1[hadoop@master ~]$ chmod 600 ~/.ssh/authorized_keys #如果免密码不成功有可能缺少这步 将公钥文件authorized_keys分发给每台slave:123[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/ 2.Java环境的安装下载jdk-8u60-linux-x64.tar.gz安装包后(放在~/bigdataspace路径下)： 12 [hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ tar -zxvf jdk-8u60-linux-x64.tar.gz 修改环境变量配置文件:123456[hadoop@master bigdataspace]$ sudo vi /etc/profile(在配置文件末尾加上如下配置)export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 让环境变量设置生效：1[hadoop@master bigdataspace]$ source /etc/profile 验证Java是否安装成功：1234[hadoop@master bigdataspace]$ java -versionjava version \"1.8.0_60\"Java(TM) SE Runtime Environment (build 1.8.0_60-b27)Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode) (每台机器上都需要按照上面的操作安装Java)每台机器上执行：1[hadoop@master ~]$ sudo chmod 777 /data/ #让所有用户可操作/data目录下的数据 3.集群上的机器实现同步时间检查时间服务是否安装:12[hadoop@master ~]$ rpm -q ntpntp-4.2.6p5-1.el6.centos.x86_64 #这表示已安装了，如果没有安装，这是空白 如果没有安装，需要执行下面的安装命令：1[hadoop@master ~]$ sudo yum install ntp 需要配置NTP服务为自启动:123456789[hadoop@master ~]$ sudo chkconfig ntpd on[hadoop@master ~]$ chkconfig --list ntpdntpd 0:off 1:off 2:on 3:on 4:on 5:on 6:off(需要打开master机器上udp协议的123端口是为了其他节点使用ntpdate通过该端口同步master机器的时间)[hadoop@master ~]$ sudo vi /etc/sysconfig/iptables(新增的端口配置)-A INPUT -m state --state NEW -m udp -p udp --dport 123 -j ACCEPT[hadoop@master ~]$ sudo service iptables restart 在配置前，先使用ntpdate手动同步下时间，免得本机与外部时间服务器时间差距太大，让ntpd不能正常同步。12[hadoop@master ~]$ sudo ntpdate pool.ntp.org26 Apr 17:12:15 ntpdate[7376]: step time server 202.112.29.82 offset 13.827386 sec 更改master机器上的相关配置文件:1[hadoop@master ~]$ sudo vim /etc/ntp.conf 123456789101112131415161718192021222324252627282930313233343536373839(下面只显示修改的必要项)# Hosts on local network are less restricted.restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap#让同一局域网ip段可以进行时间同步：restrict 10.3.19.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#外部时间服务器server pool.ntp.org iburstserver 0.asia.pool.ntp.org iburstserver 1.asia.pool.ntp.org iburstserver 1.asia.pool.ntp.org iburstserver 2.asia.pool.ntp.org iburst#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# allow update time by the upper server# Undisciplined Local Clock. This is a fake driver intended for backup# and when no outside source of synchronized time is available.# 外部时间服务器不可用时，以本地时间作为时间服务server 127.127.1.0fudge 127.127.1.0 stratum 10#############################################################其他节点/etc/ntp.conf（slave1,slave2,slave3）的配置：……..#server 3.centos.pool.ntp.org iburst#外部时间服务器，以master时间为准进行同步server master iburst…….. 1234[hadoop@master ~]$ sudo service ntpd start(每台机器上都需要，设置ntpd开机启动，并第一次手动打开ntpd)，命令如下：$ sudo chkconfig ntpd on #开机启动ntpd$ sudo service ntpd start #启动 ntpd 时间同步设置参考：http://cn.soulmachine.me/blog/20140124/ 时间同步设置总结： 每个节点上安装ntpd，并设置为开机启动，当然第一次要先手动启动，通过配置/etc/ntp.conf文件，让master作为时间同步服务器，这台机器的时间是根据联网同步网络时间的，其他节点以master的ip作为同步的地址 配置完成后，发现后面的节点时间可能还未同步，可能需要等30分钟左右，一段时间后时间都会以master为准，进行同步 4.Hadoop的安装、配置下载hadoop-2.6.0-cdh5.5.0.tar.gz安装包后(放在master机器上的~/bigdataspace路径下)：12[hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ tar -zxvf hadoop-2.6.0-cdh5.5.0.tar.gz 进入hadoop配置文件路径:1[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/etc/hadoop 1&gt; 在hadoop-env.sh中配置JAVA_HOME：1[hadoop@master hadoop]$ vi hadoop-env.sh 123# set JAVA_HOME in this file, so that it is correctly defined on# The java implementation to use.export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60 2&gt; 在yarn-env.sh中配置JAVA_HOME：1[hadoop@master hadoop]$ vi yarn-env.sh 12# some Java parametersexport JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60 3&gt; 在slaves中配置slave节点的ip或者host1[hadoop@master hadoop]$ vi slaves 123slave1slave2slave3 4&gt; 修改core-site.xml1[hadoop@master hadoop]$ vi core-site.xml 1234567891011&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop-2.6.0-cdh5.5.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5&gt; 修改hdfs-site.xml1[hadoop@master hadoop]$ vi hdfs-site.xml 1234567891011121314151617181920&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop-2.6.0-cdh5.5.0/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop-2.6.0-cdh5.5.0/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6&gt; 修改mapred-site.xml1[hadoop@master hadoop]$ vi mapred-site.xml 123456789101112131415&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7&gt; 修改yarn-site.xml1[hadoop@master hadoop]$ vi yarn-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 因为CDH版本缺少hadoop的native库，因此需要引入，否则会报错，解决方法：http://www.cnblogs.com/huaxiaoyao/p/5046374.html本次安装具体采取的解决方法：123[hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.5.0/RPMS/x86_64/hadoop-2.6.0+cdh5.5.0+921-1.cdh5.5.0.p0.15.el6.x86_64.rpm[hadoop@master bigdataspace]$ rpm2cpio *.rpm | cpio -div 在bigdataspace文件夹下1$ cp -r ./usr/lib/hadoop/lib/native/ ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/lib/native/ 删除解压后得到的文件：1234[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/etc/[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/usr/[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/var//$ rm ~/ bigdataspace/hadoop-2.6.0+cdh5.5.0+921-1.cdh5.5.0.p0.15.el6.x86_64.rpm 5.使用scp命令分发配置好的hadoop到各个子节点123$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave1:~/bigdataspace/$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave2:~/bigdataspace/$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave3:~/bigdataspace/ (每台机器)修改环境变量配置文件:1[hadoop@master bigdataspace]$ sudo vi /etc/profile (在配置文件末尾加上如下配置)12export HADOOP_HOME=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH 让环境变量设置生效：1[hadoop@master bigdataspace]$ source /etc/profile 6.启动并验证Hadoop1234[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0 #进入hadoop目录 [hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./bin/hdfs namenode –format #格式化namenode[hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./sbin/start-dfs.sh #启动dfs[hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./sbin/start-yarn.sh #启动yarn 可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程12345[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps3407 SecondaryNameNode3218 NameNode3552 ResourceManager3910 Jps 在 slave1 上应该有以下几个进程1234[hadoop@slave1 ~]$ jps2072 NodeManager2213 Jps1962 DataNode 或者在浏览器中输入 http://master:8088 ，应该有 hadoop 的管理界面出来了,并通过http://master:8088/cluster/nodes能看到 slave1、slave2、slave3节点 7.启动Hadoop自带的jobhistoryserver[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0 #进入hadoop目录[hadoop@master hadoop-2.6.0-cdh5.5.0]$ sbin/mr-jobhistory-daemon.sh start historyserver(mapred-site.xml配置文件有对jobhistory的相关配置)[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps5314 Jps19994 JobHistoryServer19068 NameNode19422 ResourceManager19263 SecondaryNameNode 参考：http://blog.csdn.net/liubei_whut/article/details/42397985 8.停止hadoop集群的问题Linux运行一段时间后，/tmp下的文件夹下面会清空一些文件，hadoop的停止脚本stop-all.sh是需要根据/tmp下面的pid文件关闭对应的进程，当/tmp下的文件被自动清理后可能会出出先的错误：123456789$ ./sbin/stop-all.shStopping namenodes on [master]master: no namenode to stopslave1: no datanode to stopslave2: no datanode to stopslave3: no datanode to stopStopping secondary namenodes [master]master: no secondarynamenode to stop…… 方法1：这时需要在/tmp文件夹下手动创建恢复这些pid文件master节点（每个文件中保存对应的进程id）：hadoop-hadoop-namenode.pidhadoop-hadoop-secondarynamenode.pidyarn-hadoop-resourcemanager.pidslave节点（每个文件中保存对应的进程id）：hadoop-hadoop-datanode.pidyarn-hadoop-nodemanager.pid方法2：使用kill -9逐个关闭相应的进程id 从根本上解决的方法：（首先使用了方法1或方法2关闭了hadoop集群）1.修改配置文件hadoop-env.sh:1234#export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;export HADOOP_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids#export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;export HADOOP_SECURE_DN_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids 2.修改配置文件yarn-env.sh:1export YARN_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids 3.创建文件夹pids：1$ mkdir /data/hadoop-2.6.0-cdh5.5.0/pids（发现会自动创建pids文件，因此不需要创建） 这2个步骤需要在各个节点都执行.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"搭建大数据平台系列(0)-机器准备","slug":"conf-os","date":"2018-07-20T05:16:05.000Z","updated":"2018-12-14T06:14:20.376Z","comments":true,"path":"2018/07/20/conf-os/","link":"","permalink":"https://blog.monbuilder.top/2018/07/20/conf-os/","excerpt":"","text":"0. 前期规划假设现在有四台机器，各自ip如下:1234192.168.1.201192.168.1.202192.168.1.203192.168.1.204 安装下面统一要求进行重装系统： 每台机器的系统为：CentOS-6.5-x86_64 每台机器的磁盘分区为： 1234/boot : 系统引导分区/ : 系统安装区/data : 系统数据分区（swap分区尚未建立，待机器内存不足需求时再建立） 每台机器上安装的都是Mininal Desktop版本 每台机器的主机名(hostname)分别为：master、slave1、slave2、slave3 每台机器的root密码都是:master 每台机器上建立的第一个非root用户都为:hadoop，密码为:hadoop 非root用户在centos中获取sudo权限的命令：123456[hadoop@slave3 ~]$ su - #切换到root用户，需要输入root密码[root@slave3 ~]# visudo -f /etc/sudoers## Allow root to run any commands anywhereroot ALL=(ALL) ALL在上面这条文字下加上如下：hadoop ALL=(ALL) ALL 这样配置后，hadoop用户可以使用sudo命令了 修改每台机器的/etc/hosts文件，如：12345[root@slave3 ~]$ sudo vi /etc/hosts192.168.1.201 master192.168.1.202 slave1192.168.1.203 slave2192.168.1.204 slave3 9.防火墙设置 因为4台机器组成的大数据平台小集群需要互相访问对方的多个端口，需要在防火墙中打开这些端口的访问（如果未打开的话），当然最方便的就是把每台机器上的防火墙关闭了。123456789101112131415161718192021222324[hadoop@master ~]$ sudo service iptables start #打开防火墙[hadoop@master ~]$ sudo service iptables status #查看防火墙[hadoop@master ~]$ sudo service iptables stop #关闭防火墙(注意：防火墙操作时需要root权限的，非root用户不使用sudo的话，不会报错，但操作无结果)修改防火墙设置(红色为新增配置：开放2000-6000,7000以上的端口):[hadoop@master ~]$ sudo vim /etc/sysconfig/iptables# Firewall configuration written by system-config-firewall# Manual customization of this file is not recommended.*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT-A INPUT -p icmp -j ACCEPT-A INPUT -i lo -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 2000:6000 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 7000: -j ACCEPT-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j REJECT --reject-with icmp-host-prohibitedCOMMIT保存上面的文件后，需要重启防火墙，让配置生效！[hadoop@master ~]$ sudo service iptables restart #重启防火墙","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"Hexo+Github Page搭建个人博客","slug":"build-blog","date":"2018-06-22T09:03:09.000Z","updated":"2018-12-14T06:14:20.390Z","comments":true,"path":"2018/06/22/build-blog/","link":"","permalink":"https://blog.monbuilder.top/2018/06/22/build-blog/","excerpt":"","text":"什么是Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。堪称在座各位喜欢Markdown的优雅人士博客建站神器哟！ 1. Quick Start1.1 创建存放Github Pages的仓库Github Pages 是面向用户、组织和项目开放的公共静态页面搭建托管服务，站点可以被免费托管在 Github 上，你可以选择使用 Github Pages 提供的域名 github.io 或者自定义域名来发布站点。 需要Github账号，请登录https://github.com/ 注册。\b\b登录了自己的github账号后，可以安装下图一样，创建自己的GitHub Pages仓库名[参考https://pages.github.com/ ]，[PS] 仓库名repository name需要约定为: 你的账号名.github.io创建好博客项目仓库后，可以通过git命名下载到本地，并编辑一下README.md从本地提交到GitHub，这样做主要是使本地文件与Github关联起来，方便后面hexo deploy,直接部署博客内容到GitHub进行更新。12345678910$ git clone https://github.com/yourGithubName/yourGithubName.github.io$ vim README.md# REAMME.md上可以简单写一些博客介绍啥的$ git config --global user.email \"you@example.com\"$ git config --global user.name \"Your Name\"$ git add ./$ git commit -m 'test'$ git push -u origin masterUsername for 'https://github.com': Builder34Password for 'https://Builder34@github.com': 1.2 Hexo安装安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js (请看https://nodejs.org/zh-cn/) Git （请看https://git-scm.com/downloads）安装好上面2个程序后，可以进行hexo的安装了：1$ npm install -g hexo-cli 1.3 Hexo初始化安装 Hexo 完成后，我们可以在本地新建一个文件夹如：builder34.github.io(\b这个目录是我们Github Pages博客项目的目录),假如我的文件夹路径为/home/test/builder34.github.io，建站初始化命令可以如下:12345$ cd /home/test/builder34.github.io$ hexo init ./$ npm install$ hexo generate 下面介绍几个常用的hexo命令(括号里面的命令为缩写形式，效果一样)： 1. hexo generate(hexo g) #生成静态文件，会在当前目录下生成一个新的叫做public的文件夹 2. hexo new &quot;postTitle&quot; #新建博客文章 3. hexo new page &quot;pageTitle&quot; #新建1个页面 4. hexo server(hexo s) #启动本地web服务预览(加参数--debug,用于调试，如：hexo s --debug) 5. hexo deploy(hexo d) #部署播客到远端（比如Github,coding,heroku等平台） 在命令行中输入hexo s --debug后，运行成功后，可以在浏览器中输入：http://localhost:4000看到自己新建的博客了。 1.4 更改主题一般我们初始化博客的文件夹后，文件结构大概如下：12345678910111213$ lltotal 1352-rw-r--r-- 1 builder34 staff 32B 4 14 01:34 README.md-rw-r--r-- 1 builder34 staff 2.3K 6 25 10:40 _config.yml-rw-r--r-- 1 builder34 staff 32K 6 26 15:50 db.json-rw-r--r-- 1 builder34 staff 458K 6 26 15:56 debug.logdrwxr-xr-x 293 builder34 staff 9.2K 6 25 10:42 node_modules-rw-r--r-- 1 builder34 staff 110K 6 22 23:59 package-lock.json-rw-r--r-- 1 builder34 staff 564B 6 22 23:59 package.jsondrwxr-xr-x 14 builder34 staff 448B 6 25 10:40 publicdrwxr-xr-x 5 builder34 staff 160B 4 17 23:12 scaffoldsdrwxr-xr-x 3 builder34 staff 96B 6 25 10:57 sourcedrwxr-xr-x 6 builder34 staff 192B 6 25 11:33 themes themes文件夹是我们博客主题的存放地方，下面我推荐一个主题：BlueLake123456$ cd themes/$ git clone https://github.com/chaooo/hexo-theme-BlueLake.git ./BlueLake$ npm install hexo-renderer-jade@0.3.0 --save$ npm install hexo-renderer-stylus --save(该主题更细致的配置，请登录上面这个github网址，阅读README.md进行定制化配置） 在Hexo配置文件（$your_blog_path/_config.yml）中把主题设置修改为BlueLake。1theme: BlueLake 完成配置后，执行下面语句，重新打开http://localhost:4000 可以看到博客以一个新的主题展现了12$ hexo g$ hexo s --debug 1.5 hexo部署到Github配置$your_blog_path/_config.yml文件的Deployment：12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/your_githubName/your_githubName.github.io.git 通过下面的命名进行博客静态页面的生成，以及部署到远端Github Pages12345678#删除静态文件,即 public 文件$ hexo clean#生成静态文件,即 public 文件$ hexo generate#部署到远程站点$ hexo deploy#也可以使用组合命令(替代上面\b2条命令)：生成静态命令并部署到远程站点$ hexo deploy -g 12使用 hexo deploy 命名部署到github失败，报上面的错误时，安装下面的插件即可解决:$ npm install hexo-deployer-git --save 至此，Hexo+Github Pages构建个人博客网站已经\b基本完成了。可以通过网页访问自己的博客地址如\b：https://builder34.github.io 2.设置博客自定义域名进入自己博客的repository仓库，通过类似如下的页面进行设置：进入了settings页面后，往下拉直到看到Github Pages模块： 所填的自定义域名是需要自己已经在万网上注册的了，并且如果\b勾选了 Enforce HTTPS 的话，你的域名是需要ssl证书的哟。 3.注意事项3.1 上传README.md并防止被渲染成文章123456#在\b博客根目录下，新建或编辑你的README.md文件$ vim README.md$ mv README.md ./sources#修改_config.yml文件,设置不渲染的文件$ vim _config.ymlskip_render: README.md 3.2 每次\bhexo deploy后Github Pages自定义域名会被重置的问题需要在\bsources目录下新建CNAME文件(注意为全大写无后缀的文件哦),文件\b内容为你需要映射到的自定义域名：1234$ vim CNAMEblog.monbuilder.top$ mv CNAME ./sources 4.写文章12$ hexo new interview-java2018INFO Created: ~/Documents/workspace/builder/blog/builder34.github.io/source/_posts/interview-java2018.md 使用上面的命令创建文章，找到对应的md文件，使用你喜欢的markdown编辑器，尽情输出吧…下面两个命令是，生成静态页面，再部署，然后就可以在你的博客网站上看到你新发布的文章了，哈哈！12$ hexo g$ hexo d","categories":[{"name":"hexo博客","slug":"hexo博客","permalink":"https://blog.monbuilder.top/categories/hexo博客/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://blog.monbuilder.top/tags/hexo/"}],"keywords":[{"name":"hexo博客","slug":"hexo博客","permalink":"https://blog.monbuilder.top/categories/hexo博客/"}]}]}