[{"title":"搭建大数据平台系列(1)-Hadoop环境搭建[hdfs,yarn,mapreduce]","date":"2018-08-09T15:00:08.000Z","path":"2018/08/09/hadoop/","text":"1.ssh免密码登录设置123[hadoop@master ~]$ ssh -versionOpenSSH_5.3p1, OpenSSL 1.0.1e-fips 11 Feb 2013Bad escape character 'rsion'. 查看ssh的版本后，如果ssh未安装则需要执行如下安装命令：1[hadoop@master ~]$ sudo yum install openssh-server 在每台机器上都执行一次下面的命令：123456$ ssh-keygen –t rsa #一路回车，提示要填的都默认不填，按回车上面执行完成后，每台机器上都会生成一个~/.ssh文件夹$ ll ~/.ssh #查看.ssh文件下的文件列表-rw-------. 1 hadoop hadoop 1580 Apr 18 16:53 authorized_keys-rw-------. 1 hadoop hadoop 1675 Apr 15 16:01 id_rsa-rw-r--r--. 1 hadoop hadoop 395 Apr 15 16:01 id_rsa.pub 把slave1，slave2，slave3上生成的公钥id_rsa.pub发给master机器：在slave1机器上：1[hadoop@slave1 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave1 在slave2机器上：1[hadoop@slave2 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave2 在slave3机器上：1[hadoop@slave3 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave3 在master机器上，将所有公钥加到新增的用于认证的公钥文件authorized_keys中：1[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub* &gt;&gt; ~/.ssh/authorized_keys 需要修改文件authorized_keys的权限（权限的设置非常重要，因为不安全的设置安全设置,会让你不能使用RSA功能 ）1[hadoop@master ~]$ chmod 600 ~/.ssh/authorized_keys #如果免密码不成功有可能缺少这步 将公钥文件authorized_keys分发给每台slave:123[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/ 2.Java环境的安装下载jdk-8u60-linux-x64.tar.gz安装包后(放在~/bigdataspace路径下)： 12 [hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ tar -zxvf jdk-8u60-linux-x64.tar.gz 修改环境变量配置文件:123456[hadoop@master bigdataspace]$ sudo vi /etc/profile(在配置文件末尾加上如下配置)export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 让环境变量设置生效：1[hadoop@master bigdataspace]$ source /etc/profile 验证Java是否安装成功：1234[hadoop@master bigdataspace]$ java -versionjava version \"1.8.0_60\"Java(TM) SE Runtime Environment (build 1.8.0_60-b27)Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode) (每台机器上都需要按照上面的操作安装Java)每台机器上执行：1[hadoop@master ~]$ sudo chmod 777 /data/ #让所有用户可操作/data目录下的数据 3.集群上的机器实现同步时间检查时间服务是否安装:12[hadoop@master ~]$ rpm -q ntpntp-4.2.6p5-1.el6.centos.x86_64 #这表示已安装了，如果没有安装，这是空白 如果没有安装，需要执行下面的安装命令：1[hadoop@master ~]$ sudo yum install ntp 需要配置NTP服务为自启动:123456789[hadoop@master ~]$ sudo chkconfig ntpd on[hadoop@master ~]$ chkconfig --list ntpdntpd 0:off 1:off 2:on 3:on 4:on 5:on 6:off(需要打开master机器上udp协议的123端口是为了其他节点使用ntpdate通过该端口同步master机器的时间)[hadoop@master ~]$ sudo vi /etc/sysconfig/iptables(新增的端口配置)-A INPUT -m state --state NEW -m udp -p udp --dport 123 -j ACCEPT[hadoop@master ~]$ sudo service iptables restart 在配置前，先使用ntpdate手动同步下时间，免得本机与外部时间服务器时间差距太大，让ntpd不能正常同步。12[hadoop@master ~]$ sudo ntpdate pool.ntp.org26 Apr 17:12:15 ntpdate[7376]: step time server 202.112.29.82 offset 13.827386 sec 更改master机器上的相关配置文件:1[hadoop@master ~]$ sudo vim /etc/ntp.conf 123456789101112131415161718192021222324252627282930313233343536373839(下面只显示修改的必要项)# Hosts on local network are less restricted.restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap#让同一局域网ip段可以进行时间同步：restrict 10.3.19.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#外部时间服务器server pool.ntp.org iburstserver 0.asia.pool.ntp.org iburstserver 1.asia.pool.ntp.org iburstserver 1.asia.pool.ntp.org iburstserver 2.asia.pool.ntp.org iburst#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# allow update time by the upper server# Undisciplined Local Clock. This is a fake driver intended for backup# and when no outside source of synchronized time is available.# 外部时间服务器不可用时，以本地时间作为时间服务server 127.127.1.0fudge 127.127.1.0 stratum 10#############################################################其他节点/etc/ntp.conf（slave1,slave2,slave3）的配置：……..#server 3.centos.pool.ntp.org iburst#外部时间服务器，以master时间为准进行同步server master iburst…….. 1234[hadoop@master ~]$ sudo service ntpd start(每台机器上都需要，设置ntpd开机启动，并第一次手动打开ntpd)，命令如下：$ sudo chkconfig ntpd on #开机启动ntpd$ sudo service ntpd start #启动 ntpd 时间同步设置参考：http://cn.soulmachine.me/blog/20140124/ 时间同步设置总结： 每个节点上安装ntpd，并设置为开机启动，当然第一次要先手动启动，通过配置/etc/ntp.conf文件，让master作为时间同步服务器，这台机器的时间是根据联网同步网络时间的，其他节点以master的ip作为同步的地址 配置完成后，发现后面的节点时间可能还未同步，可能需要等30分钟左右，一段时间后时间都会以master为准，进行同步 4.Hadoop的安装、配置下载hadoop-2.6.0-cdh5.5.0.tar.gz安装包后(放在master机器上的~/bigdataspace路径下)：12[hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ tar -zxvf hadoop-2.6.0-cdh5.5.0.tar.gz 进入hadoop配置文件路径:1[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/etc/hadoop 1&gt; 在hadoop-env.sh中配置JAVA_HOME：1[hadoop@master hadoop]$ vi hadoop-env.sh 123# set JAVA_HOME in this file, so that it is correctly defined on# The java implementation to use.export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60 2&gt; 在yarn-env.sh中配置JAVA_HOME：1[hadoop@master hadoop]$ vi yarn-env.sh 12# some Java parametersexport JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60 3&gt; 在slaves中配置slave节点的ip或者host1[hadoop@master hadoop]$ vi slaves 123slave1slave2slave3 4&gt; 修改core-site.xml1[hadoop@master hadoop]$ vi core-site.xml 1234567891011&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop-2.6.0-cdh5.5.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5&gt; 修改hdfs-site.xml1[hadoop@master hadoop]$ vi hdfs-site.xml 1234567891011121314151617181920&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop-2.6.0-cdh5.5.0/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop-2.6.0-cdh5.5.0/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6&gt; 修改mapred-site.xml1[hadoop@master hadoop]$ vi mapred-site.xml 123456789101112131415&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7&gt; 修改yarn-site.xml1[hadoop@master hadoop]$ vi yarn-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 因为CDH版本缺少hadoop的native库，因此需要引入，否则会报错，解决方法：http://www.cnblogs.com/huaxiaoyao/p/5046374.html本次安装具体采取的解决方法：123[hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.5.0/RPMS/x86_64/hadoop-2.6.0+cdh5.5.0+921-1.cdh5.5.0.p0.15.el6.x86_64.rpm[hadoop@master bigdataspace]$ rpm2cpio *.rpm | cpio -div 在bigdataspace文件夹下1$ cp -r ./usr/lib/hadoop/lib/native/ ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/lib/native/ 删除解压后得到的文件：1234[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/etc/[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/usr/[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/var//$ rm ~/ bigdataspace/hadoop-2.6.0+cdh5.5.0+921-1.cdh5.5.0.p0.15.el6.x86_64.rpm 5.使用scp命令分发配置好的hadoop到各个子节点123$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave1:~/bigdataspace/$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave2:~/bigdataspace/$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave3:~/bigdataspace/ (每台机器)修改环境变量配置文件:1[hadoop@master bigdataspace]$ sudo vi /etc/profile (在配置文件末尾加上如下配置)12export HADOOP_HOME=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH 让环境变量设置生效：1[hadoop@master bigdataspace]$ source /etc/profile 6.启动并验证Hadoop1234[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0 #进入hadoop目录 [hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./bin/hdfs namenode –format #格式化namenode[hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./sbin/start-dfs.sh #启动dfs[hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./sbin/start-yarn.sh #启动yarn 可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程12345[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps3407 SecondaryNameNode3218 NameNode3552 ResourceManager3910 Jps 在 slave1 上应该有以下几个进程1234[hadoop@slave1 ~]$ jps2072 NodeManager2213 Jps1962 DataNode 或者在浏览器中输入 http://master:8088 ，应该有 hadoop 的管理界面出来了,并通过http://master:8088/cluster/nodes能看到 slave1、slave2、slave3节点 7.启动Hadoop自带的jobhistoryserver[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0 #进入hadoop目录[hadoop@master hadoop-2.6.0-cdh5.5.0]$ sbin/mr-jobhistory-daemon.sh start historyserver(mapred-site.xml配置文件有对jobhistory的相关配置)[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps5314 Jps19994 JobHistoryServer19068 NameNode19422 ResourceManager19263 SecondaryNameNode 参考：http://blog.csdn.net/liubei_whut/article/details/42397985 8.停止hadoop集群的问题Linux运行一段时间后，/tmp下的文件夹下面会清空一些文件，hadoop的停止脚本stop-all.sh是需要根据/tmp下面的pid文件关闭对应的进程，当/tmp下的文件被自动清理后可能会出出先的错误：123456789$ ./sbin/stop-all.shStopping namenodes on [master]master: no namenode to stopslave1: no datanode to stopslave2: no datanode to stopslave3: no datanode to stopStopping secondary namenodes [master]master: no secondarynamenode to stop…… 方法1：这时需要在/tmp文件夹下手动创建恢复这些pid文件master节点（每个文件中保存对应的进程id）：hadoop-hadoop-namenode.pidhadoop-hadoop-secondarynamenode.pidyarn-hadoop-resourcemanager.pidslave节点（每个文件中保存对应的进程id）：hadoop-hadoop-datanode.pidyarn-hadoop-nodemanager.pid方法2：使用kill -9逐个关闭相应的进程id 从根本上解决的方法：（首先使用了方法1或方法2关闭了hadoop集群）1.修改配置文件hadoop-env.sh: #export HADOOP_PID_DIR=${HADOOP_PID_DIR}export HADOOP_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids #export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}export HADOOP_SECURE_DN_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids 2.修改配置文件yarn-env.sh:export YARN_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids 3.创建文件夹pids：$ mkdir /data/hadoop-2.6.0-cdh5.5.0/pids（发现会自动创建pids文件，因此不需要创建）这2个步骤需要在各个节点都执行.","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}]},{"title":"搭建大数据平台系列(0)-机器准备","date":"2018-07-20T05:16:05.000Z","path":"2018/07/20/conf-os/","text":"0. 前期规划假设现在有四台机器，各自ip如下:1234192.168.1.201192.168.1.202192.168.1.203192.168.1.204 安装下面统一要求进行重装系统： 每台机器的系统为：CentOS-6.5-x86_64 每台机器的磁盘分区为： 1234/boot : 系统引导分区/ : 系统安装区/data : 系统数据分区（swap分区尚未建立，待机器内存不足需求时再建立） 每台机器上安装的都是Mininal Desktop版本 每台机器的主机名(hostname)分别为：master、slave1、slave2、slave3 每台机器的root密码都是:master 每台机器上建立的第一个非root用户都为:hadoop，密码为:hadoop 非root用户在centos中获取sudo权限的命令：123456[hadoop@slave3 ~]$ su - #切换到root用户，需要输入root密码[root@slave3 ~]# visudo -f /etc/sudoers## Allow root to run any commands anywhereroot ALL=(ALL) ALL在上面这条文字下加上如下：hadoop ALL=(ALL) ALL 这样配置后，hadoop用户可以使用sudo命令了 修改每台机器的/etc/hosts文件，如：12345[root@slave3 ~]$ sudo vi /etc/hosts192.168.1.201 master192.168.1.202 slave1192.168.1.203 slave2192.168.1.204 slave3 9.防火墙设置 因为4台机器组成的大数据平台小集群需要互相访问对方的多个端口，需要在防火墙中打开这些端口的访问（如果未打开的话），当然最方便的就是把每台机器上的防火墙关闭了。123456789101112131415161718192021222324[hadoop@master ~]$ sudo service iptables start #打开防火墙[hadoop@master ~]$ sudo service iptables status #查看防火墙[hadoop@master ~]$ sudo service iptables stop #关闭防火墙(注意：防火墙操作时需要root权限的，非root用户不使用sudo的话，不会报错，但操作无结果)修改防火墙设置(红色为新增配置：开放2000-6000,7000以上的端口):[hadoop@master ~]$ sudo vim /etc/sysconfig/iptables# Firewall configuration written by system-config-firewall# Manual customization of this file is not recommended.*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT-A INPUT -p icmp -j ACCEPT-A INPUT -i lo -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 2000:6000 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 7000: -j ACCEPT-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j REJECT --reject-with icmp-host-prohibitedCOMMIT保存上面的文件后，需要重启防火墙，让配置生效！[hadoop@master ~]$ sudo service iptables restart #重启防火墙","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}]},{"title":"Hexo+Github Page搭建个人博客","date":"2018-06-22T09:03:09.000Z","path":"2018/06/22/build-blog/","text":"什么是Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。堪称在座各位喜欢Markdown的优雅人士博客建站神器哟！ 1. Quick Start1.1 创建存放Github Pages的仓库Github Pages 是面向用户、组织和项目开放的公共静态页面搭建托管服务，站点可以被免费托管在 Github 上，你可以选择使用 Github Pages 提供的域名 github.io 或者自定义域名来发布站点。 需要Github账号，请登录https://github.com/ 注册。\b\b登录了自己的github账号后，可以安装下图一样，创建自己的GitHub Pages仓库名[参考https://pages.github.com/ ]，[PS] 仓库名repository name需要约定为: 你的账号名.github.io创建好博客项目仓库后，可以通过git命名下载到本地，并编辑一下README.md从本地提交到GitHub，这样做主要是使本地文件与Github关联起来，方便后面hexo deploy,直接部署博客内容到GitHub进行更新。12345678910$ git clone https://github.com/yourGithubName/yourGithubName.github.io$ vim README.md# REAMME.md上可以简单写一些博客介绍啥的$ git config --global user.email \"you@example.com\"$ git config --global user.name \"Your Name\"$ git add ./$ git commit -m 'test'$ git push -u origin masterUsername for 'https://github.com': Builder34Password for 'https://Builder34@github.com': 1.2 Hexo安装安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js (请看https://nodejs.org/zh-cn/) Git （请看https://git-scm.com/downloads）安装好上面2个程序后，可以进行hexo的安装了：1$ npm install -g hexo-cli 1.3 Hexo初始化安装 Hexo 完成后，我们可以在本地新建一个文件夹如：builder34.github.io(\b这个目录是我们Github Pages博客项目的目录),假如我的文件夹路径为/home/test/builder34.github.io，建站初始化命令可以如下:12345$ cd /home/test/builder34.github.io$ hexo init ./$ npm install$ hexo generate 下面介绍几个常用的hexo命令(括号里面的命令为缩写形式，效果一样)： 1. hexo generate(hexo g) #生成静态文件，会在当前目录下生成一个新的叫做public的文件夹 2. hexo new &quot;postTitle&quot; #新建博客文章 3. hexo new page &quot;pageTitle&quot; #新建1个页面 4. hexo server(hexo s) #启动本地web服务预览(加参数--debug,用于调试，如：hexo s --debug) 5. hexo deploy(hexo d) #部署播客到远端（比如Github,coding,heroku等平台） 在命令行中输入hexo s --debug后，运行成功后，可以在浏览器中输入：http://localhost:4000看到自己新建的博客了。 1.4 更改主题一般我们初始化博客的文件夹后，文件结构大概如下：12345678910111213$ lltotal 1352-rw-r--r-- 1 builder34 staff 32B 4 14 01:34 README.md-rw-r--r-- 1 builder34 staff 2.3K 6 25 10:40 _config.yml-rw-r--r-- 1 builder34 staff 32K 6 26 15:50 db.json-rw-r--r-- 1 builder34 staff 458K 6 26 15:56 debug.logdrwxr-xr-x 293 builder34 staff 9.2K 6 25 10:42 node_modules-rw-r--r-- 1 builder34 staff 110K 6 22 23:59 package-lock.json-rw-r--r-- 1 builder34 staff 564B 6 22 23:59 package.jsondrwxr-xr-x 14 builder34 staff 448B 6 25 10:40 publicdrwxr-xr-x 5 builder34 staff 160B 4 17 23:12 scaffoldsdrwxr-xr-x 3 builder34 staff 96B 6 25 10:57 sourcedrwxr-xr-x 6 builder34 staff 192B 6 25 11:33 themes themes文件夹是我们博客主题的存放地方，下面我推荐一个主题：BlueLake123456$ cd themes/$ git clone https://github.com/chaooo/hexo-theme-BlueLake.git ./BlueLake$ npm install hexo-renderer-jade@0.3.0 --save$ npm install hexo-renderer-stylus --save(该主题更细致的配置，请登录上面这个github网址，阅读README.md进行定制化配置） 在Hexo配置文件（$your_blog_path/_config.yml）中把主题设置修改为BlueLake。1theme: BlueLake 完成配置后，执行下面语句，重新打开http://localhost:4000 可以看到博客以一个新的主题展现了12$ hexo g$ hexo s --debug 1.5 hexo部署到Github配置$your_blog_path/_config.yml文件的Deployment：12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/your_githubName/your_githubName.github.io.git 通过下面的命名进行博客静态页面的生成，以及部署到远端Github Pages12345678#删除静态文件,即 public 文件$ hexo clean#生成静态文件,即 public 文件$ hexo generate#部署到远程站点$ hexo deploy#也可以使用组合命令(替代上面\b2条命令)：生成静态命令并部署到远程站点$ hexo deploy -g 12使用 hexo deploy 命名部署到github失败，报上面的错误时，安装下面的插件即可解决:$ npm install hexo-deployer-git --save 至此，Hexo+Github Pages构建个人博客网站已经\b基本完成了。可以通过网页访问自己的博客地址如\b：https://builder34.github.io 2.设置博客自定义域名进入自己博客的repository仓库，通过类似如下的页面进行设置：进入了settings页面后，往下拉直到看到Github Pages模块： 所填的自定义域名是需要自己已经在万网上注册的了，并且如果\b勾选了 Enforce HTTPS 的话，你的域名是需要ssl证书的哟。 3.注意事项3.1 上传README.md并防止被渲染成文章123456#在\b博客根目录下，新建或编辑你的README.md文件$ vim README.md$ mv README.md ./sources#修改_config.yml文件,设置不渲染的文件$ vim _config.ymlskip_render: README.md 3.2 每次\bhexo deploy后Github Pages自定义域名会被重置的问题需要在\bsources目录下新建CNAME文件(注意为全大写无后缀的文件哦),文件\b内容为你需要映射到的自定义域名：1234$ vim CNAMEblog.monbuilder.top$ mv CNAME ./sources","tags":[{"name":"hexo","slug":"hexo","permalink":"https://blog.monbuilder.top/tags/hexo/"}]},{"title":"Hello World","date":"2018-04-13T16:27:08.379Z","path":"2018/04/14/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]