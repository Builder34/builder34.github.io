{"meta":{"title":"抹布先生M","subtitle":"记录技术与生活,keep coding","description":"Builder34的个人博客...","author":"Builder Luo","url":"https://blog.monbuilder.top"},"pages":[{"title":"标签","date":"2018-12-16T10:26:27.591Z","updated":"2018-12-16T10:26:27.591Z","comments":false,"path":"index.html","permalink":"https://blog.monbuilder.top/index.html","excerpt":"","text":""},{"title":"关于","date":"2018-12-16T10:26:27.589Z","updated":"2018-12-16T10:26:27.588Z","comments":false,"path":"about/index.html","permalink":"https://blog.monbuilder.top/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2018-12-16T10:26:27.589Z","updated":"2018-12-16T10:26:27.589Z","comments":false,"path":"books/index.html","permalink":"https://blog.monbuilder.top/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-01-10T14:44:50.009Z","updated":"2018-12-16T10:26:27.590Z","comments":true,"path":"links/index.html","permalink":"https://blog.monbuilder.top/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-12-16T10:26:27.590Z","updated":"2018-12-16T10:26:27.590Z","comments":false,"path":"repository/index.html","permalink":"https://blog.monbuilder.top/repository/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-12-16T10:26:27.588Z","updated":"2018-12-16T10:26:27.588Z","comments":false,"path":"categories/index.html","permalink":"https://blog.monbuilder.top/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-12-16T10:28:20.231Z","updated":"2018-12-16T10:28:20.231Z","comments":false,"path":"tags/index.html","permalink":"https://blog.monbuilder.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Java常用基础工具库","slug":"toolkit01","date":"2019-04-28T05:38:32.000Z","updated":"2019-05-06T03:49:08.305Z","comments":true,"path":"2019/04/28/toolkit01/","link":"","permalink":"https://blog.monbuilder.top/2019/04/28/toolkit01/","excerpt":"","text":"Java常用基础工具库下面涉及到的示例代码，皆可以通过Github地址https://github.com/Builder34/toolkit-demo 获取. 通用工具类(字符串、时间格式化、BeanUtils、IO)1. commons-lang3库1.1. org.apache.commons.lang3.StringUtils类日常代码中，我们经常和String字符串打交道，经常对字符串进行处理，稍微不注意的话，很容易出现类似NullPointerException这种简单的错误，我们经常写各种if来判断处理这些非业务的逻辑。这时，我们可以利用大牛apache的轮子，通过其StringUtils里面的一些常用方法，改善我们的代码，让我们的业务代码更简洁、优雅。示例代码： 12345678910111213141516171819@Slf4jpublic class AppTest &#123; @Test public void stringUtils()&#123; String a = \" \"; String b = null; //判断字符对象是否为空以及内容是否为空串(有空格则认为不是空串) log.info(\"StringUtils.isEmpty(a): &#123;&#125;\", StringUtils.isEmpty(a)); //判断字符对象是否为空以及内容是否为空串(有空格也会认为是空串) log.info(\"StringUtils.isBlank(a): &#123;&#125;\", StringUtils.isBlank(a)); //当b=null时，如果b.trim()则会报空指针异常，使用StringUtils.trim(b)可以避免 log.info(\"StringUtils.trim(d): &#123;&#125;\", StringUtils.trim(b)); String num = \"12.3\"; //当b=null时，如果b.trim()则会报空指针异常，使用StringUtils.trim(b)可以避免 log.info(\"org.apache.commons.lang3.StringUtils.isNumericSpace(): &#123;&#125; isNumber: &#123;&#125;\", num, StringUtils.isNumericSpace(b)); log.info(\"com.alibaba.druid.util.StringUtils.isNumber(): &#123;&#125; isNumber: &#123;&#125;\", num, com.alibaba.druid.util.StringUtils.isNumber(num)); &#125;&#125; 注意：common-lang3中的StringUtils. isNumeric()或isNumericSpace()并不能判断字符串中带小数点的数字值字符为数字。可以通过com.alibaba.druid.util.StringUtils.isNumber(str)，此方法来判断。 1.2 org.apache.commons.lang3.time.DateFormatUtils/DateUtils时间转换工具类: 12345678@Test public void dateFormatUtils() throws Exception&#123; String pattern = \"yyyy-MM-dd HH:mm:ss\"; String timeStr = DateFormatUtils.format(new Date(), pattern); long timestamp = DateUtils.parseDate(timeStr, pattern).getTime(); log.info(\"==&gt; current time: &#123;&#125;\", timeStr); log.info(\"==&gt; current time timestamp: &#123;&#125;\", timestamp); &#125; 总结 StringUtils.isEmpty(str)/StringUtils.isNotEmpty(str): 判断字符对象是否为null或空串(有空格则认为不是空串) StringUtils.isBlank(str)/StringUtils.isNotBlank(str): 判断字符对象是否为null或空串(有空格也会认为是空串) DateFormatUtils.format(date, pattern): 将Date时间对象按表达式的格式转换成时间字符串 DateUtils.parseDate(timeStr, pattern): 将时间字符串反转成Date对象 ToStringBuilder.reflectionToString(obj): 将对象内容转换成字符串输出(下一节有使用到) … 对于学习某个工具类，我们可以通过Intellij IDEA中可通过打开此类的源代码，然后通过快捷键(MacOS: command+7; Windows: Alt+7)打开查看类方法列表(Structure)，从方法名字上大概可以看出具体有那些适合自己使用的方法。 以上示例使用到的jar包可通过maven的pom.xml文件依赖导入: 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.9&lt;/version&gt; &lt;/dependency&gt; 2.common-beanutils库Map、JavaBean是我们日常业务代码中经常使用到的2种类，有时因为业务原因，Map、JavaBean需要相互转换copy啥的操作时，如果手动set/put，字段多的时候，就要吐血了。这里我们推荐使用BeanUtils来简化我们的代码 2.1.org.apache.commons.beanutils.BeanUtils类12345678910111213141516171819202122232425262728@Test public void beanUtils() throws InvocationTargetException, IllegalAccessException &#123; CompanyBean bean = new CompanyBean(); bean.setId(1); bean.setName(\"中国移动广州分公司\"); bean.setAddress(\"广州市天河区中山大道\"); bean.setTel(\"020-10086000\"); CompanyBean destObj = new CompanyBean(); //复制bean之间复制内容, 新对象destObj需要先实例化 BeanUtils.copyProperties(destObj, bean); //ToStringBuilder类来自commons-lang3库：将对象内容转换成字符串输出，方便于日志输出 log.info(\"destObj from BeanUtils.copyProperties: &#123;&#125;\", ToStringBuilder.reflectionToString(destObj)); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"id\", 2); map.put(\"name\", \"中国联通广州分公司\"); map.put(\"address\", \"广州市天河区中山大道2号\"); map.put(\"tel\", \"020-10000110\"); //将map(key,value)映射成bean BeanUtils.populate(destObj, map); log.info(\"destObj from BeanUtils.populate: &#123;&#125;\", ToStringBuilder.reflectionToString(destObj)); //复制对象，与copyProperties()方法比较，这里新对象可以不先实例化 CompanyBean cloneBean = (CompanyBean)BeanUtils.cloneBean(destObj); log.info(\"cloneBean from BeanUtils.cloneBean: &#123;&#125;\", ToStringBuilder.reflectionToString(cloneBean)); //将JavaBean转换成Map Map newMap = BeanUtils.describe(cloneBean); log.info(\"newMap from BeanUtils.describe: &#123;&#125;\", new Gson().toJson(newMap)); &#125; 日志输出如下： 1232019-01-19 13:17:21.064 [main] INFO com.monbuilder.AppTest - destObj from BeanUtils.copyProperties: com.monbuilder.bean.CompanyBean@10683d9d[id=1,name=中国移动广州分公司,address=广州市天河区中山大道1号,tel=020-10086000]2019-01-19 13:17:21.070 [main] INFO com.monbuilder.AppTest - destObj from BeanUtils.populate: com.monbuilder.bean.CompanyBean@10683d9d[id=2,name=中国联通广州分公司,address=广州市天河区中山大道2号,tel=020-10000110]2019-01-19 13:48:14.966 [main] INFO com.monbuilder.AppTest - newMap from BeanUtils.describe: &#123;\"address\":\"广州市天河区中山大道2号\",\"name\":\"中国联通广州分公司\",\"tel\":\"020-10000110\",\"id\":\"2\",\"class\":\"class com.monbuilder.bean.CompanyBean\"&#125; 总结 BeanUtils.copyProperties(destObj, sourceObj): JavaBean之间内容的复制 BeanUtils.cloneBean(obj): 复制对象 BeanUtils.populate(destObj, sourceMap): Map转换成JavaBean BeanUtils.describe(bean): 将JavaBean转换成Map 3.commons-io库org.apache.commons.io.IOUtils类这个io工具类非常有用，当我们在处理流的过程中，经常需要把流与字节数组之间相互转换，以及在处理完之后，关闭流等等这些操作时，我们需要写挺多处理逻辑，close时还需要写if判空啥的，但是使用了这个IOUtil后，我们的处理代码或简洁非常多的。 12345678@Test public void ioUtils() throws IOException &#123; InputStream io = this.getClass().getClassLoader().getResourceAsStream(\"README.md\"); BufferedReader br = new BufferedReader(new InputStreamReader(io)); log.info(\"==&gt; IOUtils.toString(br): &#123;&#125;\", IOUtils.toString(br)); IOUtils.closeQuietly(br); IOUtils.closeQuietly(io); &#125; 12019-01-19 14:29:26.140 [main] INFO com.monbuilder.AppTest - ==&gt; IOUtils.toString(br): toolkit-demo,工具类库使用示例 上面只是简单的展示将文件流内容转换成字符串，之后再关闭流，是不是非常简洁呢？IOUtils里面还有非常多的好方法可以使用，这些可以根据自己在具体的工作场景下，查看IOUtils的方法列表，找到自己需要的方法 总结IOUtils常用的方法有： IOUtils.closeQuietly(obj): 可关闭流\\Socket\\SocketServer等多种对象 IOUtils.copy(InputStream, Writer): 复制输入流 IOUtils.write(byte[], OutputStream): 将字节数组转换成流 IOUtils.toByteArray(InputStream): 将输入流转换成字节数组 IOUtils.toInputStream(String): 将字符串转换成输入流 IOUtils.toString(InputStream): 将输入流转换成字符串 上面介绍的都是来自于apache官方的类库，还有好多类与方法有待我们发掘使用。另外，还介绍另1个非常有名的工具类库：guava，来自于Google；功能也有类似的，当然也有很多的扩展使用，本文就不再详细介绍了。 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;23.2-jre&lt;/version&gt; &lt;/dependency&gt; JSON工具类在如今REST API盛行的年代，前后端分离成为标配，json成为两者之间的传输桥梁，在我们工作做打交道肯定非常频繁咯。在Java开发中，JSON工具类的选择并不少： fastjson 阿里巴巴出品，转换快，但表现不够稳定 Gson google出品，使用简单，轻量 Jackson spring官方使用，性能快，转换快，配置更灵活，不同场景下表现更稳定 比较推荐使用的是Jackson，如果我们使用Spring Boot或Spring Cloud时，构建web项目里面内置的json库就是Jackson库。 下面通过Jackson库简单封装成JavaBean/json互转的工具类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.monbuilder.util;import com.fasterxml.jackson.annotation.JsonInclude;import com.fasterxml.jackson.core.JsonParser;import com.fasterxml.jackson.databind.DeserializationFeature;import com.fasterxml.jackson.databind.ObjectMapper;import lombok.extern.slf4j.Slf4j;import java.text.SimpleDateFormat;/*** * JSON转换工具类 * @author &lt;a href=\"mailto:lcbiao34@gmail.com\"&gt;Builder34&lt;/a&gt; * @date 2018-11-01 11:14:26 * */@Slf4jpublic class JacksonUtil &#123; private static ObjectMapper objectMapper = new ObjectMapper(); static &#123; objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_CONTROL_CHARS, true); objectMapper.configure(JsonParser.Feature.ALLOW_SINGLE_QUOTES, true); objectMapper.configure(JsonParser.Feature.ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER, true); objectMapper.setDateFormat(new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")); // 设置输入时忽略JSON字符串中存在而Java对象实际没有的属性 objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); //设置不输出值为 null 的属性 objectMapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); &#125; /** * 将JSON字符串根据指定的Class反序列化成Java对象(转换过程中出现异常则返回null对象) * * @param json JSON字符串 * @param objClass JavaObject对象Class * @return 反序列化生成的Java对象 * */ public static &lt;T&gt; T toJavaObject(String json, Class&lt;T&gt; objClass) &#123; try &#123; return objectMapper.readValue(json, objClass); &#125; catch (Exception e) &#123; log.error(\"\", e); return null; &#125; &#125; /** * 将Java对象序列化成JSON字符串(转换过程中出现异常则返回空对象json串\"&#123;&#125;\") * * @param obj 待序列化生成JSON字符串的Java对象 * @return JSON字符串 */ public static String toJsonString(Object obj) &#123; try &#123; return objectMapper.writeValueAsString(obj); &#125; catch (Exception e) &#123; log.error(\"\",e); &#125; return \"&#123;&#125;\"; &#125;&#125; 运用上面的工具类，示例： 123456789101112@Test public void jsonUtils()&#123; CompanyBean bean = new CompanyBean(); bean.setId(1); bean.setName(\"中国移动广州分公司\"); bean.setAddress(\"广州市天河区中山大道1号\"); bean.setTel(\"020-10086000\"); log.info(\"==&gt; JacksonUtil.toJsonString(bean): &#123;&#125;\", JacksonUtil.toJsonString(bean)); String json = JacksonUtil.toJsonString(bean); log.info(\"==&gt; JacksonUtil.toJavaObject: &#123;&#125;\", ToStringBuilder.reflectionToString(JacksonUtil.toJavaObject(json, CompanyBean.class))); &#125; 122019-01-19 15:42:16.081 [main] INFO com.monbuilder.AppTest - ==&gt; JacksonUtil.toJsonString(bean): &#123;\"id\":1,\"name\":\"中国移动广州分公司\",\"address\":\"广州市天河区中山大道1号\",\"tel\":\"020-10086000\"&#125;2019-01-19 15:42:16.144 [main] INFO com.monbuilder.AppTest - ==&gt; JacksonUtil.toJavaObject: com.monbuilder.bean.CompanyBean@376a0d86[id=1,name=中国移动广州分公司,address=广州市天河区中山大道1号,tel=020-10086000] 最后想要说的是，轮子非常多，当然我们可以重复造轮子，但是我们更提倡的是使用已有优秀的轮子(工具类)，开发出更简洁、优雅、业务逻辑更清晰的代码。","categories":[{"name":"commons utils","slug":"commons-utils","permalink":"https://blog.monbuilder.top/categories/commons-utils/"}],"tags":[{"name":"Java工具库","slug":"Java工具库","permalink":"https://blog.monbuilder.top/tags/Java工具库/"}],"keywords":[{"name":"commons utils","slug":"commons-utils","permalink":"https://blog.monbuilder.top/categories/commons-utils/"}]},{"title":"Elasticsearch学习教程系列(2)-命令学习(二)批处理、数据操作、搜索","slug":"elasticsearch-learn02","date":"2019-03-14T12:08:49.000Z","updated":"2019-03-14T11:10:37.846Z","comments":true,"path":"2019/03/14/elasticsearch-learn02/","link":"","permalink":"https://blog.monbuilder.top/2019/03/14/elasticsearch-learn02/","excerpt":"","text":"# 上一篇文章中，我们介绍了Elasticsearch集群运行情况、索引、文档的CRUD操作了，下面让我们来愉快地学习一些新的命令吧。 批处理Elasticsearch除了能够索引，更新和删除单个文档之外，还提供了使用_bulkAPI批量执行上述任何操作的功能。此功能非常重要，因为它提供了一种非常有效的机制，可以尽可能快地执行多个操作，并尽可能少地进行网络往返。作为一个简单示例，以下调用在一个批量操作中索引两个文档（ID 1 - John Doe和ID 2 - Jane Doe）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[builder @master~] $ curl - X POST \"localhost:9200/customer/_doc/_bulk?pretty\" - H 'Content-Type: application/json' - d '&#123; \"index\": &#123; \"_id\": \"1\" &#125;&#125;&#123; \"name\": \"John Doe\"&#125;&#123; \"index\": &#123; \"_id\": \"2\" &#125;&#125;&#123; \"name\": \"Jane Doe\"&#125;'&#123; \"took\": 19, \"errors\": false, \"items\": [ &#123; \"index\": &#123; \"_index\": \"customer\", \"_type\": \"_doc\", \"_id\": \"1\", \"_version\": 6, \"result\": \"updated\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 5, \"_primary_term\": 2, \"status\": 200 &#125; &#125;, &#123; \"index\": &#123; \"_index\": \"customer\", \"_type\": \"_doc\", \"_id\": \"2\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 2, \"status\": 201 &#125; &#125;]&#125; 下面示例更新第一个文档（ ID为1）， 然后在一个批量操作中删除第二个文档（ ID为2）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[builder @master~] $ curl - X POST \"localhost:9200/customer/_doc/_bulk?pretty\" - H 'Content-Type: application/json' - d '&#123; \"index\": &#123; \"_id\": \"1\" &#125;&#125;&#123; \"name\": \"John Doe\"&#125;&#123; \"index\": &#123; \"_id\": \"2\" &#125;&#125;&#123; \"name\": \"Jane Doe\"&#125;'&#123; \"took\": 15, \"errors\": false, \"items\": [ &#123; \"index\": &#123; \"_index\": \"customer\", \"_type\": \"_doc\", \"_id\": \"1\", \"_version\": 7, \"result\": \"updated\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 6, \"_primary_term\": 2, \"status\": 200 &#125; &#125;, &#123; \"index\": &#123; \"_index\": \"customer\", \"_type\": \"_doc\", \"_id\": \"2\", \"_version\": 2, \"result\": \"updated\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 1, \"_primary_term\": 2, \"status\": 200 &#125; &#125;]&#125; 请注意，对于删除操作，之后没有相应的源文档，因为删除只需要删除文档的ID。Bulk API不会因其中一个操作失败而失败。如果单个操作因任何原因失败，它将继续处理其后的其余操作。批量API返回时，它将为每个操作提供一个状态（按照发送的顺序），以便您可以检查特定操作是否失败。 数据操作导入数据下面我们在某个文件夹中保存着1个json文件，内容如下： 12345678[builder@master ~/Developer/esTempData]$ cat accounts.json&#123;\"index\":&#123;\"_id\":\"1\"&#125;&#125;&#123;\"account_number\":1,\"balance\":39225,\"firstname\":\"Amber\",\"lastname\":\"Duke\",\"age\":32,\"gender\":\"M\",\"address\":\"880 Holmes Lane\",\"employer\":\"Pyrami\",\"email\":\"amberduke@pyrami.com\",\"city\":\"Brogan\",\"state\":\"IL\"&#125;&#123;\"index\":&#123;\"_id\":\"6\"&#125;&#125;&#123;\"account_number\":6,\"balance\":5686,\"firstname\":\"Hattie\",\"lastname\":\"Bond\",\"age\":36,\"gender\":\"M\",\"address\":\"671 Bristol Street\",\"employer\":\"Netagy\",\"email\":\"hattiebond@netagy.com\",\"city\":\"Dante\",\"state\":\"TN\"&#125;...# 我们的json文件中有1000条数据，上面只展示2条数据 导入我们的json文件数据到Elasticsearch，需要在json文件的当前路径下执行： 12[builder@master ~/Developer/esTempData]$ curl -H \"Content-Type: application/json\" -XPOST \"localhost:9200/bank/_doc/_bulk?pretty&amp;refresh\" --data-binary \"@accounts.json\"... 123456## 查看索引数据，可以看到第2行的数据有1000条记录了[builder@master ~]$ curl -X GET \"http://localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open .kibana_1 WShElb71RVigvhHRsU5vIA 1 0 3 0 11.9kb 11.9kbyellow open bank LLrKZKoNT-ifFpiv-dBc9w 5 1 1000 0 474.6kb 474.6kbyellow open customer jrkOIUCjTLec7_5OcwldYw 5 1 3 0 10.9kb 10.9kb 搜索API搜索有两种基本方式：一种是通过发送搜索参数REST请求URI和其他通过发送他们REST请求JSON主体。请求JSON体方法更具表现力，并以更可读的JSON格式定义搜索。我们将尝试一个请求URI方法的示例，但是对于本教程的其余部分，我们将专门使用请求体方法。 可以从_search端点访问用于搜索的REST API 。此示例返回bank索引中的所有文档： 1[builder@master ~]$ curl -X GET 'localhost:9200/bank/_search?q=*&amp;sort=account_number:asc&amp;pretty' 该q=*参数指示Elasticsearch匹配索引中的所有文档。该sort=account_number:asc参数指示使用account_number每个文档的字段以升序对结果进行排序。该pretty参数再次告诉Elasticsearch返回漂亮的JSON结果。 响应（部分显示）： 123456789101112131415161718192021222324252627282930&#123; \"took\" : 63, \"timed_out\" : false, \"_shards\" : &#123; \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 &#125;, \"hits\" : &#123; \"total\" : 1000, \"max_score\" : null, \"hits\" : [ &#123; \"_index\" : \"bank\", \"_type\" : \"_doc\", \"_id\" : \"0\", \"sort\": [0], \"_score\" : null, \"_source\" : &#123;\"account_number\":0,\"balance\":16623,\"firstname\":\"Bradshaw\",\"lastname\":\"Mckenzie\",\"age\":29,\"gender\":\"F\",\"address\":\"244 Columbus Place\",\"employer\":\"Euron\",\"email\":\"bradshawmckenzie@euron.com\",\"city\":\"Hobucken\",\"state\":\"CO\"&#125; &#125;, &#123; \"_index\" : \"bank\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"sort\": [1], \"_score\" : null, \"_source\" : &#123;\"account_number\":1,\"balance\":39225,\"firstname\":\"Amber\",\"lastname\":\"Duke\",\"age\":32,\"gender\":\"M\",\"address\":\"880 Holmes Lane\",\"employer\":\"Pyrami\",\"email\":\"amberduke@pyrami.com\",\"city\":\"Brogan\",\"state\":\"IL\"&#125; &#125;, ... ] &#125;&#125; 关于response响应字段含义： took - Elasticsearch执行搜索的时间（以毫秒为单位） timed_out - 告诉我们搜索是否超时 _shards - 告诉我们搜索了多少个分片，以及搜索成功/失败分片的计数 hits - 搜索结果 hits.total - 符合我们搜索条件的文档总数 hits.hits - 实际的搜索结果数组（默认为前10个文档） hits.sort - 对结果进行排序键（如果按分数排序则丢失） hits._score并max_score- 暂时忽略这些字段 下面使用JSON请求体的方法完成上述相同的搜索： 123456789$ curl -X GET 'localhost:9200/bank/_search' -d '&#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;, \"sort\": [ &#123; \"account_number\": \"asc\" &#125; ]&#125;' -H \"Content-Type:application/json\"# response 报文:&#123;\"took\":14,\"timed_out\":false,\"_shards\":&#123;\"total\":5,\"successful\":5,\"skipped\":0,\"failed\":0&#125;,\"hits\":&#123;\"total\":1000,\"max_score\":null,\"hits\":[&#123;\"_index\":\"bank\",\"_type\":\"_doc\",\"_id\":\"0\",\"_score\":null,\"_source\":&#123;\"account_number\":0,\"balance\":16623,\"firstname\":\"Bradshaw\",\"lastname\":\"Mckenzie\",\"age\":29,\"gender\":\"F\",\"address\":\"244 Columbus Place\",\"employer\":\"Euron\",\"email\":\"bradshawmckenzie@euron.com\",\"city\":\"Hobucken\",\"state\":\"CO\"&#125;,\"sort\":[0]&#125;... 好啦，本文暂时介绍这几个命令的简单使用方法。更多elasticsearch的相关知识我们在后面再娓娓道来…","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://blog.monbuilder.top/tags/Elasticsearch/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}]},{"title":"Elasticsearch学习教程系列(1)-命令学习(一) 集群健康、索引、文档操作","slug":"elasticsearch-learn01","date":"2019-03-14T04:15:22.000Z","updated":"2019-03-14T03:07:38.915Z","comments":true,"path":"2019/03/14/elasticsearch-learn01/","link":"","permalink":"https://blog.monbuilder.top/2019/03/14/elasticsearch-learn01/","excerpt":"","text":"本教程是基于Elasticsearch6.5版本编写在本系列教程上一篇文章中，我们介绍了Elasticsearch的一些基本概念、安装并运行起了一个Elasticsearch节点。现在我们已经启动并运行了节点（集群），下一步是了解如何与它进行通信。幸运的是，Elasticsearch提供了一个非常全面和强大的REST API，您可以使用它与集群进行交互。使用API 可以完成的一些事项如下： 检查集群运行情况，状态和统计信息 管理您的群集，节点和索引数据和元数据 对索引执行CRUD（创建，读取，更新和删除）和搜索操作 执行高级搜索操作，例如分页，排序，过滤，脚本编写，聚合等等 集群运行情况(Cluster Health)让我们从基本运行状况检查开始，我们可以使用它来查看集群的运行情况。我们将使用curl来执行此操作，但您可以使用任何允许您进行HTTP / REST调用的工具。假设我们仍然在我们启动Elasticsearch的同一节点上打开另一个命令shell窗口。‘’[builder@master ~]$ curl -X GET “http://localhost:9200/_cat/health?v&quot;‘’ epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent‘’ 1547394013 15:40:13 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0%‘’我们可以看到名为“elasticsearch”的群集处于green(绿色)状态。每当我们要求群集健康时，我们要么获得绿色，黄色或红色。 绿色 - 一切都很好（集群功能齐全）黄色 - 所有数据均可用，但尚未分配一些副本（群集功能齐全）红色 - 某些数据由于某种原因不可用（群集部分功能）注意：当群集为红色时，它将继续提供来自可用分片的搜索请求，但您可能需要尽快修复它，因为存在未分配的分片。同样从上面的响应中，我们可以看到总共1个节点，并且我们有0个分片，因为我们还没有数据。请注意，由于我们使用的是默认群集名称（elasticsearch），并且由于Elasticsearch默认使用单播网络发现来查找同一台计算机上的其他节点，因此您可能会意外启动计算机上的多个节点并将它们所有都加入一个集群。在这种情况下，您可能会在上面的响应中看到多个节点。查看集群中的节点列表：‘’[builder@master ]$ curl -X GET “http://localhost:9200/_cat/nodes?v&quot;‘’ ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name‘’ 127.0.0.1 20 98 8 1.90 mdi * siwsSwZ从上面的结果中，我们可以看到一个名为“siwsSwZ”的节点，它是我们集群中当前的单个节点。 索引(indices)的CRUD、查询操作查看索引列表： 12[builder@master ~]$ curl -X GET \"http://localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 因为我们的节点是刚刚建立的，还没数据，所以上面查询的结果只有一行字段名，没有索引数据 创建索引现在让我们创建一个名为“customer”的索引，然后再次列出所有索引（下面第一个命令使用PUT动词创建名为“customer”的索引。加?pretty表示返回的结果打印格式化过的JSON（如果有的话）。）： 123456789[builder@master ~]$ curl -X PUT \"localhost:9200/customer?pretty\" &#123; \"acknowledged\" : true, \"shards_acknowledged\" : true, \"index\" : \"customer\" &#125;[builder@master ~]$ curl -X GET \"localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open customer i7R-XRH8R0Kn7uLQRNC_yQ 5 1 0 0 1.1kb 1.1kb 第二个命令的结果告诉我们，我们现在有一个名为customer的索引，它有5个主分片和1个副本（默认值），并且它包含0个文档。您可能还注意到客户索引标记了黄色运行状况。回想一下我们之前的讨论，黄色表示某些副本尚未（尚未）分配。此索引发生这种情况的原因是因为默认情况下Elasticsearch为此索引创建了一个副本。由于我们目前只有一个节点在运行，因此在另一个节点加入集群的较晚时间点之前，尚无法分配一个副本（用于高可用性）。将该副本分配到第二个节点后，此索引的运行状况将变为绿色。 Tips: 上面命令中，-X PUT 后的路径不要带http哟，否则会出现如下错误的： 12345[builder@master ~]$ curl -X PUT \"http://localhost:9200/_cat/customer1?pretty\" &#123; \"error\" : \"Incorrect HTTP method for uri [/_cat/customer1?pretty] and method [PUT], allowed: [POST]\", \"status\" : 405 &#125; 查询文档现在让我们在customer索引中加入一些内容。我们将一个简单的客户文档索引到客户索引中，ID为1，如下所示： 123456789101112131415[builder@master ~]$ curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Builder Luo\"&#125;' &#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 0, \"_primary_term\" : 2 &#125; curl命令说明：-X为请求方式，-H 参数为设置请求头，-d参数为请求参数 。‘’ Tips: 关于：路径localhost:9200/customer/_doc/1?pretty‘’ localhost:9200/customer是之前创建的索引，后接/_doc/1表示为在customer索引上创建1个ID为1的文档(?pretty表示将返回的响应json格式化美观)提示：值得注意的是，Elasticsearch在你将文档编入索引之前不需要先显式创建索引。在前面的示例中，如果customer索引事先尚未存在，则Elasticsearch将自动创建customer索引。查看刚刚创建好的文档: 1234567891011[builder@master ~]$ curl -X GET \"localhost:9200/customer/_doc/1?pretty\"&#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"_source\" : &#123; \"name\" : \"Builder Luo\" &#125;&#125; 删除索引123456[builder@master ~]$ curl -X DELETE \"localhost:9200/customer?pretty\"&#123; \"acknowledged\" : true&#125;[builder@master ~]$ curl -X GET \"localhost:9200/_cat/indices?v\"health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 上面意味着索引已成功删除，我们现在回到我们在集群中没有任何内容的地方。在我们继续之前，让我们再仔细看看到目前为止我们学到的一些API命令： 1234567curl -X PUT ''localhost:9200/customer''curl -X GET ''localhost:9200/customer''curl -X DELETE ''localhost:9200/customer''curl -X PUT ''localhost:9200/customer/_doc/1'' -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Builder Luo\"&#125;'curl -X GET ''localhost:9200/customer/_doc/1''curl -X DELETE ''localhost:9200/customer/_doc/1'' 如果我们仔细研究上述命令，我们实际上可以看到我们如何在Elasticsearch中访问数据的模式。该模式可归纳如下： 1&lt;HTTP Verb&gt; Node_address/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt; 这种REST访问模式在所有API命令中都非常普遍，如果你能记住它，你将在掌握Elasticsearch方面有一个良好的开端。 替换文档Elasticsearch几乎实时提供数据操作和搜索功能。默认情况下，从索引/更新/删除数据到搜索结果中显示的时间，您可能会有一秒钟的延迟（刷新间隔）。数据在事务完成后立即可用，这是Elasticsearch与SQL等其他平台的重要区别之前，我们执行过如下命令： 1234567891011121314151617''[builder@master ~]$ curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Builder Luo\"&#125;' 下面，我们执行一样的命令，文档ID还是指定为1，只是-d参数内容有变化：''[builder@master ~]$ curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Baby Mon\"&#125;' ''&#123;'' \"_index\" : \"customer\",'' \"_type\" : \"_doc\",'' \"_id\" : \"1\",'' \"_version\" : 2,'' \"result\" : \"updated\",'' \"_shards\" : &#123;'' \"total\" : 2,'' \"successful\" : 1,'' \"failed\" : 0'' &#125;,'' \"_seq_no\" : 1,'' \"_primary_term\" : 1'' &#125; 以上内容将ID为1的文档名称从“Builder Luo”更改为“Baby Mon”, result字段值为: updated。另一方面，如果我们使用不同的ID，则会对新文档编制索引，并且索引中已有的现有文档保持不变。索引时，ID部分是可选的。如果未指定，Elasticsearch将生成随机ID，然后使用它来索引文档。Elasticsearch生成的实际ID（或前面示例中显式指定的内容）将作为索引API调用的一部分返回。此示例显示如何在没有显式ID的情况下索引文档： 123456789101112131415[builder@master ~]$ curl -X POST \"localhost:9200/customer/_doc?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"name\": \"Moonlight Chen\"&#125;' &#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"UmGaVGgBol3Y-BIhbtd0\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 0, \"_primary_term\" : 1 &#125; 可以看到上面自动生成的ID是：UmGaVGgBol3Y-BIhbtd0请注意，在上面的情况中，我们使用POST动词而不是PUT，因为我们没有指定ID。 修改文档数据除了能够重新索引和替换文档，我们还可以更新文档数据。（请注意，Elasticsearch实际上并没有在内部进行就地更新。每当我们进行更新时，Elasticsearch都会删除旧文档，然后一次性对应用了更新的新文档编制索引。） 1234567891011121314151617181920212223242526272829303132333435# -d参数，需要封装成： &#123; \"doc\": &#123;dataJSON&#125;&#125;[builder@master ~]$ curl -X POST \"localhost:9200/customer/_doc/1/_update?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"doc\": &#123; \"name\": \"My Baby Mon\", \"age\": 20&#125; &#125;' &#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 4, \"result\" : \"updated\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 3, \"_primary_term\" : 1&#125;# 也可以使用简单脚本执行更新。此示例使用脚本将年龄增加5：[builder@master ~]$ curl -X POST \"localhost:9200/customer/_doc/1/_update?pretty\" -H \"Content-Type:application/json\" -d '&#123; \"script\": \"ctx._source.age += 5\" &#125;'&#123; \"_index\" : \"customer\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 5, \"result\" : \"updated\", \"_shards\" : &#123; \"total\" : 2, \"successful\" : 1, \"failed\" : 0 &#125;, \"_seq_no\" : 4, \"_primary_term\" : 1 &#125;# 在上面的示例中，ctx._source指的是即将更新的当前源文档。 Elasticsearch提供了在给定查询条件（如SQL UPDATE-WHERE语句）的情况下更新多个文档的功能。请参阅[docs-update-by-queryAPI[https://www.elastic.co/guide/en/elasticsearch/reference/6.5/docs-update-by-query.html]]本文到这里，我们主要介绍了Elasticsearch的索引、文档的一些主要命令，我们将在下一篇文章中介绍Elasticsearch的批处理命令、以及探索操作数据的命令","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://blog.monbuilder.top/tags/Elasticsearch/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}]},{"title":"搭建大数据平台系列(4)-hive环境搭建","slug":"hive","date":"2019-03-06T14:19:33.000Z","updated":"2019-03-06T14:53:01.208Z","comments":true,"path":"2019/03/06/hive/","link":"","permalink":"https://blog.monbuilder.top/2019/03/06/hive/","excerpt":"","text":"0.准备步骤Hive 是依赖在Hadoop上的，所以他的安装不需要像Hadoop或者spark那样每个节点都安装一遍，只需在Hadoop的master节点上安装一个即可。Hive的安装前，需要Hadoop的环境，以及Mysql。 1.安装过程###1.1下载并解压安装包 1234567891011121314#下载hive-1.1.0-cdh5.5.0.tar.gz到master机器的~/bigdataspacce文件夹下#解压安装包的命令：[hadoop@master ~]$ cd ~/bigdataspacce[hadoop@master bigdataspace]$ tar -zxvf hive-1.1.0-cdh5.5.0.tar.gz#解压完成后删除压缩包：[hadoop@master bigdataspace]$ rm hive-1.1.0-cdh5.5.0.tar.gz#配置HIVE_HOME环境变量[hadoop@master ~]$ sudo vi /etc/profile(添加配置内容如下,红色为需要新增的配置)export HIVE_HOME=/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin:$PATH#让环境变量生效[hadoop@master ~]$ source /etc/profile ###1.2修改hive-env.sh配置文件 123456[hadoop@master ~]$ cd /home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf[hadoop@master conf]$ cp hive-env.sh.template hive-env.sh[hadoop@master conf]$ vi hive-env.sh#在hive-env.sh配置文件末尾加上:export HADOOP_HOME=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0export HIVE_CONF_DIR=/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf ###1.3新建hive-site.xml配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[hadoop@master conf]$ vi hive-env.sh##主要的配置内容如下：&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/hive-db/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/tmp/hive-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/tmp/$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/downloaded&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/data/hive-1.1.0-cdh5.5.0/queryLogs/$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://slave1:3306/hive?useUnicode=true&amp;amp;characterEncoding=utf8&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 1.4添加mysql-connector的jar包到hive安装路径下的lib文件夹12#$HIVE_HOME为前面hive安装的目录路径:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0[hadoop@master ~] mv mysql-connector-java-5.1.33.jar $HIVE_HOME/lib 1.5启动元数据服务12[hadoop@master ~]$ cd ~/bigdataspace/hive-1.1.0-cdh5.5.0[hadoop@master hive-1.1.0-cdh5.5.0]$ ./bin/hive --service metastore &amp; 1.6启动/停止hive (CTL)命令行123456#因为一开始配置了HIVE_HOME环境变量，可以直接在任何目录下执行hive命令了,进入hive控制台[hadoop@master bigdataspace]$ hive Logging initialized using configuration in jar:file:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/lib/hive-common-1.1.0-cdh5.5.0.jar!/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive (default)&gt; 上面报错了，解决Logging initialized using configuration in jar:file… （因为没log配置文件，直接从jar包查找） 1234567891011$ cd ~/bigdataspace/ /hive-1.1.0-cdh5.5.0/conf$ cp beeline-log4j.properties.template beeline-log4j.properties$ cp hive-log4j.properties.template hive-log4j.properties$ cp hive-exec-log4j.properties.template hive-exec-log4j.properties[hadoop@master bigdataspace]$ hiveLogging initialized using configuration in file:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive (default)&gt;hive&gt; quit; #(退出hive，使用exit也可以) 1.7启动/停止beeline命令行（CTL）1234#启动：[hadoop@master bigdataspace]$ beeline#停止：beeline&gt; !q 1.8HiveServer2的使用1234567891011121314151617181920212223242526[hadoop@master ~]$ cd ~/bigdataspace/hive-1.1.0-cdh5.5.0/bin/[hadoop@master bin]$ ./hiveserver2 &amp; #后面的&amp;表示改命名在系统后台执行(如果执行上面命令让界面无法回到命令行，可以按ctrl+C回到命令行，这里&amp;会让hiverserver2在后台继续执行)#查看HiveServer2的进程情况(如果无则hiverserver2启动失败或停止了):[hadoop@master bin]$ ps -ef |grep HiveServer2 hadoop 25545 14762 3 17:02 pts/1 00:00:21 /home/hadoop/bigdataspace/jdk1.8.0_60/bin/java -Xmx256m -Djava.library.path=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0/lib/native/ -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx512m -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/lib/hive-service-1.1.0-cdh5.5.0.jar org.apache.hive.service.server.HiveServer2hadoop 26038 14762 0 17:14 pts/1 00:00:00 grep HiveServer2(“kill -9 PID” 可以通过kill停止hiveserver2的后台服务) 使用beeline连接hiveserver2测试：(jdbc:hive2：表示连接到hiveserver2master:表示hiveserver2安装的机器host/IP10001:表示hiveserver2设置的端口号(hive-site.xml中可设置))[hadoop@master hive-1.1.0-cdh5.5.0]$ beeline -u jdbc:hive2://master:10001###这里可能会出现一些slf4j包有多个，引用异常，但是不是报错，如：SLF4J: Class path contains multiple SLF4J bindingsSLF4J: Found binding in [jar:file:/home/hadoop/bigdataspace/had…)Connecting to jdbc:hive2://master:10001Connected to: Apache Hive (version 1.1.0-cdh5.5.0)Driver: Hive JDBC (version 1.1.0-cdh5.5.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.5.0 by Apache Hive0: jdbc:hive2://master:10001&gt; 以上完成了Hive的基本安装配置。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"Elasticsearch学习教程系列(0)-入门与安装","slug":"elasticsearch-learn00","date":"2019-01-10T14:34:05.000Z","updated":"2019-03-06T14:23:56.929Z","comments":true,"path":"2019/01/10/elasticsearch-learn00/","link":"","permalink":"https://blog.monbuilder.top/2019/01/10/elasticsearch-learn00/","excerpt":"","text":"本教程是基于Elasticsearch6.5版本编写下面将介绍、安装并启动Elasticsearch，查看其中的内容以及执行索引，搜索和修改数据等基本操作的过程。在本教程结束时，您应该很好地了解Elasticsearch是什么，它是如何工作的，并希望能够获得灵感，看看如何使用它来构建复杂的搜索应用程序或从数据中挖掘智能。 0.1入门简介Elasticsearch是一个高度可扩展的开源全文搜索和分析引擎。Elasticsearch基于Apache Lucene，它允许您快速，近实时地存储，搜索和分析大量数据。它通常用作与底层引擎/技术，提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口；为具有复杂搜索功能和要求的应用程序提供支持。以下是Elasticsearch可用于的一些场景示例： 您运行在线网上商店，允许您的客户搜索您销售的产品。在这种情况下，您可以使用Elasticsearch存储整个产品目录和库存，并为它们提供搜索和自动填充建议。 您希望收集日志或交易数据，并且希望分析和挖掘此数据以查找趋势，统计信息，摘要或异常。在这种情况下，您可以使用Logstash（Elasticsearch / Logstash / Kibana堆栈的一部分）来收集，聚合和解析数据，然后让Logstash将此数据提供给Elasticsearch。一旦数据在Elasticsearch中，您就可以运行搜索和聚合来挖掘您感兴趣的任何信息。 您运行价格警报平台，允许精通价格的客户指定一条规则，例如“我有兴趣购买特定的电子产品，如果小工具的价格在下个月内从任何供应商降至X美元以下，我希望收到通知” 。在这种情况下，您可以刮取供应商价格，将其推入Elasticsearch并使用其反向搜索（Percolator）功能来匹配价格变动与客户查询，并最终在发现匹配后将警报推送给客户。 您有分析/业务智能需求，并希望快速调查，分析，可视化并询问有关大量数据的特定问题（想想数百万或数十亿条记录）。在这种情况下，您可以使用Elasticsearch存储数据，然后使用Kibana（Elasticsearch / Logstash / Kibana堆栈的一部分）构建自定义仪表板，以便可视化对您来说重要的数据方面。此外，您可以使用Elasticsearch聚合功能针对您的数据执行复杂的商业智能查询。0.2基本概念有一些概念是Elasticsearch的核心。从一开始就理解这些概念将极大地帮助简化学习过程。1.近实时(NRT)Elasticsearch是一个近实时搜索平台。这意味着从索引文档到可搜索文档的时间有一点延迟（通常是一秒）。2.集群(cluster)集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。确保不要在不同的环境中重用相同的群集名称，否则最终会导致节点加入错误的群集。例如，您可以使用logging-dev，logging-stage以及logging-prod 用于开发，登台和生产集群。请注意，如果群集中只有一个节点，那么它是完全正常的。此外，您还可以拥有多个独立的集群，每个集群都有自己唯一的集群名称。3.节点(node)节点是作为群集一部分的单个服务器，存储数据并参与群集的索引和搜索功能。就像集群一样，节点由名称标识，默认情况下，该名称是在启动时分配给节点的随机通用唯一标识符（UUID）。如果不需要默认值，可以定义所需的任何节点名称。此名称对于管理目的非常重要，您可以在其中识别网络中哪些服务器与Elasticsearch集群中的哪些节点相对应。可以将节点配置为按群集名称加入特定群集。默认情况下，每个节点都设置为加入一个名为cluster的集群elasticsearch，这意味着如果您在网络上启动了许多节点并且假设它们可以相互发现 - 它们将自动形成并加入一个名为的集群elasticsearch。在单个群集中，您可以拥有任意数量的节点。此外，如果您的网络上当前没有其他Elasticsearch节点正在运行，则默认情况下启动单个节点将形成一个名为的新单节点集群elasticsearch。4.索引(index)索引是具有某些类似特征的文档集合。例如，您可以拥有客户数据的索引，产品目录的另一个索引以及订单数据的另一个索引。索引由名称标识（必须全部小写），此名称用于在对其中的文档执行索引，搜索，更新和删除操作时引用索引。在单个群集中，您可以根据需要定义任意数量的索引。5.文档(document)文档是可以编制索引的基本信息单元。例如，您可以为单个客户提供文档，为单个产品提供另一个文档，为单个订单提供另一个文档。该文档以JSON（JavaScript Object Notation）表示，JSON是一种普遍存在的互联网数据交换格式。在索引/类型中，您可以根据需要存储任意数量的文档。请注意，尽管文档实际上驻留在索引中，但实际上必须将文档编入索引/分配给索引中的类型。6.分片和副本(Shards &amp; Replicas)索引可能存储大量可能超过单个节点的硬件限制的数据。例如，占用1TB磁盘空间的十亿个文档的单个索引可能不适合单个节点的磁盘，或者可能太慢而无法单独从单个节点提供搜索请求。为了解决这个问题，Elasticsearch提供了将索引细分为多个称为分片的功能。创建索引时，只需定义所需的分片数即可。每个分片本身都是一个功能齐全且独立的“索引”，可以托管在集群中的任何节点上。分片很重要，主要有两个原因： 它允许您水平拆分/缩放内容量 它允许您跨分片（可能在多个节点上）分布和并行化操作，从而提高性能/吞吐量分片的分布方式以及如何将其文档聚合回搜索请求的机制完全由Elasticsearch管理，对用户而言是透明的。在可以随时发生故障的网络/云环境中，非常有用，强烈建议使用故障转移机制，以防分片/节点以某种方式脱机或因任何原因消失。为此，Elasticsearch允许您将索引的分片的一个或多个副本制作成所谓的副本分片或简称副本。复制很重要，主要有两个原因： 它在碎片/节点发生故障时提供高可用性。因此，请务必注意，副本分片永远不会在与从中复制的原始/主分片相同的节点上分配。 它允许您扩展搜索量/吞吐量，因为可以在所有副本上并行执行搜索。总而言之，每个索引可以拆分为多个分片。索引也可以复制为零（表示没有副本）或更多次。复制后，每个索引都将具有主分片（从中复制的原始分片）和副本分片（主分片的副本）。可以在创建索引时为每个索引定义分片和副本的数量。创建索引后，您还可以随时动态更改副本数。您可以使用_shrink和_splitAPI 更改现有索引的分片数，但这不是一项简单的任务，并且预先计划正确数量的分片是最佳方法。默认情况下，Elasticsearch中的每个索引都分配了5个主分片和1个副本，这意味着如果群集中至少有两个节点，则索引将包含5个主分片和另外5个副本分片（1个完整副本），总计为每个索引10个分片。Tips: 每个Elasticsearch分片都是Lucene索引。单个Lucene索引中可以包含最大数量的文档。截止LUCENE-5843，限制是2,147,483,519（= Integer.MAX_VALUE - 128）文档。您可以使用_cat/shardsAPI 监控分片大小。0.3安装Elasticsearch是基于Java开发的，因此安装时，需要JDK环境。我们安装的版本是Elasticsearch6.5.4，至少需要JDK 8。我们假设您已经在您的Linux(例如CentOS6.5)环境下已经安装了JDK。 12345678910111213141516171819 [builder@master ~]$ java -version java version \"1.8.0_171\" Java(TM) SE Runtime Environment (build 1.8.0_171-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode) #下载安装包，并解压： [builder@master ~]$ cd ~ [builder@master ~]$ mdkir env [builder@master ~]$ cd env [builder@master ~]$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.5.4.tar.gz#等待下载完成后，解压 [builder@master ~]$ tar -zxvf elasticsearch-6.5.4.tar.gz [builder@master ~]$ cd elasticsearch-6.5.4 # 在elasticsearch命令后加 -d 参数，是指以守护线程启动程序，即是后台运行 [builder@master ~]$ ./bin/elasticsearch -d 到这里Elasticsearch的安装已经完成了，是不是非常简单呢？另外，我们启动的时候是可以通过 -E 参数指定集群名称(cluster.name)或者节点名称(node.name)的： [builder@master ~]$ ./bin/elasticsearch -Ecluster.name=esCluster -Enode.name=builder01 -d Tips: 默认情况下，Elasticsearch使用port 9200来提供对其REST API的访问。如有必要，可以配置此端口。","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://blog.monbuilder.top/tags/Elasticsearch/"}],"keywords":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://blog.monbuilder.top/categories/搜索引擎/"}]},{"title":"搭建大数据平台系列(3)-hbase环境搭建","slug":"hbase","date":"2018-12-13T09:21:59.000Z","updated":"2018-12-14T06:14:20.392Z","comments":true,"path":"2018/12/13/hbase/","link":"","permalink":"https://blog.monbuilder.top/2018/12/13/hbase/","excerpt":"","text":"1.安装步骤Hbase的安装需要在hadoop和zookeeper安装完成的基础上进行安装部署，所以，需要在安装hbase前准备好hadoop和zookeeper的环境（请看本系列前几篇文章）123456789101112131415161718`1.下载hbase-1.0.0-cdh5.5.0.tar.gz到master机器的/bigdataspacce文件夹下2.解压安装包的命令：[hadoop@master ]()$ cd /bigdataspacce[hadoop@master bigdataspace]()$ tar -zxvf hbase-1.0.0-cdh5.5.0.tar.gz3.解压完成后删除压缩包：[hadoop@master bigdataspace]()$ rm hbase-1.0.0-cdh5.5.0.tar.gz4.修改hbase-env.sh、hbase-site.xml配置文件以及regionservers文件（配置dataNode节点）$ cd /home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0/conf$ vi hbase-env.sh# The java implementation to use. Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/(在上面这条注释下加上：)export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60……export HBASE_PID_DIR=/data/hbase-1.0.0-cdh5.5.0/pids# export HBASE_MANAGES_ZK=true #设置hbase是否管理zookeeperexport HBASE_MANAGES_ZK=trueexport HBASE_MANAGES_ZK=false #使用独立的ZooKeeper时需要修改HBASE_MANAGES_ZK值为false，为不使用默认自带的ZooKeeper实例。 12345678910111213141516171819202122232425262728293031`$ vi hbase-site.xml(修改配置文件内容为如下)\\&lt;configuration\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.rootdir\\&lt;/name\\&gt;\\&lt;value\\&gt;hdfs://master:8020/hbase\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.cluster.distributed\\&lt;/name\\&gt;\\&lt;value\\&gt;true\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.zookeeper.quorum\\&lt;/name\\&gt;\\&lt;value\\&gt;slave1,slave2,slave3\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.zookeeper.property.dataDir\\&lt;/name\\&gt;\\&lt;value\\&gt;/data/zookeeper-3.4.5-cdh5.5.0/var/data\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;property\\&gt;\\&lt;name\\&gt;hbase.tmp.dir\\&lt;/name\\&gt;\\&lt;value\\&gt;/data/hbase-1.0.0-cdh5.5.0/tmp\\&lt;/value\\&gt;\\&lt;/property\\&gt;\\&lt;/configuration\\&gt;(hdfs://master:8020/hbase,这里的hbase目录未建好的话是需要hdfs dfs –mkdir 新建的目录)$ vi regionserversslave1slave2slave3(以上使用对应的ip配置也可以)``` `5.配置HBASE_HOME$ vi /etc/profile(加上如下配置)export HBASE_HOME=/home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH6.使用scp命令把hbase分发到各个节点$ scp -r hbase-1.0.0-cdh5.5.0/ hadoop@slave1:/bigdataspace/$ scp -r hbase-1.0.0-cdh5.5.0/ hadoop@slave2:/bigdataspace/$ scp -r hbase-1.0.0-cdh5.5.0/ hadoop@slave3:/bigdataspace/ 然后在各个节点上执行第5步：配置HBASE_HOME_1` `7.Hbase的启动与停止启动hbase时要确保hdfs已经启动，HBase的启动顺序为：HDFS->Zookeeper->HBase，启动Hbase的命令如下(在master机器上)：hadoop@master $ cd /home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0/bin(注意，如果设置了hbase管理zookeeper，则需要先关闭手动启动的各节点zookeeper)如slave1机器：hadoop@slave1 $ /bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/zkServer.sh stop在master机器： hadoop@master bin$ ./start-hbase.sh hadoop@master bin$ jps29385 HMaster19994 JobHistoryServer19068 NameNode29757 Jps19422 ResourceManager19263 SecondaryNameNode1` `如slave1机器：hadoop@slave1 bin$ jps12768 DataNode17971 HRegionServer12884 NodeManager22704 QuorumPeerMain18169 Jps17851 HQuorumPeer #hbase管理zookeeper的进程, 如果export HBASE_MANAGES_ZK=true，才会出现上面的进程如果HQuorumPeer不存在，而是QuorumPeerMain则表明需要手动关闭zookeeper，hbase才能接手管理。Hbase停止命令：hadoop@master bin$ ./stop-hbase.sh``## 2.验证启动成功访问HBase web 页面：http://master:60010/","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"搭建大数据平台系列(2)-zookeeper环境搭建","slug":"zookeeper","date":"2018-11-05T08:08:10.000Z","updated":"2018-12-14T06:14:20.400Z","comments":true,"path":"2018/11/05/zookeeper/","link":"","permalink":"https://blog.monbuilder.top/2018/11/05/zookeeper/","excerpt":"","text":"1.安装步骤Zookeeper集群一般配置奇数个，在本次测试机是部署到slave1，slave2，slave3这3台机器上。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541.下载zookeeper-3.4.5-cdh5.5.0.tar.gz到slave1机器的~/bigdataspacce文件夹下2.解压安装包的命令：[hadoop@slave1 bigdataspace]$ tar -zxvf zookeeper-3.4.5-cdh5.5.0.tar.gz3.解压完成后删除压缩包：[hadoop@slave1 bigdataspace]$ rm zookeeper-3.4.5-cdh5.5.0.tar.gz4.配置zoo.cfg文件 $ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/conf $ cp zoo_sample.cfg zoo.cfg $ vi zoo.cfg (修改文件中的dataDir配置) dataDir=/data/zookeeper-3.4.5-cdh5.5.0/var/data dataLogDir=/data/zookeeper-3.4.5-cdh5.5.0/var/dataLog/（并在clientPort下面新增如下配置）server.1=slave1:2888:3888server.2=slave2:2888:3888server.3=slave3:2888:38885.建立dataDir对应路径的文件夹，并进入该data文件夹下新建一个文件myid： $ mkdir -p /data/zookeeper-3.4.5-cdh5.5.0/var/dataLog$ mkdir -p /data/zookeeper-3.4.5-cdh5.5.0/var/data $ cd /data/zookeeper-3.4.5-cdh5.5.0/var/data $ vi myid (myid文件内容为zoo.cfg中配的server号码，如server.1则myid文件中只保存1，每台机器都配自己对应的号码) $ cat myid 1 $6.以上对zookeeper的配置基本完成，下面使用scp把zookeeper发到各个节点：$ scp -r zookeeper-3.4.5-cdh5.5.0/ hadoop@slave2:~/bigdataspace/$ scp -r zookeeper-3.4.5-cdh5.5.0/ hadoop@slave3:~/bigdataspace/7.通过scp把myid传到各个节点，并修改其zoo.cfg配置文件对应的server号码(如server.2=slave1:52888:53888则myid文件存入2) $ scp -r /data/zookeeper-3.4.5-cdh5.5.0/ hadoop@slave2:/data/ $ scp -r /data/zookeeper-3.4.5-cdh5.5.0/ hadoop@slave3:/data/ (然后到到各个节点上修改/data/zookeeper-3.4.5-cdh5.5.0/var/data/myid文件),如： [hadoop@slave2 ~]$ vi /data/zookeeper-3.4.5-cdh5.5.0/var/data/myid 2 [hadoop@slave2 ~]$8.zookeeper.out以及log4j日志文件的设置[hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/conf[hadoop@slave1 conf]$ vi log4j.properties# Define some default values that can be overridden by system propertieszookeeper.root.logger=INFO, ROLLINGFILE……log4j.appender.ROLLINGFILE=org.apache.log4j.DailyRollingFileAppender查看zkServer.sh脚本，发现运行时会先加载zookeeper-3.4.5-cdh5.5.0/libexec/zkEnv.sh，不存在会加载zookeeper-3.4.5-cdh5.5.0/bin/zkEnv.sh[hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/libexec[hadoop@slave1 libexec]$ vi zkEnv.shif [ \"x$&#123;ZOO_LOG_DIR&#125;\" = \"x\" ]then ZOO_LOG_DIR=\"/data/zookeeper-3.4.5-cdh5.5.0/logs\"fiif [ \"x$&#123;ZOO_LOG4J_PROP&#125;\" = \"x\" ]thenZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\"也可以把zookeeper-3.4.5-cdh5.5.0/bin/zkEnv.sh文件的配置修改成上面一样. 1239.启动zookeeper服务： [hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin [hadoop@slave1 bin]$ ./ zkServer.sh start #启动zookeeper（每台机器都要执行此命令） 2.验证1234567 [hadoop@slave1 bin]$ jps #使用jps命令 25906 Jps20536 QuorumPeerMain #zookeeper的进程19994 JobHistoryServer19068 NameNode19422 ResourceManager19263 SecondaryNameNode 如上，含有QuorumPeerMain表明安装成功12345678910111213[hadoop@master bin]$ ./ zkServer.sh stop #停止zookeeper（每台机器都要执行此命令）[hadoop@master bin]$ ./zkServer.sh status #查看角色状态命令JMX enabled by defaultUsing config: /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/../conf/zoo.cfgMode: follower（Mode: follower/leader，leader这个角色只有一台机器，是通过zookeeper的选举算法产生）如果出现如下错误，[hadoop@master bin]$ ./zkServer.sh statusJMX enabled by defaultUsing config: /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/../conf/zoo.cfgError contacting service. It is probably not running.极大可能是因为防火墙端口被限制了，可以打开这些被用到的端口(注意：只启用一台zookeeper也是会 出现这个错误，需要启动2台以上的节点) 12#进入zookeeper客户端的命令[hadoop@master bin]$ bin/zkCli.sh -server master:2181","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"rabbitMQ的介绍与安装使用","slug":"rabbitmq","date":"2018-10-24T01:47:37.000Z","updated":"2018-12-14T06:14:20.387Z","comments":true,"path":"2018/10/24/rabbitmq/","link":"","permalink":"https://blog.monbuilder.top/2018/10/24/rabbitmq/","excerpt":"","text":"0. 消息中间件的介绍RabbitMQ是一个“传统”消息代理，可以实现各种消息传递协议。它是首批实现合理级别功能，客户端库，开发工具和质量文档的开源消息代理之一。RabbitMQ最初是为实现AMQP而开发的，AMQP是一种开放式线路协议，具有强大的路由功能。虽然Java具有像JMS这样的消息传递标准，但它对于需要分布式消息传递的非Java应用程序没有帮助，因为它严重限制了任何集成场景，微服务或单片机。随着AMQP的出现，跨语言的灵活性成为开源消息代理的真实存在。RabbitMQ被设计为通用消息代理，采用点对点，请求/回复和pub-sub通信样式模式的多种变体。它使用智能代理/哑消费者模型，专注于向消费者提供一致的消息传递，消费者的消费速度与经纪人跟踪消费者状态的速度大致相似。它是成熟的，在正确配置时表现良好，得到很好的支持（客户端库Java，.NET，node.js，Ruby，PHP和更多语言），并且有许多可用的插件可以将它扩展到更多的用例和集成场景。 1.kafka,rabbitMQ的对比RabbitMQ中的通信可以根据需要同步或异步。发布者向交换发送消息，消费者从队列中检索消息。通过交换将生产者与队列分离可确保生产者不会受到硬编码路由决策的影响。RabbitMQ还提供了许多分布式部署方案（并且确实要求所有节点都能够解析主机名）。可以将多节点群集设置为群集联合，并且不依赖于外部服务（但某些群集形成插件可以使用AWS API，DNS，Consul等）。Apache Kafka专为高容量发布 - 订阅消息和流而设计，旨在持久，快速和可扩展。从本质上讲，Kafka提供了一个持久的消息存储，类似于日志，在服务器集群中运行，它存储称为主题的类别中的记录流。 2.rabbitMQ的安装与使用2.1在基于RPM的Linux上安装（RHEL，CentOS，Fedora，openSUSE），RabbitMQ RPM包需要sudo权限(或使用root用户)才能安装和管理。123456789101112131415161718192021222324252627282930313233343536373839####1.安装Erlang[root@slave1 ~]# yum install erlang[root@slave1 ~]# yum update erlang####2.安装RabbitMQ服务器 ##下载rabbitMQ rpm安装包[root@slave1 ~]# wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.8/rabbitmq-server-3.7.8-1.el6.noarch.rpm[root@slave1 ~]# yum install rabbitmq-server-3.7.8-1.el6.noarch.rpm ## 开机启动rabbitmq-server[root@slave1 ~]# chkconfig rabbitmq-server on #启动rabbitMQ服务[root@slave1 ~]# service rabbitmq-server start #停止rabbitMQ服务[root@slave1 ~]# service rabbitmq-server stop #重启rabbitMQ服务[root@slave1 ~]# service rabbitmq-server restart #修改配置文件[root@slave1 ~]# cd /etc/rabbitmq/[root@slave1 rabbitmq]# cp /usr/share/doc/rabbitmq-server-3.7.8/rabbitmq.config.example ./[root@slave1 rabbitmq]# mv rabbitmq.config.example rabbitmq.config[root@slave1 rabbitmq]# vim rabbitmq.config[&#123;rabbit, [&#123;tcp_listeners, [5672]&#125;, &#123;loopback_users, [\"test\"]&#125;]&#125;]. #开启Web管理插件，这样我们就可以通过浏览器来进行管理了[root@slave1 ~]# rabbitmq-plugins enable rabbitmq_management[root@slave1 ~]# service rabbitmq-server restart[root@slave1 ~]# vim /etc/rabbitmq/rabbitmq.config[root@slave1 ~]# service rabbitmq-server restart ##增加rabbitmq ui的登录用户[root@slave1 ~]# rabbitmqctl add_user test 123456[root@slave1 ~]# rabbitmqctl set_user_tags test administrator[root@slave1 ~]# rabbitmqctl set_permissions -p \"/\" test \".*\" \".*\" \".*\"[root@slave1 ~]# rabbitmqctl list_usersListing userstest [administrator]guest [administrator] 2.2 在macOS中安装rabbitMQ1234567891011#使用homebrew安装rabbitMQ,若安装过homebrew可以忽略下面第一句命令$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"$ brew update$ brew install rabbitmq#启动命令$ brew services start rabbitmq#重启命令$ brew services restart rabbitmq#停止命令$ brew services stop rabbitmq 使用浏览器访问ip:15672(rabbitMQ的UI界面)，可使用上面设置的test用户密码进行登录","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://blog.monbuilder.top/categories/消息中间件/"}],"tags":[{"name":"rabbitMQ 分布式","slug":"rabbitMQ-分布式","permalink":"https://blog.monbuilder.top/tags/rabbitMQ-分布式/"}],"keywords":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://blog.monbuilder.top/categories/消息中间件/"}]},{"title":"activiti6-tutorial00","slug":"activiti6-tutorial00","date":"2018-10-16T03:02:23.000Z","updated":"2018-12-14T06:14:20.396Z","comments":true,"path":"2018/10/16/activiti6-tutorial00/","link":"","permalink":"https://blog.monbuilder.top/2018/10/16/activiti6-tutorial00/","excerpt":"","text":"","categories":[],"tags":[],"keywords":[]},{"title":"webSphere性能监控(JMX)","slug":"websphere-monitor","date":"2018-09-21T08:12:32.000Z","updated":"2018-12-14T06:14:20.399Z","comments":true,"path":"2018/09/21/websphere-monitor/","link":"","permalink":"https://blog.monbuilder.top/2018/09/21/websphere-monitor/","excerpt":"","text":"1.监控方式1.1 部署自带perfServletApp监控项目以下步骤为支持JMX组件采集，websphere所需做的一些配置操作：1&gt;访问websphere的web console页面，例如：https://localhost:9043/ibm/console/2&gt;点击左侧树形菜单：应用程序-&gt;应用程序类型-&gt;WebSphere企业应用程序；进入页面，查看有无安装了perfServletApp，若无再点击：安装 按钮3&gt;在新新页面中选择：本地路径；找到并上传perfServletApp.ear部署包(此包路径在$WEBSPHERE_INSTALL_HOME/installableApps/PerfServletApp.ear)，可以copy到本地在上传4&gt;点击下一步，开始安装，一路点“下一步”，不用做任何修改填空，直到点击“完成”，最后保存配置5&gt;启动perfServletApp程序6&gt;配置perfServletApp访问的安全校验,点击：应用程序-&gt;perfServletApp,进入安全用户配置,按照下列步骤，最后确认并保存到主配置 7&gt;需要以上配置生效，需要重启webSphere服务 1.2 配置新增ssl证书,通过soap协议访问mbean以下步骤为支持JMX组件采集，websphere所需做的一些配置操作：","categories":[{"name":"webSphere","slug":"webSphere","permalink":"https://blog.monbuilder.top/categories/webSphere/"}],"tags":[{"name":"性能监控","slug":"性能监控","permalink":"https://blog.monbuilder.top/tags/性能监控/"}],"keywords":[{"name":"webSphere","slug":"webSphere","permalink":"https://blog.monbuilder.top/categories/webSphere/"}]},{"title":"搭建大数据平台系列(1)-Hadoop环境搭建[hdfs,yarn,mapreduce]","slug":"hadoop","date":"2018-08-09T15:00:08.000Z","updated":"2018-12-14T06:14:20.398Z","comments":true,"path":"2018/08/09/hadoop/","link":"","permalink":"https://blog.monbuilder.top/2018/08/09/hadoop/","excerpt":"","text":"1.ssh免密码登录设置123[hadoop@master ~]$ ssh -versionOpenSSH_5.3p1, OpenSSL 1.0.1e-fips 11 Feb 2013Bad escape character 'rsion'. 查看ssh的版本后，如果ssh未安装则需要执行如下安装命令：1[hadoop@master ~]$ sudo yum install openssh-server 在每台机器上都执行一次下面的命令：123456$ ssh-keygen –t rsa #一路回车，提示要填的都默认不填，按回车上面执行完成后，每台机器上都会生成一个~/.ssh文件夹$ ll ~/.ssh #查看.ssh文件下的文件列表-rw-------. 1 hadoop hadoop 1580 Apr 18 16:53 authorized_keys-rw-------. 1 hadoop hadoop 1675 Apr 15 16:01 id_rsa-rw-r--r--. 1 hadoop hadoop 395 Apr 15 16:01 id_rsa.pub 把slave1，slave2，slave3上生成的公钥id_rsa.pub发给master机器：在slave1机器上：1[hadoop@slave1 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave1 在slave2机器上：1[hadoop@slave2 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave2 在slave3机器上：1[hadoop@slave3 ~]$ scp ~/.ssh/id_rsa.pub hadoop@master:~/.ssh/id_rsa.pub.slave3 在master机器上，将所有公钥加到新增的用于认证的公钥文件authorized_keys中：1[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub* &gt;&gt; ~/.ssh/authorized_keys 需要修改文件authorized_keys的权限（权限的设置非常重要，因为不安全的设置安全设置,会让你不能使用RSA功能 ）1[hadoop@master ~]$ chmod 600 ~/.ssh/authorized_keys #如果免密码不成功有可能缺少这步 将公钥文件authorized_keys分发给每台slave:123[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/[hadoop@master ~]$ scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/ 2.Java环境的安装下载jdk-8u60-linux-x64.tar.gz安装包后(放在~/bigdataspace路径下)： 12 [hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ tar -zxvf jdk-8u60-linux-x64.tar.gz 修改环境变量配置文件:123456[hadoop@master bigdataspace]$ sudo vi /etc/profile(在配置文件末尾加上如下配置)export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 让环境变量设置生效：1[hadoop@master bigdataspace]$ source /etc/profile 验证Java是否安装成功：1234[hadoop@master bigdataspace]$ java -versionjava version \"1.8.0_60\"Java(TM) SE Runtime Environment (build 1.8.0_60-b27)Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode) (每台机器上都需要按照上面的操作安装Java)每台机器上执行：1[hadoop@master ~]$ sudo chmod 777 /data/ #让所有用户可操作/data目录下的数据 3.集群上的机器实现同步时间检查时间服务是否安装:12[hadoop@master ~]$ rpm -q ntpntp-4.2.6p5-1.el6.centos.x86_64 #这表示已安装了，如果没有安装，这是空白 如果没有安装，需要执行下面的安装命令：1[hadoop@master ~]$ sudo yum install ntp 需要配置NTP服务为自启动:123456789[hadoop@master ~]$ sudo chkconfig ntpd on[hadoop@master ~]$ chkconfig --list ntpdntpd 0:off 1:off 2:on 3:on 4:on 5:on 6:off(需要打开master机器上udp协议的123端口是为了其他节点使用ntpdate通过该端口同步master机器的时间)[hadoop@master ~]$ sudo vi /etc/sysconfig/iptables(新增的端口配置)-A INPUT -m state --state NEW -m udp -p udp --dport 123 -j ACCEPT[hadoop@master ~]$ sudo service iptables restart 在配置前，先使用ntpdate手动同步下时间，免得本机与外部时间服务器时间差距太大，让ntpd不能正常同步。12[hadoop@master ~]$ sudo ntpdate pool.ntp.org26 Apr 17:12:15 ntpdate[7376]: step time server 202.112.29.82 offset 13.827386 sec 更改master机器上的相关配置文件:1[hadoop@master ~]$ sudo vim /etc/ntp.conf 123456789101112131415161718192021222324252627282930313233343536373839(下面只显示修改的必要项)# Hosts on local network are less restricted.restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap#让同一局域网ip段可以进行时间同步：restrict 10.3.19.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#外部时间服务器server pool.ntp.org iburstserver 0.asia.pool.ntp.org iburstserver 1.asia.pool.ntp.org iburstserver 1.asia.pool.ntp.org iburstserver 2.asia.pool.ntp.org iburst#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# allow update time by the upper server# Undisciplined Local Clock. This is a fake driver intended for backup# and when no outside source of synchronized time is available.# 外部时间服务器不可用时，以本地时间作为时间服务server 127.127.1.0fudge 127.127.1.0 stratum 10#############################################################其他节点/etc/ntp.conf（slave1,slave2,slave3）的配置：……..#server 3.centos.pool.ntp.org iburst#外部时间服务器，以master时间为准进行同步server master iburst…….. 1234[hadoop@master ~]$ sudo service ntpd start(每台机器上都需要，设置ntpd开机启动，并第一次手动打开ntpd)，命令如下：$ sudo chkconfig ntpd on #开机启动ntpd$ sudo service ntpd start #启动 ntpd 时间同步设置参考：http://cn.soulmachine.me/blog/20140124/ 时间同步设置总结： 每个节点上安装ntpd，并设置为开机启动，当然第一次要先手动启动，通过配置/etc/ntp.conf文件，让master作为时间同步服务器，这台机器的时间是根据联网同步网络时间的，其他节点以master的ip作为同步的地址 配置完成后，发现后面的节点时间可能还未同步，可能需要等30分钟左右，一段时间后时间都会以master为准，进行同步 4.Hadoop的安装、配置下载hadoop-2.6.0-cdh5.5.0.tar.gz安装包后(放在master机器上的~/bigdataspace路径下)：12[hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ tar -zxvf hadoop-2.6.0-cdh5.5.0.tar.gz 进入hadoop配置文件路径:1[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/etc/hadoop 1&gt; 在hadoop-env.sh中配置JAVA_HOME：1[hadoop@master hadoop]$ vi hadoop-env.sh 123# set JAVA_HOME in this file, so that it is correctly defined on# The java implementation to use.export JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60 2&gt; 在yarn-env.sh中配置JAVA_HOME：1[hadoop@master hadoop]$ vi yarn-env.sh 12# some Java parametersexport JAVA_HOME=/home/hadoop/bigdataspace/jdk1.8.0_60 3&gt; 在slaves中配置slave节点的ip或者host1[hadoop@master hadoop]$ vi slaves 123slave1slave2slave3 4&gt; 修改core-site.xml1[hadoop@master hadoop]$ vi core-site.xml 1234567891011&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop-2.6.0-cdh5.5.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5&gt; 修改hdfs-site.xml1[hadoop@master hadoop]$ vi hdfs-site.xml 1234567891011121314151617181920&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop-2.6.0-cdh5.5.0/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop-2.6.0-cdh5.5.0/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6&gt; 修改mapred-site.xml1[hadoop@master hadoop]$ vi mapred-site.xml 123456789101112131415&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7&gt; 修改yarn-site.xml1[hadoop@master hadoop]$ vi yarn-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 因为CDH版本缺少hadoop的native库，因此需要引入，否则会报错，解决方法：http://www.cnblogs.com/huaxiaoyao/p/5046374.html本次安装具体采取的解决方法：123[hadoop@master ~]$ cd ~/bigdataspace[hadoop@master bigdataspace]$ wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.5.0/RPMS/x86_64/hadoop-2.6.0+cdh5.5.0+921-1.cdh5.5.0.p0.15.el6.x86_64.rpm[hadoop@master bigdataspace]$ rpm2cpio *.rpm | cpio -div 在bigdataspace文件夹下1$ cp -r ./usr/lib/hadoop/lib/native/ ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/lib/native/ 删除解压后得到的文件：1234[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/etc/[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/usr/[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/var//$ rm ~/ bigdataspace/hadoop-2.6.0+cdh5.5.0+921-1.cdh5.5.0.p0.15.el6.x86_64.rpm 5.使用scp命令分发配置好的hadoop到各个子节点123$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave1:~/bigdataspace/$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave2:~/bigdataspace/$ scp –r ~/bigdataspace/hadoop-2.6.0-cdh5.5.0/ hadoop@slave3:~/bigdataspace/ (每台机器)修改环境变量配置文件:1[hadoop@master bigdataspace]$ sudo vi /etc/profile (在配置文件末尾加上如下配置)12export HADOOP_HOME=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH 让环境变量设置生效：1[hadoop@master bigdataspace]$ source /etc/profile 6.启动并验证Hadoop1234[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0 #进入hadoop目录 [hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./bin/hdfs namenode –format #格式化namenode[hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./sbin/start-dfs.sh #启动dfs[hadoop@master hadoop-2.6.0-cdh5.5.0]$ ./sbin/start-yarn.sh #启动yarn 可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程12345[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps3407 SecondaryNameNode3218 NameNode3552 ResourceManager3910 Jps 在 slave1 上应该有以下几个进程1234[hadoop@slave1 ~]$ jps2072 NodeManager2213 Jps1962 DataNode 或者在浏览器中输入 http://master:8088 ，应该有 hadoop 的管理界面出来了,并通过http://master:8088/cluster/nodes能看到 slave1、slave2、slave3节点 7.启动Hadoop自带的jobhistoryserver[hadoop@master ~]$ cd ~/bigdataspace/hadoop-2.6.0-cdh5.5.0 #进入hadoop目录[hadoop@master hadoop-2.6.0-cdh5.5.0]$ sbin/mr-jobhistory-daemon.sh start historyserver(mapred-site.xml配置文件有对jobhistory的相关配置)[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps5314 Jps19994 JobHistoryServer19068 NameNode19422 ResourceManager19263 SecondaryNameNode 参考：http://blog.csdn.net/liubei_whut/article/details/42397985 8.停止hadoop集群的问题Linux运行一段时间后，/tmp下的文件夹下面会清空一些文件，hadoop的停止脚本stop-all.sh是需要根据/tmp下面的pid文件关闭对应的进程，当/tmp下的文件被自动清理后可能会出出先的错误：123456789$ ./sbin/stop-all.shStopping namenodes on [master]master: no namenode to stopslave1: no datanode to stopslave2: no datanode to stopslave3: no datanode to stopStopping secondary namenodes [master]master: no secondarynamenode to stop…… 方法1：这时需要在/tmp文件夹下手动创建恢复这些pid文件master节点（每个文件中保存对应的进程id）：hadoop-hadoop-namenode.pidhadoop-hadoop-secondarynamenode.pidyarn-hadoop-resourcemanager.pidslave节点（每个文件中保存对应的进程id）：hadoop-hadoop-datanode.pidyarn-hadoop-nodemanager.pid方法2：使用kill -9逐个关闭相应的进程id 从根本上解决的方法：（首先使用了方法1或方法2关闭了hadoop集群）1.修改配置文件hadoop-env.sh:1234#export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;export HADOOP_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids#export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;export HADOOP_SECURE_DN_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids 2.修改配置文件yarn-env.sh:1export YARN_PID_DIR=/data/hadoop-2.6.0-cdh5.5.0/pids 3.创建文件夹pids：1$ mkdir /data/hadoop-2.6.0-cdh5.5.0/pids（发现会自动创建pids文件，因此不需要创建） 这2个步骤需要在各个节点都执行.","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"搭建大数据平台系列(0)-机器准备","slug":"conf-os","date":"2018-07-20T05:16:05.000Z","updated":"2018-12-14T06:14:20.376Z","comments":true,"path":"2018/07/20/conf-os/","link":"","permalink":"https://blog.monbuilder.top/2018/07/20/conf-os/","excerpt":"","text":"0. 前期规划假设现在有四台机器，各自ip如下:1234192.168.1.201192.168.1.202192.168.1.203192.168.1.204 安装下面统一要求进行重装系统： 每台机器的系统为：CentOS-6.5-x86_64 每台机器的磁盘分区为： 1234/boot : 系统引导分区/ : 系统安装区/data : 系统数据分区（swap分区尚未建立，待机器内存不足需求时再建立） 每台机器上安装的都是Mininal Desktop版本 每台机器的主机名(hostname)分别为：master、slave1、slave2、slave3 每台机器的root密码都是:master 每台机器上建立的第一个非root用户都为:hadoop，密码为:hadoop 非root用户在centos中获取sudo权限的命令：123456[hadoop@slave3 ~]$ su - #切换到root用户，需要输入root密码[root@slave3 ~]# visudo -f /etc/sudoers## Allow root to run any commands anywhereroot ALL=(ALL) ALL在上面这条文字下加上如下：hadoop ALL=(ALL) ALL 这样配置后，hadoop用户可以使用sudo命令了 修改每台机器的/etc/hosts文件，如：12345[root@slave3 ~]$ sudo vi /etc/hosts192.168.1.201 master192.168.1.202 slave1192.168.1.203 slave2192.168.1.204 slave3 9.防火墙设置 因为4台机器组成的大数据平台小集群需要互相访问对方的多个端口，需要在防火墙中打开这些端口的访问（如果未打开的话），当然最方便的就是把每台机器上的防火墙关闭了。123456789101112131415161718192021222324[hadoop@master ~]$ sudo service iptables start #打开防火墙[hadoop@master ~]$ sudo service iptables status #查看防火墙[hadoop@master ~]$ sudo service iptables stop #关闭防火墙(注意：防火墙操作时需要root权限的，非root用户不使用sudo的话，不会报错，但操作无结果)修改防火墙设置(红色为新增配置：开放2000-6000,7000以上的端口):[hadoop@master ~]$ sudo vim /etc/sysconfig/iptables# Firewall configuration written by system-config-firewall# Manual customization of this file is not recommended.*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT-A INPUT -p icmp -j ACCEPT-A INPUT -i lo -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 2000:6000 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 7000: -j ACCEPT-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j REJECT --reject-with icmp-host-prohibitedCOMMIT保存上面的文件后，需要重启防火墙，让配置生效！[hadoop@master ~]$ sudo service iptables restart #重启防火墙","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.monbuilder.top/tags/Hadoop/"}],"keywords":[{"name":"大数据","slug":"大数据","permalink":"https://blog.monbuilder.top/categories/大数据/"}]},{"title":"Hexo+Github Page搭建个人博客","slug":"build-blog","date":"2018-06-22T09:03:09.000Z","updated":"2018-12-14T06:14:20.390Z","comments":true,"path":"2018/06/22/build-blog/","link":"","permalink":"https://blog.monbuilder.top/2018/06/22/build-blog/","excerpt":"","text":"什么是Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。堪称在座各位喜欢Markdown的优雅人士博客建站神器哟！ 1. Quick Start1.1 创建存放Github Pages的仓库Github Pages 是面向用户、组织和项目开放的公共静态页面搭建托管服务，站点可以被免费托管在 Github 上，你可以选择使用 Github Pages 提供的域名 github.io 或者自定义域名来发布站点。 需要Github账号，请登录https://github.com/ 注册。\b\b登录了自己的github账号后，可以安装下图一样，创建自己的GitHub Pages仓库名[参考https://pages.github.com/ ]，[PS] 仓库名repository name需要约定为: 你的账号名.github.io创建好博客项目仓库后，可以通过git命名下载到本地，并编辑一下README.md从本地提交到GitHub，这样做主要是使本地文件与Github关联起来，方便后面hexo deploy,直接部署博客内容到GitHub进行更新。12345678910$ git clone https://github.com/yourGithubName/yourGithubName.github.io$ vim README.md# REAMME.md上可以简单写一些博客介绍啥的$ git config --global user.email \"you@example.com\"$ git config --global user.name \"Your Name\"$ git add ./$ git commit -m 'test'$ git push -u origin masterUsername for 'https://github.com': Builder34Password for 'https://Builder34@github.com': 1.2 Hexo安装安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js (请看https://nodejs.org/zh-cn/) Git （请看https://git-scm.com/downloads）安装好上面2个程序后，可以进行hexo的安装了：1$ npm install -g hexo-cli 1.3 Hexo初始化安装 Hexo 完成后，我们可以在本地新建一个文件夹如：builder34.github.io(\b这个目录是我们Github Pages博客项目的目录),假如我的文件夹路径为/home/test/builder34.github.io，建站初始化命令可以如下:12345$ cd /home/test/builder34.github.io$ hexo init ./$ npm install$ hexo generate 下面介绍几个常用的hexo命令(括号里面的命令为缩写形式，效果一样)： 1. hexo generate(hexo g) #生成静态文件，会在当前目录下生成一个新的叫做public的文件夹 2. hexo new &quot;postTitle&quot; #新建博客文章 3. hexo new page &quot;pageTitle&quot; #新建1个页面 4. hexo server(hexo s) #启动本地web服务预览(加参数--debug,用于调试，如：hexo s --debug) 5. hexo deploy(hexo d) #部署播客到远端（比如Github,coding,heroku等平台） 在命令行中输入hexo s --debug后，运行成功后，可以在浏览器中输入：http://localhost:4000看到自己新建的博客了。 1.4 更改主题一般我们初始化博客的文件夹后，文件结构大概如下：12345678910111213$ lltotal 1352-rw-r--r-- 1 builder34 staff 32B 4 14 01:34 README.md-rw-r--r-- 1 builder34 staff 2.3K 6 25 10:40 _config.yml-rw-r--r-- 1 builder34 staff 32K 6 26 15:50 db.json-rw-r--r-- 1 builder34 staff 458K 6 26 15:56 debug.logdrwxr-xr-x 293 builder34 staff 9.2K 6 25 10:42 node_modules-rw-r--r-- 1 builder34 staff 110K 6 22 23:59 package-lock.json-rw-r--r-- 1 builder34 staff 564B 6 22 23:59 package.jsondrwxr-xr-x 14 builder34 staff 448B 6 25 10:40 publicdrwxr-xr-x 5 builder34 staff 160B 4 17 23:12 scaffoldsdrwxr-xr-x 3 builder34 staff 96B 6 25 10:57 sourcedrwxr-xr-x 6 builder34 staff 192B 6 25 11:33 themes themes文件夹是我们博客主题的存放地方，下面我推荐一个主题：BlueLake123456$ cd themes/$ git clone https://github.com/chaooo/hexo-theme-BlueLake.git ./BlueLake$ npm install hexo-renderer-jade@0.3.0 --save$ npm install hexo-renderer-stylus --save(该主题更细致的配置，请登录上面这个github网址，阅读README.md进行定制化配置） 在Hexo配置文件（$your_blog_path/_config.yml）中把主题设置修改为BlueLake。1theme: BlueLake 完成配置后，执行下面语句，重新打开http://localhost:4000 可以看到博客以一个新的主题展现了12$ hexo g$ hexo s --debug 1.5 hexo部署到Github配置$your_blog_path/_config.yml文件的Deployment：12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/your_githubName/your_githubName.github.io.git 通过下面的命名进行博客静态页面的生成，以及部署到远端Github Pages12345678#删除静态文件,即 public 文件$ hexo clean#生成静态文件,即 public 文件$ hexo generate#部署到远程站点$ hexo deploy#也可以使用组合命令(替代上面\b2条命令)：生成静态命令并部署到远程站点$ hexo deploy -g 12使用 hexo deploy 命名部署到github失败，报上面的错误时，安装下面的插件即可解决:$ npm install hexo-deployer-git --save 至此，Hexo+Github Pages构建个人博客网站已经\b基本完成了。可以通过网页访问自己的博客地址如\b：https://builder34.github.io 2.设置博客自定义域名进入自己博客的repository仓库，通过类似如下的页面进行设置：进入了settings页面后，往下拉直到看到Github Pages模块： 所填的自定义域名是需要自己已经在万网上注册的了，并且如果\b勾选了 Enforce HTTPS 的话，你的域名是需要ssl证书的哟。 3.注意事项3.1 上传README.md并防止被渲染成文章123456#在\b博客根目录下，新建或编辑你的README.md文件$ vim README.md$ mv README.md ./sources#修改_config.yml文件,设置不渲染的文件$ vim _config.ymlskip_render: README.md 3.2 每次\bhexo deploy后Github Pages自定义域名会被重置的问题需要在\bsources目录下新建CNAME文件(注意为全大写无后缀的文件哦),文件\b内容为你需要映射到的自定义域名：1234$ vim CNAMEblog.monbuilder.top$ mv CNAME ./sources 4.写文章12$ hexo new interview-java2018INFO Created: ~/Documents/workspace/builder/blog/builder34.github.io/source/_posts/interview-java2018.md 使用上面的命令创建文章，找到对应的md文件，使用你喜欢的markdown编辑器，尽情输出吧…下面两个命令是，生成静态页面，再部署，然后就可以在你的博客网站上看到你新发布的文章了，哈哈！12$ hexo g$ hexo d","categories":[{"name":"hexo博客","slug":"hexo博客","permalink":"https://blog.monbuilder.top/categories/hexo博客/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://blog.monbuilder.top/tags/hexo/"}],"keywords":[{"name":"hexo博客","slug":"hexo博客","permalink":"https://blog.monbuilder.top/categories/hexo博客/"}]}]}