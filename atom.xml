<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>抹布先生M</title>
  
  <subtitle>记录技术与生活,keep coding</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.monbuilder.top/"/>
  <updated>2019-03-14T03:07:38.915Z</updated>
  <id>https://blog.monbuilder.top/</id>
  
  <author>
    <name>Builder Luo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Elasticsearch学习教程系列(1)-命令学习(一) 集群健康、索引、文档操作</title>
    <link href="https://blog.monbuilder.top/2019/03/14/elasticsearch-learn01/"/>
    <id>https://blog.monbuilder.top/2019/03/14/elasticsearch-learn01/</id>
    <published>2019-03-14T04:15:22.000Z</published>
    <updated>2019-03-14T03:07:38.915Z</updated>
    
    <content type="html"><![CDATA[<p><em>本教程是基于Elasticsearch6.5版本编写</em><br>在本系列教程上一篇文章中，我们介绍了Elasticsearch的一些基本概念、安装并运行起了一个Elasticsearch节点。现在我们已经启动并运行了节点（集群），下一步是了解如何与它进行通信。幸运的是，Elasticsearch提供了一个非常全面和强大的REST API，您可以使用它与集群进行交互。使用API 可以完成的一些事项如下：</p><ul><li>检查集群运行情况，状态和统计信息</li><li>管理您的群集，节点和索引数据和元数据</li><li>对索引执行CRUD（创建，读取，更新和删除）和搜索操作</li><li>执行高级搜索操作，例如分页，排序，过滤，脚本编写，聚合等等</li></ul><h2 id="集群运行情况-Cluster-Health"><a href="#集群运行情况-Cluster-Health" class="headerlink" title="集群运行情况(Cluster Health)"></a>集群运行情况(Cluster Health)</h2><p>让我们从基本运行状况检查开始，我们可以使用它来查看集群的运行情况。我们将使用curl来执行此操作，但您可以使用任何允许您进行HTTP / REST调用的工具。假设我们仍然在我们启动Elasticsearch的同一节点上打开另一个命令shell窗口。<br>‘’[builder@master ~]$ curl -X GET “<a href="http://localhost:9200/_cat/health?v&quot;" target="_blank" rel="noopener">http://localhost:9200/_cat/health?v&quot;</a><br>‘’ epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent<br>‘’ 1547394013 15:40:13  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%<br>‘’<br>我们可以看到名为“elasticsearch”的群集处于green(绿色)状态。<br>每当我们要求群集健康时，我们要么获得绿色，黄色或红色。</p><blockquote><p>绿色 - 一切都很好（集群功能齐全）<br>黄色 - 所有数据均可用，但尚未分配一些副本（群集功能齐全）<br>红色 - 某些数据由于某种原因不可用（群集部分功能）<br><strong>注意：当群集为红色时，它将继续提供来自可用分片的搜索请求，但您可能需要尽快修复它，因为存在未分配的分片。</strong><br>同样从上面的响应中，我们可以看到总共1个节点，并且我们有0个分片，因为我们还没有数据。请注意，由于我们使用的是默认群集名称（elasticsearch），并且由于Elasticsearch默认使用单播网络发现来查找同一台计算机上的其他节点，因此您可能会意外启动计算机上的多个节点并将它们所有都加入一个集群。在这种情况下，您可能会在上面的响应中看到多个节点。<br>查看集群中的节点列表：<br>‘’[builder@master ]$ curl -X GET “<a href="http://localhost:9200/_cat/nodes?v&quot;" target="_blank" rel="noopener">http://localhost:9200/_cat/nodes?v&quot;</a><br>‘’ ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name<br>‘’ 127.0.0.1           20          98   8    1.90                  mdi       *      siwsSwZ<br>从上面的结果中，我们可以看到一个名为“siwsSwZ”的节点，它是我们集群中当前的单个节点。</p></blockquote><h2 id="索引-indices-的CRUD、查询操作"><a href="#索引-indices-的CRUD、查询操作" class="headerlink" title="索引(indices)的CRUD、查询操作"></a>索引(indices)的CRUD、查询操作</h2><p>查看索引列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$ curl -X GET <span class="string">"http://localhost:9200/_cat/indices?v"</span></span><br><span class="line">health status index uuid pri rep docs.count docs.deleted store.size pri.store.size</span><br></pre></td></tr></table></figure><p>因为我们的节点是刚刚建立的，还没数据，所以上面查询的结果只有一行字段名，没有索引数据</p><h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><p>现在让我们创建一个名为“customer”的索引，然后再次列出所有索引（下面第一个命令使用PUT动词创建名为“customer”的索引。加?pretty表示返回的结果打印格式化过的JSON（如果有的话）。）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$ curl -X PUT <span class="string">"localhost:9200/customer?pretty"</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="string">"acknowledged"</span> : <span class="literal">true</span>,</span><br><span class="line">   <span class="string">"shards_acknowledged"</span> : <span class="literal">true</span>,</span><br><span class="line">   <span class="string">"index"</span> : <span class="string">"customer"</span></span><br><span class="line"> &#125;</span><br><span class="line">[builder@master ~]$ curl -X GET <span class="string">"localhost:9200/_cat/indices?v"</span></span><br><span class="line">health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size</span><br><span class="line">yellow open   customer i7R-XRH8R0Kn7uLQRNC_yQ   5   1          0            0      1.1kb          1.1kb</span><br></pre></td></tr></table></figure><p>第二个命令的结果告诉我们，我们现在有一个名为customer的索引，它有5个主分片和1个副本（默认值），并且它包含0个文档。<br>您可能还注意到客户索引标记了黄色运行状况。回想一下我们之前的讨论，黄色表示某些副本尚未（尚未）分配。此索引发生这种情况的原因是因为默认情况下Elasticsearch为此索引创建了一个副本。由于我们目前只有一个节点在运行，因此在另一个节点加入集群的较晚时间点之前，尚无法分配一个副本（用于高可用性）。将该副本分配到第二个节点后，此索引的运行状况将变为绿色。</p><p><em>Tips: 上面命令中，-X PUT 后的路径不要带http哟，否则会出现如下错误的：</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$ curl -X PUT <span class="string">"http://localhost:9200/_cat/customer1?pretty"</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="string">"error"</span> : <span class="string">"Incorrect HTTP method for uri [/_cat/customer1?pretty] and method [PUT], allowed: [POST]"</span>,</span><br><span class="line">   <span class="string">"status"</span> : 405</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="查询文档"><a href="#查询文档" class="headerlink" title="查询文档"></a>查询文档</h3><p>现在让我们在customer索引中加入一些内容。我们将一个简单的客户文档索引到客户索引中，ID为1，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$  curl -X PUT  <span class="string">"localhost:9200/customer/_doc/1?pretty"</span> -H <span class="string">"Content-Type:application/json"</span> -d <span class="string">'&#123; "name": "Builder Luo"&#125;'</span> </span><br><span class="line"> &#123;</span><br><span class="line">   <span class="string">"_index"</span> : <span class="string">"customer"</span>,</span><br><span class="line">   <span class="string">"_type"</span> : <span class="string">"_doc"</span>,</span><br><span class="line">   <span class="string">"_id"</span> : <span class="string">"1"</span>,</span><br><span class="line">   <span class="string">"_version"</span> : 1,</span><br><span class="line">   <span class="string">"result"</span> : <span class="string">"created"</span>,</span><br><span class="line">   <span class="string">"_shards"</span> : &#123;</span><br><span class="line">     <span class="string">"total"</span> : 2,</span><br><span class="line">     <span class="string">"successful"</span> : 1,</span><br><span class="line">     <span class="string">"failed"</span> : 0</span><br><span class="line">   &#125;,</span><br><span class="line">   <span class="string">"_seq_no"</span> : 0,</span><br><span class="line">   <span class="string">"_primary_term"</span> : 2</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>curl命令说明：-X为请求方式，-H 参数为设置请求头，-d参数为请求参数 。<br>‘’ Tips:   关于：路径localhost:9200/customer/_doc/1?pretty<br>‘’ localhost:9200/customer是之前创建的索引，后接/_doc/1表示为在customer索引上创建1个ID为1的文档(?pretty表示将返回的响应json格式化美观)<br><strong>提示：值得注意的是，Elasticsearch在你将文档编入索引之前不需要先显式创建索引。在前面的示例中，如果customer索引事先尚未存在，则Elasticsearch将自动创建customer索引。</strong><br>查看刚刚创建好的文档:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$ curl -X GET <span class="string">"localhost:9200/customer/_doc/1?pretty"</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"_index"</span> : <span class="string">"customer"</span>,</span><br><span class="line">   <span class="string">"_type"</span> : <span class="string">"_doc"</span>,</span><br><span class="line">   <span class="string">"_id"</span> : <span class="string">"1"</span>,</span><br><span class="line">   <span class="string">"_version"</span> : 1,</span><br><span class="line">   <span class="string">"found"</span> : <span class="literal">true</span>,</span><br><span class="line">   <span class="string">"_source"</span> : &#123;</span><br><span class="line">     <span class="string">"name"</span> : <span class="string">"Builder Luo"</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$ curl -X DELETE <span class="string">"localhost:9200/customer?pretty"</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"acknowledged"</span> : <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">[builder@master ~]$  curl -X GET <span class="string">"localhost:9200/_cat/indices?v"</span></span><br><span class="line">health status index uuid pri rep docs.count docs.deleted store.size pri.store.size</span><br></pre></td></tr></table></figure><p>上面意味着索引已成功删除，我们现在回到我们在集群中没有任何内容的地方。<br>在我们继续之前，让我们再仔细看看到目前为止我们学到的一些API命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">curl -X PUT <span class="string">''</span>localhost:9200/customer<span class="string">''</span></span><br><span class="line">curl -X GET <span class="string">''</span>localhost:9200/customer<span class="string">''</span></span><br><span class="line">curl -X DELETE <span class="string">''</span>localhost:9200/customer<span class="string">''</span></span><br><span class="line"></span><br><span class="line">curl -X PUT <span class="string">''</span>localhost:9200/customer/_doc/1<span class="string">''</span>  -H <span class="string">"Content-Type:application/json"</span> -d <span class="string">'&#123; "name": "Builder Luo"&#125;'</span></span><br><span class="line">curl -X GET <span class="string">''</span>localhost:9200/customer/_doc/1<span class="string">''</span></span><br><span class="line">curl -X DELETE <span class="string">''</span>localhost:9200/customer/_doc/1<span class="string">''</span></span><br></pre></td></tr></table></figure><p>如果我们仔细研究上述命令，我们实际上可以看到我们如何在Elasticsearch中访问数据的模式。该模式可归纳如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;HTTP Verb&gt;        Node_address/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt;</span><br></pre></td></tr></table></figure><p>这种REST访问模式在所有API命令中都非常普遍，如果你能记住它，你将在掌握Elasticsearch方面有一个良好的开端。</p><h3 id="替换文档"><a href="#替换文档" class="headerlink" title="替换文档"></a>替换文档</h3><p>Elasticsearch几乎实时提供数据操作和搜索功能。默认情况下，从索引/更新/删除数据到搜索结果中显示的时间，您可能会有一秒钟的延迟（刷新间隔）。<br>数据在事务完成后立即可用，这是Elasticsearch与SQL等其他平台的重要区别<br>之前，我们执行过如下命令：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">''[builder@master <span class="symbol">~]$  curl -X PUT  "localhost</span>:<span class="number">9200</span>/customer/_doc/<span class="number">1</span>?pretty<span class="string">" -H "</span>Content-Type:application/json<span class="string">" -d '&#123; "</span>name<span class="string">": "</span>Builder Luo<span class="string">"&#125;' </span></span><br><span class="line">下面，我们执行一样的命令，文档ID还是指定为<span class="number">1</span>，只是-d参数内容有变化：</span><br><span class="line">''[builder@master <span class="symbol">~]$  curl -X PUT  "localhost</span>:<span class="number">9200</span>/customer/_doc/<span class="number">1</span>?pretty<span class="string">" -H "</span>Content-Type:application/json<span class="string">" -d '&#123; "</span>name<span class="string">": "</span>Baby Mon<span class="string">"&#125;' </span></span><br><span class="line">''&#123;</span><br><span class="line">''   <span class="string">"_index"</span> : <span class="string">"customer"</span>,</span><br><span class="line">''   <span class="string">"_type"</span> : <span class="string">"_doc"</span>,</span><br><span class="line">''   <span class="string">"_id"</span> : <span class="string">"1"</span>,</span><br><span class="line">''   <span class="string">"_version"</span> : <span class="number">2</span>,</span><br><span class="line">''   <span class="string">"result"</span> : <span class="string">"updated"</span>,</span><br><span class="line">''   <span class="string">"_shards"</span> : &#123;</span><br><span class="line">''     <span class="string">"total"</span> : <span class="number">2</span>,</span><br><span class="line">''     <span class="string">"successful"</span> : <span class="number">1</span>,</span><br><span class="line">''     <span class="string">"failed"</span> : <span class="number">0</span></span><br><span class="line">''   &#125;,</span><br><span class="line">''   <span class="string">"_seq_no"</span> : <span class="number">1</span>,</span><br><span class="line">''   <span class="string">"_primary_term"</span> : <span class="number">1</span></span><br><span class="line">'' &#125;</span><br></pre></td></tr></table></figure><p>以上内容将ID为1的文档名称从“Builder Luo”更改为“Baby Mon”, result字段值为: updated。另一方面，如果我们使用不同的ID，则会对新文档编制索引，并且索引中已有的现有文档保持不变。<br><em>索引时，ID部分是可选的。如果未指定，Elasticsearch将生成随机ID，然后使用它来索引文档。Elasticsearch生成的实际ID（或前面示例中显式指定的内容）将作为索引API调用的一部分返回。</em><br>此示例显示如何在没有显式ID的情况下索引文档：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$  curl -X POST  <span class="string">"localhost:9200/customer/_doc?pretty"</span> -H <span class="string">"Content-Type:application/json"</span> -d <span class="string">'&#123; "name": "Moonlight Chen"&#125;'</span></span><br><span class="line"> &#123;</span><br><span class="line">   <span class="string">"_index"</span> : <span class="string">"customer"</span>,</span><br><span class="line">   <span class="string">"_type"</span> : <span class="string">"_doc"</span>,</span><br><span class="line">   <span class="string">"_id"</span> : <span class="string">"UmGaVGgBol3Y-BIhbtd0"</span>,</span><br><span class="line">   <span class="string">"_version"</span> : 1,</span><br><span class="line">   <span class="string">"result"</span> : <span class="string">"created"</span>,</span><br><span class="line">   <span class="string">"_shards"</span> : &#123;</span><br><span class="line">     <span class="string">"total"</span> : 2,</span><br><span class="line">     <span class="string">"successful"</span> : 1,</span><br><span class="line">     <span class="string">"failed"</span> : 0</span><br><span class="line">   &#125;,</span><br><span class="line">   <span class="string">"_seq_no"</span> : 0,</span><br><span class="line">   <span class="string">"_primary_term"</span> : 1</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>可以看到上面自动生成的ID是：UmGaVGgBol3Y-BIhbtd0<br><strong>请注意，在上面的情况中，我们使用POST动词而不是PUT，因为我们没有指定ID。</strong></p><h3 id="修改文档数据"><a href="#修改文档数据" class="headerlink" title="修改文档数据"></a>修改文档数据</h3><p>除了能够重新索引和替换文档，我们还可以更新文档数据。（请注意，Elasticsearch实际上并没有在内部进行就地更新。每当我们进行更新时，Elasticsearch都会删除旧文档，然后一次性对应用了更新的新文档编制索引。）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -d参数，需要封装成： &#123; "doc": &#123;dataJSON&#125;&#125;</span></span><br><span class="line">[builder@master ~]$  curl -X POST  <span class="string">"localhost:9200/customer/_doc/1/_update?pretty"</span> -H <span class="string">"Content-Type:application/json"</span> -d <span class="string">'&#123; "doc": &#123; "name": "My Baby Mon",  "age": 20&#125; &#125;'</span> </span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"_index"</span> : <span class="string">"customer"</span>,</span><br><span class="line">    <span class="string">"_type"</span> : <span class="string">"_doc"</span>,</span><br><span class="line">    <span class="string">"_id"</span> : <span class="string">"1"</span>,</span><br><span class="line">    <span class="string">"_version"</span> : 4,</span><br><span class="line">    <span class="string">"result"</span> : <span class="string">"updated"</span>,</span><br><span class="line">    <span class="string">"_shards"</span> : &#123;</span><br><span class="line">        <span class="string">"total"</span> : 2,</span><br><span class="line">        <span class="string">"successful"</span> : 1,</span><br><span class="line">        <span class="string">"failed"</span> : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"_seq_no"</span> : 3,</span><br><span class="line">    <span class="string">"_primary_term"</span> : 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以使用简单脚本执行更新。此示例使用脚本将年龄增加5：</span></span><br><span class="line">[builder@master ~]$  curl -X POST  <span class="string">"localhost:9200/customer/_doc/1/_update?pretty"</span> -H <span class="string">"Content-Type:application/json"</span> -d <span class="string">'&#123; "script":  "ctx._source.age += 5" &#125;'</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"_index"</span> : <span class="string">"customer"</span>,</span><br><span class="line">   <span class="string">"_type"</span> : <span class="string">"_doc"</span>,</span><br><span class="line">   <span class="string">"_id"</span> : <span class="string">"1"</span>,</span><br><span class="line">   <span class="string">"_version"</span> : 5,</span><br><span class="line">   <span class="string">"result"</span> : <span class="string">"updated"</span>,</span><br><span class="line">   <span class="string">"_shards"</span> : &#123;</span><br><span class="line">     <span class="string">"total"</span> : 2,</span><br><span class="line">     <span class="string">"successful"</span> : 1,</span><br><span class="line">     <span class="string">"failed"</span> : 0</span><br><span class="line">   &#125;,</span><br><span class="line">   <span class="string">"_seq_no"</span> : 4,</span><br><span class="line">   <span class="string">"_primary_term"</span> : 1</span><br><span class="line"> &#125;</span><br><span class="line"><span class="comment"># 在上面的示例中，ctx._source指的是即将更新的当前源文档。</span></span><br></pre></td></tr></table></figure><p>Elasticsearch提供了在给定查询条件（如SQL UPDATE-WHERE语句）的情况下更新多个文档的功能。请参阅[docs-update-by-queryAPI[<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.5/docs-update-by-query.html]]" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.5/docs-update-by-query.html]]</a><br>本文到这里，我们主要介绍了Elasticsearch的索引、文档的一些主要命令，我们将在下一篇文章中介绍Elasticsearch的批处理命令、以及探索操作数据的命令</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;本教程是基于Elasticsearch6.5版本编写&lt;/em&gt;&lt;br&gt;在本系列教程上一篇文章中，我们介绍了Elasticsearch的一些基本概念、安装并运行起了一个Elasticsearch节点。现在我们已经启动并运行了节点（集群），下一步是了解如何与它进行通信。
      
    
    </summary>
    
      <category term="搜索引擎" scheme="https://blog.monbuilder.top/categories/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
    
      <category term="Elasticsearch" scheme="https://blog.monbuilder.top/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>搭建大数据平台系列(4)-hive环境搭建</title>
    <link href="https://blog.monbuilder.top/2019/03/06/hive/"/>
    <id>https://blog.monbuilder.top/2019/03/06/hive/</id>
    <published>2019-03-06T14:19:33.000Z</published>
    <updated>2019-03-06T14:53:01.208Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-准备步骤"><a href="#0-准备步骤" class="headerlink" title="0.准备步骤"></a>0.准备步骤</h2><p>Hive 是依赖在Hadoop上的，所以他的安装不需要像Hadoop或者spark那样每个节点都安装一遍，只需在Hadoop的master节点上安装一个即可。Hive的安装前，需要Hadoop的环境，以及Mysql。</p><h2 id="1-安装过程"><a href="#1-安装过程" class="headerlink" title="1.安装过程"></a>1.安装过程</h2><p>###1.1下载并解压安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">下载hive-1.1.0-cdh5.5.0.tar.gz到master机器的~/bigdataspacce文件夹下</span></span><br><span class="line"><span class="meta">#</span><span class="bash">解压安装包的命令：</span></span><br><span class="line">[hadoop@master ~]$ cd  ~/bigdataspacce</span><br><span class="line">[hadoop@master bigdataspace]$  tar -zxvf  hive-1.1.0-cdh5.5.0.tar.gz</span><br><span class="line"><span class="meta">#</span><span class="bash">解压完成后删除压缩包：</span></span><br><span class="line">[hadoop@master bigdataspace]$  rm  hive-1.1.0-cdh5.5.0.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置HIVE_HOME环境变量</span></span><br><span class="line">[hadoop@master ~]$  sudo  vi  /etc/profile</span><br><span class="line">(添加配置内容如下,红色为需要新增的配置)</span><br><span class="line">export HIVE_HOME=/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0</span><br><span class="line">export PATH=$JAVA_HOME/bin:$HBASE_HOME/bin:$HIVE_HOME/bin:$PATH</span><br><span class="line"><span class="meta">#</span><span class="bash">让环境变量生效</span></span><br><span class="line">[hadoop@master ~]$  source  /etc/profile</span><br></pre></td></tr></table></figure><p>###1.2修改hive-env.sh配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$  cd  /home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf</span><br><span class="line">[hadoop@master conf]$  cp hive-env.sh.template  hive-env.sh</span><br><span class="line">[hadoop@master conf]$  vi  hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">在hive-env.sh配置文件末尾加上:</span></span><br><span class="line">export HADOOP_HOME=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0</span><br><span class="line">export HIVE_CONF_DIR=/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/conf</span><br></pre></td></tr></table></figure><p>###1.3新建hive-site.xml配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master conf]$  vi  hive-env.sh</span><br><span class="line">##主要的配置内容如下：</span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml version=<span class="string">"1.0"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive-1.1.0-cdh5.5.0/hive-db/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive-1.1.0-cdh5.5.0/tmp/hive-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive-1.1.0-cdh5.5.0/tmp/$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive-1.1.0-cdh5.5.0/downloaded<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">Temporary local directory for added resources in the remote file system.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive-1.1.0-cdh5.5.0/queryLogs/$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Location of Hive run time structured log file<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">jdbc:mysql://slave1:3306/hive?useUnicode=true&amp;amp;characterEncoding=utf8</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-4添加mysql-connector的jar包到hive安装路径下的lib文件夹"><a href="#1-4添加mysql-connector的jar包到hive安装路径下的lib文件夹" class="headerlink" title="1.4添加mysql-connector的jar包到hive安装路径下的lib文件夹"></a>1.4添加mysql-connector的jar包到hive安装路径下的lib文件夹</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="variable">$HIVE_HOME</span>为前面hive安装的目录路径:/home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0</span></span><br><span class="line">[hadoop@master ~] mv mysql-connector-java-5.1.33.jar  $HIVE_HOME/lib</span><br></pre></td></tr></table></figure><h3 id="1-5启动元数据服务"><a href="#1-5启动元数据服务" class="headerlink" title="1.5启动元数据服务"></a>1.5启动元数据服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ cd  ~/bigdataspace/hive-1.1.0-cdh5.5.0</span><br><span class="line">[hadoop@master hive-1.1.0-cdh5.5.0]$ ./bin/hive --service metastore  &amp;</span><br></pre></td></tr></table></figure><h3 id="1-6启动-停止hive-CTL-命令行"><a href="#1-6启动-停止hive-CTL-命令行" class="headerlink" title="1.6启动/停止hive (CTL)命令行"></a>1.6启动/停止hive (CTL)命令行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">因为一开始配置了HIVE_HOME环境变量，可以直接在任何目录下执行hive命令了,进入hive控制台</span></span><br><span class="line">[hadoop@master bigdataspace]$  hive </span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/bigdataspace</span><br><span class="line">/hive-1.1.0-cdh5.5.0/lib/hive-common-1.1.0-cdh5.5.0.jar!/hive-log4j.properties</span><br><span class="line">WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure><p><em>上面报错了，解决Logging initialized using configuration in jar:file… （因为没log配置文件，直接从jar包查找）</em></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">  <span class="built_in">cd</span>  ~/bigdataspace/ /hive-1.1.0-cdh5.5.0/conf</span></span><br><span class="line"><span class="meta">$</span><span class="bash">  cp  beeline-log4j.properties.template  beeline-log4j.properties</span></span><br><span class="line"><span class="meta">$</span><span class="bash">  cp  hive-log4j.properties.template  hive-log4j.properties</span></span><br><span class="line"><span class="meta">$</span><span class="bash">  cp  hive-exec-log4j.properties.template  hive-exec-log4j.properties</span></span><br><span class="line">[hadoop@master bigdataspace]$  hive</span><br><span class="line">Logging initialized using configuration in file:/home/hadoop/bigdataspace/</span><br><span class="line">hive-1.1.0-cdh5.5.0/conf/hive-log4j.properties</span><br><span class="line">WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</span><br><span class="line">hive (default)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">  quit;     <span class="comment">#(退出hive，使用exit也可以)</span></span></span><br></pre></td></tr></table></figure><h3 id="1-7启动-停止beeline命令行（CTL）"><a href="#1-7启动-停止beeline命令行（CTL）" class="headerlink" title="1.7启动/停止beeline命令行（CTL）"></a>1.7启动/停止beeline命令行（CTL）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">启动：</span></span><br><span class="line">[hadoop@master bigdataspace]$  beeline</span><br><span class="line"><span class="meta">#</span><span class="bash">停止：</span></span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash">  !q</span></span><br></pre></td></tr></table></figure><h3 id="1-8HiveServer2的使用"><a href="#1-8HiveServer2的使用" class="headerlink" title="1.8HiveServer2的使用"></a>1.8HiveServer2的使用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ cd  ~/bigdataspace/hive-1.1.0-cdh5.5.0/bin/</span><br><span class="line">[hadoop@master bin]$  ./hiveserver2  &amp;   #后面的&amp;表示改命名在系统后台执行</span><br><span class="line">(如果执行上面命令让界面无法回到命令行，可以按ctrl+C回到命令行，这里&amp;会让hiverserver2在后台继续执行)</span><br><span class="line"><span class="meta">#</span><span class="bash">查看HiveServer2的进程情况(如果无则hiverserver2启动失败或停止了):</span></span><br><span class="line">[hadoop@master bin]$  ps  -ef |grep  HiveServer2  </span><br><span class="line">hadoop   25545 14762  3 17:02 pts/1    00:00:21 /home/hadoop/bigdataspace/jdk1.8.0_60/bin/java -Xmx256m -Djava.library.path=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0/lib/native/ -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx512m -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /home/hadoop/bigdataspace/hive-1.1.0-cdh5.5.0/lib/hive-service-1.1.0-cdh5.5.0.jar org.apache.hive.service.server.HiveServer2</span><br><span class="line">hadoop   26038 14762  0 17:14 pts/1    00:00:00 grep HiveServer2</span><br><span class="line"></span><br><span class="line">(“kill  -9  PID” 可以通过kill停止hiveserver2的后台服务) </span><br><span class="line"></span><br><span class="line">使用beeline连接hiveserver2测试：</span><br><span class="line">(</span><br><span class="line">jdbc:hive2：表示连接到hiveserver2</span><br><span class="line">master:表示hiveserver2安装的机器host/IP</span><br><span class="line">10001:表示hiveserver2设置的端口号(hive-site.xml中可设置)</span><br><span class="line">)</span><br><span class="line">[hadoop@master hive-1.1.0-cdh5.5.0]$ beeline -u jdbc:hive2://master:10001</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##这里可能会出现一些slf4j包有多个，引用异常，但是不是报错，如：</span></span></span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/bigdataspace/had…)</span><br><span class="line">Connecting to jdbc:hive2://master:10001</span><br><span class="line">Connected to: Apache Hive (version 1.1.0-cdh5.5.0)</span><br><span class="line">Driver: Hive JDBC (version 1.1.0-cdh5.5.0)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.1.0-cdh5.5.0 by Apache Hive</span><br><span class="line">0: jdbc:hive2://master:10001&gt;</span><br></pre></td></tr></table></figure><p>以上完成了Hive的基本安装配置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-准备步骤&quot;&gt;&lt;a href=&quot;#0-准备步骤&quot; class=&quot;headerlink&quot; title=&quot;0.准备步骤&quot;&gt;&lt;/a&gt;0.准备步骤&lt;/h2&gt;&lt;p&gt;Hive 是依赖在Hadoop上的，所以他的安装不需要像Hadoop或者spark那样每个节点都安装一遍，只
      
    
    </summary>
    
      <category term="大数据" scheme="https://blog.monbuilder.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://blog.monbuilder.top/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch学习教程系列(0)-入门与安装</title>
    <link href="https://blog.monbuilder.top/2019/01/10/elasticsearch-learn00/"/>
    <id>https://blog.monbuilder.top/2019/01/10/elasticsearch-learn00/</id>
    <published>2019-01-10T14:34:05.000Z</published>
    <updated>2019-03-06T14:23:56.929Z</updated>
    
    <content type="html"><![CDATA[<p><em>本教程是基于Elasticsearch6.5版本编写</em><br>下面将介绍、安装并启动Elasticsearch，查看其中的内容以及执行索引，搜索和修改数据等基本操作的过程。在本教程结束时，您应该很好地了解Elasticsearch是什么，它是如何工作的，并希望能够获得灵感，看看如何使用它来构建复杂的搜索应用程序或从数据中挖掘智能。</p><h2 id="0-1入门简介"><a href="#0-1入门简介" class="headerlink" title="0.1入门简介"></a>0.1入门简介</h2><p>Elasticsearch是一个高度可扩展的开源全文搜索和分析引擎。Elasticsearch基于Apache Lucene，它允许您快速，近实时地存储，搜索和分析大量数据。它通常用作与底层引擎/技术，提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口；为具有复杂搜索功能和要求的应用程序提供支持。<br>以下是Elasticsearch可用于的一些场景示例：</p><ul><li>您运行在线网上商店，允许您的客户搜索您销售的产品。在这种情况下，您可以使用Elasticsearch存储整个产品目录和库存，并为它们提供搜索和自动填充建议。</li><li>您希望收集日志或交易数据，并且希望分析和挖掘此数据以查找趋势，统计信息，摘要或异常。在这种情况下，您可以使用Logstash（Elasticsearch / Logstash / Kibana堆栈的一部分）来收集，聚合和解析数据，然后让Logstash将此数据提供给Elasticsearch。一旦数据在Elasticsearch中，您就可以运行搜索和聚合来挖掘您感兴趣的任何信息。</li><li>您运行价格警报平台，允许精通价格的客户指定一条规则，例如“我有兴趣购买特定的电子产品，如果小工具的价格在下个月内从任何供应商降至X美元以下，我希望收到通知” 。在这种情况下，您可以刮取供应商价格，将其推入Elasticsearch并使用其反向搜索（Percolator）功能来匹配价格变动与客户查询，并最终在发现匹配后将警报推送给客户。</li><li>您有分析/业务智能需求，并希望快速调查，分析，可视化并询问有关大量数据的特定问题（想想数百万或数十亿条记录）。在这种情况下，您可以使用Elasticsearch存储数据，然后使用Kibana（Elasticsearch / Logstash / Kibana堆栈的一部分）构建自定义仪表板，以便可视化对您来说重要的数据方面。此外，您可以使用Elasticsearch聚合功能针对您的数据执行复杂的商业智能查询。<h2 id="0-2基本概念"><a href="#0-2基本概念" class="headerlink" title="0.2基本概念"></a>0.2基本概念</h2><strong>有一些概念是Elasticsearch的核心。从一开始就理解这些概念将极大地帮助简化学习过程。</strong><br><em>1.近实时(NRT)</em><br>Elasticsearch是一个近实时搜索平台。这意味着从索引文档到可搜索文档的时间有一点延迟（通常是一秒）。<br><em>2.集群(cluster)</em><br>集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。确保不要在不同的环境中重用相同的群集名称，否则最终会导致节点加入错误的群集。例如，您可以使用logging-dev，logging-stage以及logging-prod 用于开发，登台和生产集群。请注意，如果群集中只有一个节点，那么它是完全正常的。此外，您还可以拥有多个独立的集群，每个集群都有自己唯一的集群名称。<br><em>3.节点(node)</em><br>节点是作为群集一部分的单个服务器，存储数据并参与群集的索引和搜索功能。就像集群一样，节点由名称标识，默认情况下，该名称是在启动时分配给节点的随机通用唯一标识符（UUID）。如果不需要默认值，可以定义所需的任何节点名称。此名称对于管理目的非常重要，您可以在其中识别网络中哪些服务器与Elasticsearch集群中的哪些节点相对应。可以将节点配置为按群集名称加入特定群集。默认情况下，每个节点都设置为加入一个名为cluster的集群elasticsearch，这意味着如果您在网络上启动了许多节点并且假设它们可以相互发现 - 它们将自动形成并加入一个名为的集群elasticsearch。在单个群集中，您可以拥有任意数量的节点。此外，如果您的网络上当前没有其他Elasticsearch节点正在运行，则默认情况下启动单个节点将形成一个名为的新单节点集群elasticsearch。<br><em>4.索引(index)</em><br>索引是具有某些类似特征的文档集合。例如，您可以拥有客户数据的索引，产品目录的另一个索引以及订单数据的另一个索引。索引由名称标识（必须全部小写），此名称用于在对其中的文档执行索引，搜索，更新和删除操作时引用索引。在单个群集中，您可以根据需要定义任意数量的索引。<br><em>5.文档(document)</em><br>文档是可以编制索引的基本信息单元。例如，您可以为单个客户提供文档，为单个产品提供另一个文档，为单个订单提供另一个文档。该文档以JSON（JavaScript Object Notation）表示，JSON是一种普遍存在的互联网数据交换格式。在索引/类型中，您可以根据需要存储任意数量的文档。请注意，尽管文档实际上驻留在索引中，但实际上必须将文档编入索引/分配给索引中的类型。<br><em>6.分片和副本(Shards &amp; Replicas)</em><br>索引可能存储大量可能超过单个节点的硬件限制的数据。例如，占用1TB磁盘空间的十亿个文档的单个索引可能不适合单个节点的磁盘，或者可能太慢而无法单独从单个节点提供搜索请求。为了解决这个问题，Elasticsearch提供了将索引细分为多个称为分片的功能。创建索引时，只需定义所需的分片数即可。每个分片本身都是一个功能齐全且独立的“索引”，可以托管在集群中的任何节点上。<br>分片很重要，主要有两个原因：</li><li>它允许您水平拆分/缩放内容量</li><li>它允许您跨分片（可能在多个节点上）分布和并行化操作，从而提高性能/吞吐量<br>分片的分布方式以及如何将其文档聚合回搜索请求的机制完全由Elasticsearch管理，对用户而言是透明的。在可以随时发生故障的网络/云环境中，非常有用，强烈建议使用故障转移机制，以防分片/节点以某种方式脱机或因任何原因消失。为此，Elasticsearch允许您将索引的分片的一个或多个副本制作成所谓的副本分片或简称副本。<br>复制很重要，主要有两个原因：</li><li>它在碎片/节点发生故障时提供高可用性。因此，请务必注意，副本分片永远不会在与从中复制的原始/主分片相同的节点上分配。</li><li>它允许您扩展搜索量/吞吐量，因为可以在所有副本上并行执行搜索。<br>总而言之，每个索引可以拆分为多个分片。索引也可以复制为零（表示没有副本）或更多次。复制后，每个索引都将具有主分片（从中复制的原始分片）和副本分片（主分片的副本）。可以在创建索引时为每个索引定义分片和副本的数量。创建索引后，您还可以随时动态更改副本数。您可以使用_shrink和_splitAPI 更改现有索引的分片数，但这不是一项简单的任务，并且预先计划正确数量的分片是最佳方法。默认情况下，Elasticsearch中的每个索引都分配了5个主分片和1个副本，这意味着如果群集中至少有两个节点，则索引将包含5个主分片和另外5个副本分片（1个完整副本），总计为每个索引10个分片。<br><code>Tips:  每个Elasticsearch分片都是Lucene索引。单个Lucene索引中可以包含最大数量的文档。截止LUCENE-5843，限制是2,147,483,519（= Integer.MAX_VALUE - 128）文档。您可以使用_cat/shardsAPI 监控分片大小。</code><h2 id="0-3安装"><a href="#0-3安装" class="headerlink" title="0.3安装"></a>0.3安装</h2>Elasticsearch是基于Java开发的，因此安装时，需要JDK环境。我们安装的版本是Elasticsearch6.5.4，至少需要JDK 8。我们假设您已经在您的Linux(例如CentOS6.5)环境下已经安装了JDK。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[builder@master ~]$ java -version</span><br><span class="line">java version "1.8.0_171"</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_171-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">下载安装包，并解压：</span></span><br><span class="line">[builder@master ~]$ cd ~</span><br><span class="line">[builder@master ~]$ mdkir env</span><br><span class="line">[builder@master ~]$ cd env</span><br><span class="line">[builder@master ~]$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.5.4.tar.gz</span><br><span class="line"><span class="meta">#</span><span class="bash">等待下载完成后，解压</span></span><br><span class="line">[builder@master ~]$ tar -zxvf elasticsearch-6.5.4.tar.gz</span><br><span class="line">[builder@master ~]$ cd elasticsearch-6.5.4</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在elasticsearch命令后加 -d 参数，是指以守护线程启动程序，即是后台运行</span></span><br><span class="line">[builder@master ~]$ ./bin/elasticsearch -d</span><br><span class="line"></span><br><span class="line">到这里Elasticsearch的安装已经完成了，是不是非常简单呢？</span><br><span class="line">另外，我们启动的时候是可以通过 -E 参数指定集群名称(cluster.name)或者节点名称(node.name)的：</span><br><span class="line">[builder@master ~]$ ./bin/elasticsearch -Ecluster.name=esCluster -Enode.name=builder01  -d</span><br></pre></td></tr></table></figure><p><code>Tips:   默认情况下，Elasticsearch使用port 9200来提供对其REST API的访问。如有必要，可以配置此端口。</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;本教程是基于Elasticsearch6.5版本编写&lt;/em&gt;&lt;br&gt;下面将介绍、安装并启动Elasticsearch，查看其中的内容以及执行索引，搜索和修改数据等基本操作的过程。在本教程结束时，您应该很好地了解Elasticsearch是什么，它是如何工作的，并希
      
    
    </summary>
    
      <category term="搜索引擎" scheme="https://blog.monbuilder.top/categories/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
    
      <category term="Elasticsearch" scheme="https://blog.monbuilder.top/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>搭建大数据平台系列(3)-hbase环境搭建</title>
    <link href="https://blog.monbuilder.top/2018/12/13/hbase/"/>
    <id>https://blog.monbuilder.top/2018/12/13/hbase/</id>
    <published>2018-12-13T09:21:59.000Z</published>
    <updated>2018-12-14T06:14:20.392Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-安装步骤"><a href="#1-安装步骤" class="headerlink" title="1.安装步骤"></a>1.安装步骤</h2><p>Hbase的安装需要在hadoop和zookeeper安装完成的基础上进行安装部署，所以，需要在安装hbase前准备好hadoop和zookeeper的环境（请看本系列前几篇文章）<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">`1.下载hbase-1.0.0-cdh5.5.0.tar.gz到master机器的/bigdataspacce文件夹下</span><br><span class="line">2.解压安装包的命令：</span><br><span class="line">[<span class="string">hadoop@master </span>](<span class="link"></span>)$ cd  /bigdataspacce</span><br><span class="line">[<span class="string">hadoop@master bigdataspace</span>](<span class="link"></span>)$  tar -zxvf hbase-1.0.0-cdh5.5.0.tar.gz</span><br><span class="line">3.解压完成后删除压缩包：</span><br><span class="line">[<span class="string">hadoop@master bigdataspace</span>](<span class="link"></span>)$  rm hbase-1.0.0-cdh5.5.0.tar.gz</span><br><span class="line">4.修改hbase-env.sh、hbase-site.xml配置文件以及regionservers文件（配置dataNode节点）</span><br><span class="line">$  cd  /home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0/conf</span><br><span class="line">$  vi  hbase-env.sh</span><br><span class="line"><span class="section"># The java implementation to use.  Java 1.7+ required.</span></span><br><span class="line"><span class="section"># export JAVA_HOME=/usr/java/jdk1.6.0/</span></span><br><span class="line">(在上面这条注释下加上：)</span><br><span class="line">export JAVA<span class="emphasis">_HOME=/home/hadoop/bigdataspace/jdk1.8.0_</span>60</span><br><span class="line">……</span><br><span class="line">export HBASE<span class="emphasis">_PID_</span>DIR=/data/hbase-1.0.0-cdh5.5.0/pids</span><br><span class="line"><span class="section"># export HBASE_MANAGES_ZK=true   #设置hbase是否管理zookeeper</span></span><br><span class="line">export HBASE<span class="emphasis">_MANAGES_</span>ZK=true</span><br><span class="line">export HBASE<span class="emphasis">_MANAGES_</span>ZK=false               #使用独立的ZooKeeper时需要修改HBASE<span class="emphasis">_MANAGES_</span>ZK值为false，为不使用默认自带的ZooKeeper实例。</span><br></pre></td></tr></table></figure></p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">`$  vi  hbase-site.xml</span><br><span class="line">(修改配置文件内容为如下)</span><br><span class="line"><span class="symbol">\&lt;</span>configuration<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>name<span class="symbol">\&gt;</span>hbase.rootdir<span class="symbol">\&lt;</span>/name<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>value<span class="symbol">\&gt;</span>hdfs://master:8020/hbase<span class="symbol">\&lt;</span>/value<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>/property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>name<span class="symbol">\&gt;</span>hbase.cluster.distributed<span class="symbol">\&lt;</span>/name<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>value<span class="symbol">\&gt;</span>true<span class="symbol">\&lt;</span>/value<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>/property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>name<span class="symbol">\&gt;</span>hbase.zookeeper.quorum<span class="symbol">\&lt;</span>/name<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>value<span class="symbol">\&gt;</span>slave1,slave2,slave3<span class="symbol">\&lt;</span>/value<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>/property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>name<span class="symbol">\&gt;</span>hbase.zookeeper.property.dataDir<span class="symbol">\&lt;</span>/name<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>value<span class="symbol">\&gt;</span>/data/zookeeper-3.4.5-cdh5.5.0/var/data<span class="symbol">\&lt;</span>/value<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>/property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>name<span class="symbol">\&gt;</span>hbase.tmp.dir<span class="symbol">\&lt;</span>/name<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>value<span class="symbol">\&gt;</span>/data/hbase-1.0.0-cdh5.5.0/tmp<span class="symbol">\&lt;</span>/value<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>/property<span class="symbol">\&gt;</span></span><br><span class="line"><span class="symbol">\&lt;</span>/configuration<span class="symbol">\&gt;</span></span><br><span class="line">(hdfs://master:8020/hbase,这里的hbase目录未建好的话是需要hdfs dfs –mkdir 新建的目录)</span><br><span class="line">$  vi  regionservers</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line">slave3</span><br><span class="line">(以上使用对应的ip配置也可以)</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p>`5.配置HBASE_HOME<br>$  vi  /etc/profile<br>(加上如下配置)<br>export HBASE_HOME=/home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0<br>export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HBASE_HOME/bin:$PATH<br>6.使用scp命令把hbase分发到各个节点<br>$  scp  -r  hbase-1.0.0-cdh5.5.0/ hadoop@slave1:/bigdataspace/<br>$  scp  -r  hbase-1.0.0-cdh5.5.0/ hadoop@slave2:/bigdataspace/<br>$  scp  -r  hbase-1.0.0-cdh5.5.0/ hadoop@slave3:/bigdataspace/</p><p>然后在各个节点上执行第5步：配置HBASE_HOME_<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`</span><br></pre></td></tr></table></figure></p><p>`7.Hbase的启动与停止<br>启动hbase时要确保hdfs已经启动，HBase的启动顺序为：HDFS->Zookeeper->HBase，启动Hbase的命令如下(在master机器上)：<br><a href="">hadoop@master </a>$  cd  /home/hadoop/bigdataspace/hbase-1.0.0-cdh5.5.0/bin<br>(注意，如果设置了hbase管理zookeeper，则需要先关闭手动启动的各节点zookeeper)<br>如slave1机器：<br><a href="">hadoop@slave1 </a>$  /bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/zkServer.sh  stop<br>在master机器：<br> <a href="">hadoop@master bin</a>$  ./start-hbase.sh</p><p><a href="">hadoop@master bin</a>$ jps<br>29385 HMaster<br>19994 JobHistoryServer<br>19068 NameNode<br>29757 Jps<br>19422 ResourceManager<br>19263 SecondaryNameNode<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`</span><br></pre></td></tr></table></figure></p><p>`如slave1机器：<br><a href="">hadoop@slave1 bin</a>$ jps<br>12768 DataNode<br>17971 HRegionServer<br>12884 NodeManager<br>22704 QuorumPeerMain<br>18169 Jps<br>17851 HQuorumPeer  #hbase管理zookeeper的进程, </p><h1 id="如果export-HBASE-MANAGES-ZK-true，才会出现上面的进程"><a href="#如果export-HBASE-MANAGES-ZK-true，才会出现上面的进程" class="headerlink" title="如果export HBASE_MANAGES_ZK=true，才会出现上面的进程"></a>如果export HBASE_MANAGES_ZK=true，才会出现上面的进程</h1><p>如果HQuorumPeer不存在，而是QuorumPeerMain则表明需要手动关闭zookeeper，hbase才能接手管理。<br>Hbase停止命令：<br><a href="">hadoop@master bin</a>$  ./stop-hbase.sh<br><code>`</code><br>`## 2.验证启动成功<br><em>访问HBase web 页面：<a href="http://master:60010/" target="_blank" rel="noopener">http://master:60010/</a></em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-安装步骤&quot;&gt;&lt;a href=&quot;#1-安装步骤&quot; class=&quot;headerlink&quot; title=&quot;1.安装步骤&quot;&gt;&lt;/a&gt;1.安装步骤&lt;/h2&gt;&lt;p&gt;Hbase的安装需要在hadoop和zookeeper安装完成的基础上进行安装部署，所以，需要在安装hbas
      
    
    </summary>
    
      <category term="大数据" scheme="https://blog.monbuilder.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://blog.monbuilder.top/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>搭建大数据平台系列(2)-zookeeper环境搭建</title>
    <link href="https://blog.monbuilder.top/2018/11/05/zookeeper/"/>
    <id>https://blog.monbuilder.top/2018/11/05/zookeeper/</id>
    <published>2018-11-05T08:08:10.000Z</published>
    <updated>2018-12-14T06:14:20.400Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-安装步骤"><a href="#1-安装步骤" class="headerlink" title="1.安装步骤"></a>1.安装步骤</h2><p>Zookeeper集群一般配置奇数个，在本次测试机是部署到slave1，slave2，slave3这3台机器上。<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>.下载zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>.tar.gz到slave1机器的~/bigdataspacce文件夹下</span><br><span class="line"><span class="number">2</span>.解压安装包的命令：</span><br><span class="line">[hadoop@slave1 bigdataspace]$  tar -zxvf zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>.tar.gz</span><br><span class="line"><span class="number">3</span>.解压完成后删除压缩包：</span><br><span class="line">[hadoop@slave1 bigdataspace]$  rm zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>.tar.gz</span><br><span class="line"><span class="number">4</span>.配置zoo.cfg文件</span><br><span class="line">     $  cd /home/hadoop/bigdataspace/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/conf</span><br><span class="line">     $  cp zoo_sample.cfg zoo.cfg</span><br><span class="line">     $  vi zoo.cfg</span><br><span class="line">        (修改文件中的dataDir配置)</span><br><span class="line">        dataDir=/data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/data</span><br><span class="line">        dataLogDir=/data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/dataLog/</span><br><span class="line">（并在clientPort下面新增如下配置）</span><br><span class="line">server<span class="meta">.1</span>=slave1:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="meta">.2</span>=slave2:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="meta">.3</span>=slave3:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line"><span class="number">5</span>.建立dataDir对应路径的文件夹，并进入该data文件夹下新建一个文件myid：</span><br><span class="line">    $ mkdir -p  /data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/dataLog</span><br><span class="line">$ mkdir -p  /data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/data</span><br><span class="line">    $ cd /data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/data</span><br><span class="line">    $ vi myid</span><br><span class="line">    (myid文件内容为zoo.cfg中配的server号码，如server<span class="meta">.1</span>则myid文件中只保存<span class="number">1</span>，每台机器都配自己对应的号码)</span><br><span class="line">    $ cat myid</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">    $</span><br><span class="line"><span class="number">6</span>.以上对zookeeper的配置基本完成，下面使用scp把zookeeper发到各个节点：</span><br><span class="line">$  scp -r zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/ hadoop@slave2:~/bigdataspace/</span><br><span class="line">$  scp -r zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/ hadoop@slave3:~/bigdataspace/</span><br><span class="line"><span class="number">7</span>.通过scp把myid传到各个节点，并修改其zoo.cfg配置文件对应的server号码</span><br><span class="line">(如server<span class="meta">.2</span>=slave1:<span class="number">52888</span>:<span class="number">53888</span>则myid文件存入<span class="number">2</span>)</span><br><span class="line">    $  scp -r /data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/ hadoop@slave2:/data/</span><br><span class="line">    $  scp -r /data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/ hadoop@slave3:/data/</span><br><span class="line">    (然后到到各个节点上修改/data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/data/myid文件),如：</span><br><span class="line">    [hadoop@slave2 ~]$ vi  /data/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/var/data/myid</span><br><span class="line">    <span class="number">2</span></span><br><span class="line">    [hadoop@slave2 ~]$</span><br><span class="line"><span class="number">8.</span>zookeeper.out以及log4j日志文件的设置</span><br><span class="line">[hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/conf</span><br><span class="line">[hadoop@slave1 conf]$ vi log4j.properties</span><br><span class="line"># Define some <span class="meta">default</span> values that can be overridden by system properties</span><br><span class="line">zookeeper.root.logger=INFO, ROLLINGFILE</span><br><span class="line">……</span><br><span class="line">log4j.appender.ROLLINGFILE=org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">查看zkServer.sh脚本，发现运行时会先加载zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/libexec/zkEnv.sh，不存在会加载zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/bin/zkEnv.sh</span><br><span class="line">[hadoop@slave1 ~]$ cd /home/hadoop/bigdataspace/zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/libexec</span><br><span class="line">[hadoop@slave1 libexec]$ vi  zkEnv.sh</span><br><span class="line">if [ <span class="string">"x$&#123;ZOO_LOG_DIR&#125;"</span> = <span class="string">"x"</span> ]</span><br><span class="line">then</span><br><span class="line">    ZOO_LOG_DIR=<span class="string">"/data/zookeeper-3.4.5-cdh5.5.0/logs"</span></span><br><span class="line">fi</span><br><span class="line">if [ <span class="string">"x$&#123;ZOO_LOG4J_PROP&#125;"</span> = <span class="string">"x"</span> ]</span><br><span class="line">then</span><br><span class="line">ZOO_LOG4J_PROP=<span class="string">"INFO,ROLLINGFILE"</span></span><br><span class="line">也可以把zookeeper-<span class="number">3.4</span><span class="meta">.5</span>-cdh5<span class="meta">.5</span><span class="meta">.0</span>/bin/zkEnv.sh文件的配置修改成上面一样.</span><br></pre></td></tr></table></figure></p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">9</span>.启动zookeeper服务：</span><br><span class="line">    [hadoop<span class="variable">@slave1</span> ~]<span class="variable">$ </span>cd /home/hadoop/bigdataspace/zookeeper-<span class="number">3.4</span>.<span class="number">5</span>-cdh5.<span class="number">5.0</span>/bin</span><br><span class="line">    [hadoop<span class="variable">@slave1</span> bin]<span class="variable">$ </span>./ zkServer.sh start   <span class="comment">#启动zookeeper（每台机器都要执行此命令）</span></span><br></pre></td></tr></table></figure><h2 id="2-验证"><a href="#2-验证" class="headerlink" title="2.验证"></a>2.验证</h2><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    [hadoop@slave1 bin]$ jps   #使用jps命令</span><br><span class="line">    <span class="number">25906</span> Jps</span><br><span class="line"><span class="number">20536</span> QuorumPeerMain   #zookeeper的进程</span><br><span class="line"><span class="number">19994</span> JobHistoryServer</span><br><span class="line"><span class="number">19068</span> NameNode</span><br><span class="line"><span class="number">19422</span> ResourceManager</span><br><span class="line"><span class="number">19263</span> SecondaryNameNode</span><br></pre></td></tr></table></figure><p>如上，含有QuorumPeerMain表明安装成功<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master bin]$ ./ zkServer.sh stop   #停止zookeeper（每台机器都要执行此命令）</span><br><span class="line">[hadoop@master bin]$ ./zkServer.sh status  #查看角色状态命令</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/<span class="built_in">..</span>/conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">（Mode: follower/leader，leader这个角色只有一台机器，是通过zookeeper的选举算法产生）</span><br><span class="line">如果出现如下错误，</span><br><span class="line">[hadoop@master bin]$ ./zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/bigdataspace/zookeeper-3.4.5-cdh5.5.0/bin/<span class="built_in">..</span>/conf/zoo.cfg</span><br><span class="line"><span class="builtin-name">Error</span> contacting service. It is probably <span class="keyword">not</span> running.</span><br><span class="line">极大可能是因为防火墙端口被限制了，可以打开这些被用到的端口</span><br><span class="line">(注意：只启用一台zookeeper也是会 出现这个错误，需要启动2台以上的节点)</span><br></pre></td></tr></table></figure></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#进入zookeeper客户端的命令</span></span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bin</span>]$  bin/zkCli.sh -server <span class="literal">master</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-安装步骤&quot;&gt;&lt;a href=&quot;#1-安装步骤&quot; class=&quot;headerlink&quot; title=&quot;1.安装步骤&quot;&gt;&lt;/a&gt;1.安装步骤&lt;/h2&gt;&lt;p&gt;Zookeeper集群一般配置奇数个，在本次测试机是部署到slave1，slave2，slave3这3台机器
      
    
    </summary>
    
      <category term="大数据" scheme="https://blog.monbuilder.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://blog.monbuilder.top/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>rabbitMQ的介绍与安装使用</title>
    <link href="https://blog.monbuilder.top/2018/10/24/rabbitmq/"/>
    <id>https://blog.monbuilder.top/2018/10/24/rabbitmq/</id>
    <published>2018-10-24T01:47:37.000Z</published>
    <updated>2018-12-14T06:14:20.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-消息中间件的介绍"><a href="#0-消息中间件的介绍" class="headerlink" title="0. 消息中间件的介绍"></a>0. 消息中间件的介绍</h2><p>RabbitMQ是一个“传统”消息代理，可以实现各种消息传递协议。它是首批实现合理级别功能，客户端库，开发工具和质量文档的开源消息代理之一。RabbitMQ最初是为实现AMQP而开发的，AMQP是一种开放式线路协议，具有强大的路由功能。虽然Java具有像JMS这样的消息传递标准，但它对于需要分布式消息传递的非Java应用程序没有帮助，因为它严重限制了任何集成场景，微服务或单片机。随着AMQP的出现，跨语言的灵活性成为开源消息代理的真实存在。<br>RabbitMQ被设计为通用消息代理，采用点对点，请求/回复和pub-sub通信样式模式的多种变体。它使用智能代理/哑消费者模型，专注于向消费者提供一致的消息传递，消费者的消费速度与经纪人跟踪消费者状态的速度大致相似。它是成熟的，在正确配置时表现良好，得到很好的支持（客户端库Java，.NET，node.js，Ruby，PHP和更多语言），并且有许多可用的插件可以将它扩展到更多的用例和集成场景。</p><h2 id="1-kafka-rabbitMQ的对比"><a href="#1-kafka-rabbitMQ的对比" class="headerlink" title="1.kafka,rabbitMQ的对比"></a>1.kafka,rabbitMQ的对比</h2><p>RabbitMQ中的通信可以根据需要同步或异步。发布者向交换发送消息，消费者从队列中检索消息。通过交换将生产者与队列分离可确保生产者不会受到硬编码路由决策的影响。RabbitMQ还提供了许多分布式部署方案（并且确实要求所有节点都能够解析主机名）。可以将多节点群集设置为群集联合，并且不依赖于外部服务（但某些群集形成插件可以使用AWS API，DNS，Consul等）。<br>Apache Kafka专为高容量发布 - 订阅消息和流而设计，旨在持久，快速和可扩展。从本质上讲，Kafka提供了一个持久的消息存储，类似于日志，在服务器集群中运行，它存储称为主题的类别中的记录流。</p><h2 id="2-rabbitMQ的安装与使用"><a href="#2-rabbitMQ的安装与使用" class="headerlink" title="2.rabbitMQ的安装与使用"></a>2.rabbitMQ的安装与使用</h2><h3 id="2-1在基于RPM的Linux上安装（RHEL，CentOS，Fedora，openSUSE），RabbitMQ-RPM包需要sudo权限-或使用root用户-才能安装和管理。"><a href="#2-1在基于RPM的Linux上安装（RHEL，CentOS，Fedora，openSUSE），RabbitMQ-RPM包需要sudo权限-或使用root用户-才能安装和管理。" class="headerlink" title="2.1在基于RPM的Linux上安装（RHEL，CentOS，Fedora，openSUSE），RabbitMQ RPM包需要sudo权限(或使用root用户)才能安装和管理。"></a>2.1在基于RPM的Linux上安装（RHEL，CentOS，Fedora，openSUSE），RabbitMQ RPM包需要sudo权限(或使用root用户)才能安装和管理。</h3><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">####1.安装Erlang</span></span><br><span class="line"></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># yum install erlang</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># yum update erlang</span></span><br><span class="line"></span><br><span class="line"><span class="meta">####2.安装RabbitMQ服务器</span></span><br><span class="line"> <span class="meta">##下载rabbitMQ rpm安装包</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.8/rabbitmq-server-3.7.8-1.el6.noarch.rpm</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># yum install rabbitmq-server-3.7.8-1.el6.noarch.rpm</span></span><br><span class="line"> <span class="meta">## 开机启动rabbitmq-server</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># chkconfig rabbitmq-server on</span></span><br><span class="line"> <span class="meta">#启动rabbitMQ服务</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># service rabbitmq-server start</span></span><br><span class="line"> <span class="meta">#停止rabbitMQ服务</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># service rabbitmq-server stop</span></span><br><span class="line"> <span class="meta">#重启rabbitMQ服务</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta"># service rabbitmq-server restart</span></span><br><span class="line"> <span class="meta">#修改配置文件</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  cd /etc/rabbitmq/</span></span><br><span class="line">[root<span class="symbol">@slave1</span> rabbitmq]<span class="meta">#  cp /usr/share/doc/rabbitmq-server-3.7.8/rabbitmq.config.example ./</span></span><br><span class="line">[root<span class="symbol">@slave1</span> rabbitmq]<span class="meta">#  mv rabbitmq.config.example rabbitmq.config</span></span><br><span class="line">[root<span class="symbol">@slave1</span> rabbitmq]<span class="meta">#  vim rabbitmq.config</span></span><br><span class="line">[</span><br><span class="line">&#123;rabbit, [&#123;tcp_listeners, [<span class="number">5672</span>]&#125;, &#123;loopback_users, [<span class="string">"test"</span>]&#125;]&#125;</span><br><span class="line">].</span><br><span class="line"></span><br><span class="line"> <span class="meta">#开启Web管理插件，这样我们就可以通过浏览器来进行管理了</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  rabbitmq-plugins enable rabbitmq_management</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  service rabbitmq-server restart</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  vim /etc/rabbitmq/rabbitmq.config</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  service rabbitmq-server restart</span></span><br><span class="line"> <span class="meta">##增加rabbitmq ui的登录用户</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  rabbitmqctl add_user test 123456</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  rabbitmqctl  set_user_tags  test  administrator</span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  rabbitmqctl set_permissions -p <span class="string">"/"</span> test <span class="string">".*"</span> <span class="string">".*"</span> <span class="string">".*"</span></span></span><br><span class="line">[root<span class="symbol">@slave1</span> ~]<span class="meta">#  rabbitmqctl list_users</span></span><br><span class="line">Listing users</span><br><span class="line">test[administrator]</span><br><span class="line">guest[administrator]</span><br></pre></td></tr></table></figure><h3 id="2-2-在macOS中安装rabbitMQ"><a href="#2-2-在macOS中安装rabbitMQ" class="headerlink" title="2.2 在macOS中安装rabbitMQ"></a>2.2 在macOS中安装rabbitMQ</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">使用homebrew安装rabbitMQ,若安装过homebrew可以忽略下面第一句命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> /usr/bin/ruby -e <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)</span>"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew install rabbitmq</span></span><br><span class="line"><span class="meta">#</span><span class="bash">启动命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew services start rabbitmq</span></span><br><span class="line"><span class="meta">#</span><span class="bash">重启命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew services restart rabbitmq</span></span><br><span class="line"><span class="meta">#</span><span class="bash">停止命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew services stop rabbitmq</span></span><br></pre></td></tr></table></figure><p>使用浏览器访问ip:15672(rabbitMQ的UI界面)，可使用上面设置的test用户密码进行登录</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-消息中间件的介绍&quot;&gt;&lt;a href=&quot;#0-消息中间件的介绍&quot; class=&quot;headerlink&quot; title=&quot;0. 消息中间件的介绍&quot;&gt;&lt;/a&gt;0. 消息中间件的介绍&lt;/h2&gt;&lt;p&gt;RabbitMQ是一个“传统”消息代理，可以实现各种消息传递协议。它是首
      
    
    </summary>
    
      <category term="消息中间件" scheme="https://blog.monbuilder.top/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="rabbitMQ 分布式" scheme="https://blog.monbuilder.top/tags/rabbitMQ-%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>activiti6-tutorial00</title>
    <link href="https://blog.monbuilder.top/2018/10/16/activiti6-tutorial00/"/>
    <id>https://blog.monbuilder.top/2018/10/16/activiti6-tutorial00/</id>
    <published>2018-10-16T03:02:23.000Z</published>
    <updated>2018-12-14T06:14:20.396Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>webSphere性能监控(JMX)</title>
    <link href="https://blog.monbuilder.top/2018/09/21/websphere-monitor/"/>
    <id>https://blog.monbuilder.top/2018/09/21/websphere-monitor/</id>
    <published>2018-09-21T08:12:32.000Z</published>
    <updated>2018-12-14T06:14:20.399Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-监控方式"><a href="#1-监控方式" class="headerlink" title="1.监控方式"></a>1.监控方式</h3><h4 id="1-1-部署自带perfServletApp监控项目"><a href="#1-1-部署自带perfServletApp监控项目" class="headerlink" title="1.1 部署自带perfServletApp监控项目"></a>1.1 部署自带perfServletApp监控项目</h4><p>以下步骤为支持JMX组件采集，websphere所需做的一些配置操作：<br>1&gt;访问websphere的web console页面，例如：<a href="https://localhost:9043/ibm/console/" target="_blank" rel="noopener">https://localhost:9043/ibm/console/</a><br>2&gt;点击左侧树形菜单：应用程序-&gt;应用程序类型-&gt;WebSphere企业应用程序；进入页面，查看有无安装了perfServletApp，若无再点击：安装 按钮<br><img src="/2018/09/21/websphere-monitor/img/perfServletApp_install.png" alt="perfServletApp_install"><br>3&gt;在新新页面中选择：本地路径；找到并上传perfServletApp.ear部署包(此包路径在$WEBSPHERE_INSTALL_HOME/installableApps/PerfServletApp.ear)，<br>可以copy到本地在上传<br><img src="/2018/09/21/websphere-monitor/img/choose_install_file.png" alt="choose_install_file"><br>4&gt;点击下一步，开始安装，一路点“下一步”，不用做任何修改填空，直到点击“完成”，最后保存配置<br><img src="/2018/09/21/websphere-monitor/img/install_success.png" alt="install_success"><br>5&gt;启动perfServletApp程序<br><img src="/2018/09/21/websphere-monitor/img/start_servlet.png" alt="start_servlet"><br>6&gt;配置perfServletApp访问的安全校验,点击：应用程序-&gt;perfServletApp,进入安全用户配置,按照下列步骤，最后确认并保存到主配置<br><img src="/2018/09/21/websphere-monitor/img/application_perm_open.png" alt="application_perm_open"></p><p><img src="/2018/09/21/websphere-monitor/img/servlet_access_perm.png" alt="servlet_access_perm"></p><p><img src="/2018/09/21/websphere-monitor/img/perm_user_mapping.png" alt="perm_user_mapping"></p><p><img src="/2018/09/21/websphere-monitor/img/perm_choose_user.png" alt="perm_choose_user"></p><p>7&gt;需要以上配置生效，需要重启webSphere服务</p><h4 id="1-2-配置新增ssl证书-通过soap协议访问mbean"><a href="#1-2-配置新增ssl证书-通过soap协议访问mbean" class="headerlink" title="1.2 配置新增ssl证书,通过soap协议访问mbean"></a>1.2 配置新增ssl证书,通过soap协议访问mbean</h4><p>以下步骤为支持JMX组件采集，websphere所需做的一些配置操作：</p><p><img src="/2018/09/21/websphere-monitor/img/ikeyman_path.png" alt="ikeyman_path"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_1.png" alt="keystore_create_1"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_2.png" alt="keystore_create_2"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_3.png" alt="keystore_create_3"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_4.png" alt="keystore_create_4"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_5.png" alt="keystore_create_5"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_6.png" alt="keystore_create_6"><br><img src="/2018/09/21/websphere-monitor/img/keystore_create_7.png" alt="keystore_create_7"><br><img src="/2018/09/21/websphere-monitor/img/truststore_create_1.png" alt="truststore_create_1"><br><img src="/2018/09/21/websphere-monitor/img/truststore_create_2.png" alt="truststore_create_2"><br><img src="/2018/09/21/websphere-monitor/img/truststore_create_3.png" alt="truststore_create_3"><br><img src="/2018/09/21/websphere-monitor/img/truststore_create_4.png" alt="truststore_create_4"><br><img src="/2018/09/21/websphere-monitor/img/add_ssl_store_1.png" alt="add_ssl_store_1"><br><img src="/2018/09/21/websphere-monitor/img/add_ssl_store_2.png" alt="add_ssl_store_2"><br><img src="/2018/09/21/websphere-monitor/img/add_ssl_store_3.png" alt="add_ssl_store_3"><br><img src="/2018/09/21/websphere-monitor/img/add_ssl_store_4.png" alt="add_ssl_store_4"><br><img src="/2018/09/21/websphere-monitor/img/add_ssl_store_5.png" alt="add_ssl_store_5"></p><p><img src="/2018/09/21/websphere-monitor/img/stopserver_new_ssl_error.png" alt="stopserver_new_ssl_error"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-监控方式&quot;&gt;&lt;a href=&quot;#1-监控方式&quot; class=&quot;headerlink&quot; title=&quot;1.监控方式&quot;&gt;&lt;/a&gt;1.监控方式&lt;/h3&gt;&lt;h4 id=&quot;1-1-部署自带perfServletApp监控项目&quot;&gt;&lt;a href=&quot;#1-1-部署自带per
      
    
    </summary>
    
      <category term="webSphere" scheme="https://blog.monbuilder.top/categories/webSphere/"/>
    
    
      <category term="性能监控" scheme="https://blog.monbuilder.top/tags/%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>2018版Java面试题汇总</title>
    <link href="https://blog.monbuilder.top/2018/08/16/interview-java2018/"/>
    <id>https://blog.monbuilder.top/2018/08/16/interview-java2018/</id>
    <published>2018-08-16T02:39:09.000Z</published>
    <updated>2018-12-14T06:14:20.397Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Java集合框架"><a href="#1-Java集合框架" class="headerlink" title="1.Java集合框架"></a>1.Java集合框架</h2><h3 id="1-1-Hashtable与Hashmap的区别"><a href="#1-1-Hashtable与Hashmap的区别" class="headerlink" title="1.1 Hashtable与Hashmap的区别"></a>1.1 Hashtable与Hashmap的区别</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">答：继承不同，hashtable继承自dictionary，hashmap继承自abstractmap。</span><br><span class="line">Hashtable: 生成一个新的，空的hashtable，使用默认的capacity容量为<span class="number">11</span>，factor增长因子为<span class="number">0.75</span>, 其实就是因为这个<span class="built_in">put</span>方法是synchronized的，所以可以保证其线程安全。</span><br><span class="line">Hashmap：允许<span class="literal">null</span> key/<span class="built_in">value</span>，线程不安全</span><br><span class="line">另外，ConcurrentHashMap所谓线程安全是如果没有哈希冲突使用compareAndSwapObject方式新增节点，如果哈希冲突的时候锁住哈希冲突的节点，这样新增的节点是线程安全的，而 ConcurrentHashMap又不像hashtable那样整个<span class="built_in">put</span>方法被锁定，所以性能比hashtable要好，因为这样不影响其他节点的插入和读取。（concurrenthashmap也不允许空键值，抛空指针异常，CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。）</span><br></pre></td></tr></table></figure><h3 id="1-2-arraylist-linkedlist-vector-区别"><a href="#1-2-arraylist-linkedlist-vector-区别" class="headerlink" title="1.2 arraylist linkedlist vector 区别"></a>1.2 arraylist linkedlist vector 区别</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">答：<span class="number">1.</span> 对ArrayList和LinkedList而言，在列表末尾增加一个元素所花的开销都是固定的。对ArrayList而言，主要是在内部数组中增加一项，指向所添加的元素，偶尔可能会导致对数组重新进行分配；而对LinkedList而言，这个开销是统一的，分配一个内部Entry对象。</span><br><span class="line">    <span class="number">2.</span> 在ArrayList的中间插入或删除一个元素意味着这个列表中剩余的元素都会被移动；而在LinkedList的中间插入或删除一个元素的开销是固定的。</span><br><span class="line">    <span class="number">3.</span> LinkedList不支持高效的随机元素访问。</span><br><span class="line">    <span class="number">4.</span> ArrayList的空间浪费主要体现在在<span class="type">list</span>列表的结尾预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗相当的空间</span><br><span class="line">可以这样说：当操作是在一列数据的后面添加数据而不是在前面或中间,并且需要随机地访问其中的元素时,使用ArrayList会提供比较好的性能；当你的操作是在一列数据的前面或中间添加或删除数据,并且按照顺序访问其中的元素时,就应该使用LinkedList了。</span><br><span class="line">    (遗留问题：在长度为<span class="number">10000</span>的链表和数组中分别随机插入一个元素，哪个效率更高？【<span class="number">58</span>到家面试题】答案是：数组。为啥？)</span><br></pre></td></tr></table></figure><h3 id="1-3-Java-集合部分都有哪些接口，主要体现了哪些设计模式？"><a href="#1-3-Java-集合部分都有哪些接口，主要体现了哪些设计模式？" class="headerlink" title="1.3 Java 集合部分都有哪些接口，主要体现了哪些设计模式？"></a>1.3 Java 集合部分都有哪些接口，主要体现了哪些设计模式？</h3><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">答：Java 集合部分主要有Collection、<span class="built_in">List</span>、<span class="built_in">Set</span>、<span class="built_in">Map</span>、Comparator、<span class="built_in">Iterator</span> 等，主要体现的设计模式是策略模式和迭代模式</span><br><span class="line">策略模式主要体现在每个接口有不同的实现，可以完成互换，如<span class="built_in">List</span> 接口下有ArrayList 和LinkedList,在不同的场景下可以互换。</span><br><span class="line">迭代模式主要体现在<span class="built_in">Iterator</span> 的实现，为不同的数据存储方式(数组、链表、散列表等)提供了统一的访问方式。</span><br><span class="line">Comparator 体现的设计模式是什么？ -- 策略模式，即不改变对象自身，而使用一个策略对象去改变它的行为。</span><br><span class="line">注：策略模式的优缺点是什么：</span><br><span class="line">优点：（<span class="number">1</span>）将具体算法逻辑与客户类分离，（<span class="number">2</span>）避免了大量的<span class="keyword">if</span> <span class="keyword">else</span> 判断</span><br><span class="line">缺点：（<span class="number">1</span>）每个算法一个类，产生了太多的类，（<span class="number">2</span>）客户端要知道所有的策略类，以便决定使用哪一个。</span><br></pre></td></tr></table></figure><h2 id="2-线程"><a href="#2-线程" class="headerlink" title="2.线程"></a>2.线程</h2><h3 id="2-1-线程五状态"><a href="#2-1-线程五状态" class="headerlink" title="2.1 线程五状态"></a>2.1 线程五状态</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">新建、就绪、执行、阻塞、死亡</span><br><span class="line">其中，阻塞状态(Blocked)</span><br><span class="line">线程运行过程中，可能由于各种原因进入阻塞状态:</span><br><span class="line">     <span class="number">1</span>&gt;线程通过调用sleep方法进入睡眠状态；</span><br><span class="line">     <span class="number">2</span>&gt;线程调用一个在I/O上被阻塞的操作，即该操作在输入输出操作完成之前不会返回到它的调用者；</span><br><span class="line">     <span class="number">3</span>&gt;线程试图得到一个锁，而该锁正被其他线程持有；</span><br><span class="line">     <span class="number">4</span>&gt;线程在等待某个触发条件；</span><br><span class="line">所谓阻塞状态是正在运行的线程没有运行结束，暂时让出CPU，这时其他处于就绪状态的线程就可以获得CPU时间，进入运行状态。</span><br><span class="line">    死亡状态(Dead)</span><br><span class="line">有两个原因会导致线程死亡：</span><br><span class="line">    <span class="number">1</span>) <span class="keyword">run</span><span class="bash">方法正常退出而自然死亡，</span></span><br><span class="line"><span class="bash">    2) 一个未捕获的异常终止了run方法而使线程猝死。</span></span><br></pre></td></tr></table></figure><h3 id="2-2-线程安全性的五种类别"><a href="#2-2-线程安全性的五种类别" class="headerlink" title="2.2 线程安全性的五种类别"></a>2.2 线程安全性的五种类别</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">答：①. 不可变 -- 不可变的对象一定是线程安全的，并且永远也不需要额外的同步。Java 类库中大多数基本数值类如 Integer 、 String 和 BigInteger 都是不可变的。</span><br><span class="line">    ②. 线程安全 -- 线程安全的对象，由类的规格说明所规定的约束在对象被多个线程访问时仍然有效，不管运行时环境如何排列，线程都不需要任何额外的同步。这种线程安全性保证是很严格的 -- 许多类，如 Hashtable 或者 Vector都不能满足这种严格的定义。</span><br><span class="line">    ③. 有条件的线程安全 -- 有条件的线程安全类对于单独的操作可以是线程安全的，但是某些操作序列可能需要外部同步。条件线程安全的最常见的例子是遍历由 Hashtable 或者 Vector 或者返回的迭代器。</span><br><span class="line">    ④. 线程兼容 -- 线程兼容类不是线程安全的，但是可以通过正确使用同步而在并发环境中安全地使用。这可能意味着用一个 synchronized 块包围每一个方法调用，或者创建一个包装器对象，其中每一个方法都是同步的(就像 Collections.synchronizedList() 一样)。许多常见的类是线程兼容的，如集合类 ArrayList 和 HashMap 、 java.text.SimpleDateFormat 、或者 JDBC 类<span class="built_in"> Connection </span>和 ResultSet。</span><br><span class="line">    ⑤. 线程对立 -- 线程对立类是那些不管是否调用了外部同步都不能在并发使用时安全地呈现的类。线程对立很少见，当类修改静态数据，而静态数据会影响在其他线程中执行的其他类的行为，这时通常会出现线程对立。线程对立类的一个例子是调用 System.setOut() 的类。</span><br></pre></td></tr></table></figure><h3 id="2-3-简单理解线程池技术"><a href="#2-3-简单理解线程池技术" class="headerlink" title="2.3 简单理解线程池技术"></a>2.3 简单理解线程池技术</h3><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">多线程技术主要解决处理器单元内多个线程执行的问题，它可以显著减少处理器单元的闲置时间，增加处理器单元的吞吐能力。</span><br><span class="line">    假设一个服务器完成一项任务所需时间为：T1 创建线程时间，T2 在线程中执行任务的时间，T3 销毁线程时间。</span><br><span class="line">    如果：T1 + T3 远大于 T2，则可以采用线程池，以提高服务器性能。</span><br><span class="line">    一个线程池包括以下四个基本组成部分：</span><br><span class="line">    <span class="number">1</span>、线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务；</span><br><span class="line">    <span class="number">2</span>、工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务；</span><br><span class="line">    <span class="number">3</span>、任务接口（<span class="keyword">Task</span>）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等；</span><br><span class="line">    <span class="number">4</span>、任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。</span><br><span class="line">    线程池技术正是关注如何缩短或调整T1,T3时间的技术，从而提高服务器程序性能的。它把T1，T3分别安排在服务器程序的启动和结束的时间段或者一些空闲的时间段，这样在服务器程序处理客户请求时，不会有T1，T3的开销了。</span><br><span class="line">    线程池不仅调整T1,T3产生的时间段，而且它还显著减少了创建线程的数目，看一个例子：</span><br><span class="line">    假设一个服务器一天要处理<span class="number">50000</span>个请求，并且每个请求需要一个单独的线程完成。在线程池中，线程数一般是固定的，所以产生线程总数不会超过线程池中线程的数目，而如果服务器不利用线程池来处理这些请求则线程总数为<span class="number">50000</span>。一般线程池大小是远小于<span class="number">50000</span>。所以利用线程池的服务器程序不会为了创建<span class="number">50000</span>而在处理请求时浪费时间，从而提高效率。</span><br><span class="line">代码实现中并没有实现任务接口，而是把Runnable对象加入到线程池管理器（ThreadPool），然后剩下的事情就由线程池管理器（ThreadPool）来完成了。</span><br><span class="line">Java通过Executors提供四种线程池，分别为：</span><br><span class="line">    newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。</span><br><span class="line">    newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。</span><br><span class="line">    newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。</span><br><span class="line">    newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。</span><br><span class="line"><span class="comment">// 执行任务,其实只是把任务加入任务队列，什么时候执行有线程池管理器觉定</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> execute(Runnable <span class="keyword">task</span>) &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (taskQueue) &#123;</span><br><span class="line">            taskQueue.add(<span class="keyword">task</span>);</span><br><span class="line">            taskQueue.notify();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 批量执行任务,其实只是把任务加入任务队列，什么时候执行有线程池管理器觉定</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> execute(Runnable[] <span class="keyword">task</span>) &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (taskQueue) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Runnable t : <span class="keyword">task</span>)</span><br><span class="line">                taskQueue.add(t);</span><br><span class="line">            taskQueue.notify();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 批量执行任务,其实只是把任务加入任务队列，什么时候执行有线程池管理器觉定</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> execute(List&lt;Runnable&gt; <span class="keyword">task</span>) &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (taskQueue) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Runnable t : <span class="keyword">task</span>)</span><br><span class="line">                taskQueue.add(t);</span><br><span class="line">            taskQueue.notify();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-说一下synchronized原理"><a href="#2-3-说一下synchronized原理" class="headerlink" title="2.3 说一下synchronized原理"></a>2.3 说一下synchronized原理</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">答：①.synchronized的字节码表示：</span><br><span class="line">      在java语言中存在两种内建的synchronized语法：1、synchronized语句；2、synchronized方法。对于synchronized语句当Java源代码被javac编译成bytecode的时候，会在同步块的入口位置和退出位置分别插入monitorenter和monitorexit字节码指令。而synchronized方法则会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的access_flags字段中的synchronized标志位置1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示Class做为锁对象。</span><br><span class="line">    ②.JVM中锁的优化：</span><br><span class="line">      简单来说在JVM中monitorenter和monitorexit字节码依赖于底层的操作系统的Mutex <span class="keyword">Lock</span>来实现的，但是由于使用<span class="keyword">Mutex</span> <span class="keyword">Lock</span>需要将当前线程挂起并从用户态切换到内核态来执行，这种切换的代价是非常昂贵的；然而在现实中的大部分情况下，同步方法是运行在单线程环境（无锁竞争环境）如果每次都调用<span class="keyword">Mutex</span> <span class="keyword">Lock</span>那么将严重的影响程序的性能。不过在jdk1<span class="number">.6</span>中对锁的实现引入了大量的优化，如锁粗化（<span class="keyword">Lock</span> Coarsening）、锁消除（<span class="keyword">Lock</span> Elimination）、轻量级锁（Lightweight Locking）、偏向锁（Biased Locking）、适应性自旋（Adaptive Spinning）等技术来减少锁操作的开销。</span><br><span class="line">    ③.在JVM中创建对象时会在对象前面加上两个字大小的对象头（<span class="keyword">Object</span> header），在<span class="number">32</span>位机器上一个字为<span class="number">32</span><span class="built_in">bit</span>，根据不同的状态位Mark World中存放不同的内容，在轻量级锁中，Mark Word被分成两部分，刚开始时LockWord为被设置为HashCode、最低三位表示LockWord所处的状态，初始状态为<span class="number">001</span>表示无锁状态。</span><br><span class="line">    ④.Monitor <span class="built_in">Record</span>是线程私有的数据结构，每一个线程都有一个可用monitor <span class="built_in">record</span>列表，同时还有一个全局的可用列表；那么这些monitor <span class="built_in">record</span>有什么用呢？每一个被锁住的对象都会和一个monitor <span class="built_in">record</span>关联（对象头中的LockWord指向monitor <span class="built_in">record</span>的起始地址，由于这个地址是<span class="number">8</span><span class="keyword">byte</span>对齐的所以LockWord的最低三位可以用来作为状态），同时monitor <span class="built_in">record</span>中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。</span><br><span class="line">    ⑤.获取锁（monitorenter）的大概过程如下：</span><br><span class="line">      当对象处于无锁状态时（RecordWord值为HashCode，状态位为<span class="number">001</span>），线程首先从自己的可用moniter <span class="built_in">record</span>列表中取得一个空闲的moniter <span class="built_in">record</span>，初始Nest和Owner值分别被预先设置为<span class="number">1</span>和该线程自己的标识，一旦monitor <span class="built_in">record</span>准备好然后我们通过CAS原子指令安装该monitor <span class="built_in">record</span>的起始地址到对象头的LockWord字段来膨胀.</span><br><span class="line">      对象已经被膨胀同时Owner中保存的线程标识为获取锁的线程自己，这就是重入（reentrant）锁的情况，只需要简单的将Nest加<span class="number">1</span>即可。不需要任何原子操作，效率非常高</span><br><span class="line">      对象已膨胀但Owner的值为<span class="literal">NULL</span>，当一个锁上存在阻塞或等待的线程同时锁的前一个拥有者刚释放锁时会出现这种状态，此时多个线程通过CAS原子指令在多线程竞争状态下试图将Owner设置为自己的标识来获得锁，竞争失败的线程在则会进入到第四种情况的执行路径。</span><br><span class="line">      对象处于膨胀状态同时Owner不为<span class="literal">NULL</span>(被锁住)，在调用操作系统的重量级的互斥锁之前先自旋一定的次数，当达到一定的次数时如果仍然没有成功获得锁，则开始准备进入阻塞状态，首先将rfThis的值原子性的加<span class="number">1</span>，由于在加<span class="number">1</span>的过程中可能会被其他线程破坏<span class="keyword">Object</span>和monitor <span class="built_in">record</span>之间的关联，所以在原子性加<span class="number">1</span>后需要再进行一次比较以确保LockWord的值没有被改变，当发现被改变后则要重新进行monitorenter过程。同时再一次观察Owner是否为<span class="literal">NULL</span>，如果是则调用CAS参与竞争锁，锁竞争失败则进入到阻塞状态。</span><br><span class="line">    ⑥.释放锁（monitorexit）的大概过程如下：</span><br><span class="line">      首先检查该对象是否处于膨胀状态并且该线程是这个锁的拥有者，如果发现不对则抛出异常；</span><br><span class="line">      检查Nest字段是否大于<span class="number">1</span>，如果大于<span class="number">1</span>则简单的将Nest减<span class="number">1</span>并继续拥有锁，如果等于<span class="number">1</span>，则进入到下一步；</span><br><span class="line">      检查rfThis是否大于<span class="number">0</span>，设置Owner为<span class="literal">NULL</span>然后唤醒一个正在阻塞或等待的线程再一次试图获取锁，如果等于<span class="number">0</span>则进入到下一步；</span><br><span class="line">      缩小（deflate）一个对象，通过将对象的LockWord置换回原来的HashCode值来解除和monitor <span class="built_in">record</span>之间的关联来释放锁，同时将monitor <span class="built_in">record</span>放回到线程是有的可用monitor <span class="built_in">record</span>列表。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Java集合框架&quot;&gt;&lt;a href=&quot;#1-Java集合框架&quot; class=&quot;headerlink&quot; title=&quot;1.Java集合框架&quot;&gt;&lt;/a&gt;1.Java集合框架&lt;/h2&gt;&lt;h3 id=&quot;1-1-Hashtable与Hashmap的区别&quot;&gt;&lt;a href
      
    
    </summary>
    
      <category term="面试" scheme="https://blog.monbuilder.top/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="Java面试" scheme="https://blog.monbuilder.top/tags/Java%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>搭建大数据平台系列(1)-Hadoop环境搭建[hdfs,yarn,mapreduce]</title>
    <link href="https://blog.monbuilder.top/2018/08/09/hadoop/"/>
    <id>https://blog.monbuilder.top/2018/08/09/hadoop/</id>
    <published>2018-08-09T15:00:08.000Z</published>
    <updated>2018-12-14T06:14:20.398Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-ssh免密码登录设置"><a href="#1-ssh免密码登录设置" class="headerlink" title="1.ssh免密码登录设置"></a>1.ssh免密码登录设置</h2><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ ssh -version</span><br><span class="line">OpenSSH_5<span class="number">.3</span>p1, OpenSSL <span class="number">1.0</span><span class="number">.1</span>e-fips <span class="number">11</span> Feb <span class="number">2013</span></span><br><span class="line">Bad escape character 'rsion'.</span><br></pre></td></tr></table></figure><p>查看ssh的版本后，如果ssh未安装则需要执行如下安装命令：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> sudo  yum  install openssh-server</span><br></pre></td></tr></table></figure></p><p>在每台机器上都执行一次下面的命令：<br><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$  ssh-keygen   –t   rsa     <span class="comment">#一路回车，提示要填的都默认不填，按回车</span></span><br><span class="line">上面执行完成后，每台机器上都会生成一个~/.ssh文件夹</span><br><span class="line">$  ll  ~/.ssh     <span class="comment">#查看.ssh文件下的文件列表</span></span><br><span class="line">-rw-------.<span class="number"> 1 </span>hadoop hadoop<span class="number"> 1580 </span>Apr<span class="number"> 18 </span>16:53 authorized_keys</span><br><span class="line">-rw-------.<span class="number"> 1 </span>hadoop hadoop<span class="number"> 1675 </span>Apr<span class="number"> 15 </span>16:01 id_rsa</span><br><span class="line">-rw-r--r--.<span class="number"> 1 </span>hadoop hadoop <span class="number"> 395 </span>Apr<span class="number"> 15 </span>16:01 id_rsa.pub</span><br></pre></td></tr></table></figure></p><p>把slave1，slave2，slave3上生成的公钥id_rsa.pub发给master机器：<br>在slave1机器上：<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@slave1</span> ~]<span class="variable">$ </span>scp  ~<span class="regexp">/.ssh/id</span>_rsa.pub  hadoop<span class="variable">@master</span><span class="symbol">:~/</span>.ssh/id_rsa.pub.slave1</span><br></pre></td></tr></table></figure></p><p>在slave2机器上：<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@slave2</span> ~]<span class="variable">$ </span>scp  ~<span class="regexp">/.ssh/id</span>_rsa.pub  hadoop<span class="variable">@master</span><span class="symbol">:~/</span>.ssh/id_rsa.pub.slave2</span><br></pre></td></tr></table></figure></p><p>在slave3机器上：<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@slave3</span> ~]<span class="variable">$ </span>scp  ~<span class="regexp">/.ssh/id</span>_rsa.pub  hadoop<span class="variable">@master</span><span class="symbol">:~/</span>.ssh/id_rsa.pub.slave3</span><br></pre></td></tr></table></figure></p><p>在master机器上，将所有公钥加到新增的用于认证的公钥文件authorized_keys中：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master  ~]$  cat  ~<span class="regexp">/.ssh/id</span>_rsa.pub*  <span class="meta">&gt;&gt;  </span>~<span class="regexp">/.ssh/authorized</span>_keys</span><br></pre></td></tr></table></figure></p><p>需要修改文件authorized_keys的权限（权限的设置非常重要，因为不安全的设置安全设置,会让你不能使用RSA功能 ）<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span>  <span class="title">~]$</span>  chmod  <span class="number">600</span>  ~/.ssh/authorized_keys  <span class="comment">#如果免密码不成功有可能缺少这步</span></span><br></pre></td></tr></table></figure></p><p>将公钥文件authorized_keys分发给每台slave:<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@master</span>  ~]<span class="variable">$ </span> scp  ~<span class="regexp">/.ssh/authorized</span>_keys   hadoop<span class="variable">@slave1</span><span class="symbol">:~/</span>.ssh/</span><br><span class="line">[hadoop<span class="variable">@master</span>  ~]<span class="variable">$ </span> scp  ~<span class="regexp">/.ssh/authorized</span>_keys   hadoop<span class="variable">@slave1</span><span class="symbol">:~/</span>.ssh/</span><br><span class="line">[hadoop<span class="variable">@master</span>  ~]<span class="variable">$ </span> scp  ~<span class="regexp">/.ssh/authorized</span>_keys   hadoop<span class="variable">@slave1</span><span class="symbol">:~/</span>.ssh/</span><br></pre></td></tr></table></figure></p><h2 id="2-Java环境的安装"><a href="#2-Java环境的安装" class="headerlink" title="2.Java环境的安装"></a>2.Java环境的安装</h2><pre><code>下载jdk-8u60-linux-x64.tar.gz安装包后(放在~/bigdataspace路径下)：</code></pre>  <figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> cd  ~/bigdataspace</span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$  tar  -zxvf  jdk-<span class="number">8</span>u60-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><p>修改环境变量配置文件:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master bigdataspace]$ sudo vi /etc/profile</span><br><span class="line"></span><br><span class="line">(在配置文件末尾加上如下配置)</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/home/hadoop/bigdataspace/jdk1.8.0_60</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$JAVA_HOME</span>/bin:$PATH</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">CLASSPATH</span>=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure></p><p>让环境变量设置生效：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$ source /etc/profile</span><br></pre></td></tr></table></figure></p><p>验证Java是否安装成功：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master <span class="keyword">bigdataspace]$ </span> <span class="keyword">java </span> -version</span><br><span class="line"><span class="keyword">java </span>version <span class="string">"1.8.0_60"</span></span><br><span class="line"><span class="keyword">Java(TM) </span>SE Runtime Environment (<span class="keyword">build </span><span class="number">1</span>.<span class="number">8</span>.<span class="number">0</span>_60-<span class="keyword">b27)</span></span><br><span class="line"><span class="keyword">Java </span>HotSpot(TM) <span class="number">64</span>-<span class="keyword">Bit </span>Server VM (<span class="keyword">build </span><span class="number">25</span>.<span class="number">60</span>-<span class="keyword">b23, </span>mixed mode)</span><br></pre></td></tr></table></figure></p><p>(每台机器上都需要按照上面的操作安装Java)<br>每台机器上执行：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> sudo chmod <span class="number">777</span> /data/  <span class="comment">#让所有用户可操作/data目录下的数据</span></span><br></pre></td></tr></table></figure></p><h2 id="3-集群上的机器实现同步时间"><a href="#3-集群上的机器实现同步时间" class="headerlink" title="3.集群上的机器实现同步时间"></a>3.集群上的机器实现同步时间</h2><p>检查时间服务是否安装:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ rpm -q ntp</span><br><span class="line">ntp-4.2.6p5-1.el6.centos.x86_64    #这表示已安装了，如果没有安装，这是空白</span><br></pre></td></tr></table></figure></p><p>如果没有安装，需要执行下面的安装命令：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> sudo yum install ntp</span><br></pre></td></tr></table></figure></p><p>需要配置NTP服务为自启动:<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@master</span> ~]<span class="variable">$ </span>sudo chkconfig ntpd on</span><br><span class="line">[hadoop<span class="variable">@master</span> ~]<span class="variable">$ </span>chkconfig --list ntpd</span><br><span class="line">ntpd      <span class="number">0</span><span class="symbol">:off</span>   <span class="number">1</span><span class="symbol">:off</span>   <span class="number">2</span><span class="symbol">:on</span>    <span class="number">3</span><span class="symbol">:on</span>    <span class="number">4</span><span class="symbol">:on</span>    <span class="number">5</span><span class="symbol">:on</span>    <span class="number">6</span><span class="symbol">:off</span></span><br><span class="line"></span><br><span class="line">(需要打开master机器上udp协议的<span class="number">123</span>端口是为了其他节点使用ntpdate通过该端口同步master机器的时间)</span><br><span class="line">[hadoop<span class="variable">@master</span> ~]<span class="variable">$ </span>sudo vi /etc/sysconfig/iptables</span><br><span class="line">(新增的端口配置)</span><br><span class="line">-A INPUT -m state --state NEW -m udp -p udp --dport <span class="number">123</span> -j ACCEPT</span><br><span class="line">[hadoop<span class="variable">@master</span> ~]<span class="variable">$ </span>sudo service iptables restart</span><br></pre></td></tr></table></figure></p><p>在配置前，先使用ntpdate手动同步下时间，免得本机与外部时间服务器时间差距太大，让ntpd不能正常同步。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ sudo ntpdate  pool.ntp.org</span><br><span class="line">26 Apr 17:12:15 ntpdate[7376]: <span class="keyword">step</span> time<span class="built_in"> server </span>202.112.29.82 offset 13.827386 sec</span><br></pre></td></tr></table></figure></p><p>更改master机器上的相关配置文件:<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> sudo  vim  /etc/ntp.conf</span><br></pre></td></tr></table></figure></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">(下面只显示修改的必要项)</span><br><span class="line"><span class="comment"># Hosts on local network are less restricted.</span></span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"><span class="comment">#让同一局域网ip段可以进行时间同步：</span></span><br><span class="line">restrict 10.3.19.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"><span class="comment"># Use public servers from the pool.ntp.org project.</span></span><br><span class="line"><span class="comment"># Please consider joining the pool (http://www.pool.ntp.org/join.html).</span></span><br><span class="line"><span class="comment">#server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#外部时间服务器</span></span><br><span class="line">server pool.ntp.org iburst</span><br><span class="line">server 0.asia.pool.ntp.org iburst</span><br><span class="line">server 1.asia.pool.ntp.org iburst</span><br><span class="line">server 1.asia.pool.ntp.org iburst</span><br><span class="line">server 2.asia.pool.ntp.org iburst</span><br><span class="line"><span class="comment">#broadcast 192.168.1.255 autokey        # broadcast server</span></span><br><span class="line"><span class="comment">#broadcastclient                        # broadcast client</span></span><br><span class="line"><span class="comment">#broadcast 224.0.1.1 autokey            # multicast server</span></span><br><span class="line"><span class="comment">#multicastclient 224.0.1.1              # multicast client</span></span><br><span class="line"><span class="comment">#manycastserver 239.255.254.254         # manycast server</span></span><br><span class="line"><span class="comment">#manycastclient 239.255.254.254 autokey # manycast client</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># allow update time by the upper server</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Undisciplined Local Clock. This is a fake driver intended for backup</span></span><br><span class="line"><span class="comment"># and when no outside source of synchronized time is available.</span></span><br><span class="line"><span class="comment"># 外部时间服务器不可用时，以本地时间作为时间服务</span></span><br><span class="line">server  127.127.1.0</span><br><span class="line">fudge   127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line"><span class="comment">#############################################################</span></span><br><span class="line">其他节点/etc/ntp.conf（slave1,slave2,slave3）的配置：</span><br><span class="line">……<span class="built_in">..</span></span><br><span class="line"><span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#外部时间服务器，以master时间为准进行同步</span></span><br><span class="line">server master  iburst</span><br><span class="line">……<span class="built_in">..</span></span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ sudo <span class="built_in"> service </span> ntpd  start</span><br><span class="line">(每台机器上都需要，设置ntpd开机启动，并第一次手动打开ntpd)，命令如下：</span><br><span class="line">$  sudo chkconfig ntpd on  #开机启动ntpd</span><br><span class="line">$  sudo<span class="built_in"> service </span>ntpd start  #启动 ntpd</span><br></pre></td></tr></table></figure><p>时间同步设置参考：<a href="http://cn.soulmachine.me/blog/20140124/" target="_blank" rel="noopener">http://cn.soulmachine.me/blog/20140124/</a></p><p>时间同步设置总结：<br>        每个节点上安装ntpd，并设置为开机启动，当然第一次要先手动启动，通过配置/etc/ntp.conf文件，让master作为时间同步服务器，这台机器的时间是根据联网同步网络时间的，其他节点以master的ip作为同步的地址</p><p>配置完成后，发现后面的节点时间可能还未同步，可能需要等30分钟左右，一段时间后时间都会以master为准，进行同步</p><h2 id="4-Hadoop的安装、配置"><a href="#4-Hadoop的安装、配置" class="headerlink" title="4.Hadoop的安装、配置"></a>4.Hadoop的安装、配置</h2><p>下载hadoop-2.6.0-cdh5.5.0.tar.gz安装包后(放在master机器上的~/bigdataspace路径下)：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> cd  ~/bigdataspace</span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$  tar  -zxvf  hadoop-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">5.0</span>.tar.gz</span><br></pre></td></tr></table></figure></p><p>进入hadoop配置文件路径:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master ~]$ cd  ~/bigdataspace/hadoop<span class="number">-2.6</span><span class="number">.0</span>-cdh5<span class="number">.5</span><span class="number">.0</span>/etc/hadoop</span><br></pre></td></tr></table></figure></p><h3 id="1-gt-在hadoop-env-sh中配置JAVA-HOME："><a href="#1-gt-在hadoop-env-sh中配置JAVA-HOME：" class="headerlink" title="1&gt;    在hadoop-env.sh中配置JAVA_HOME："></a>1&gt;    在hadoop-env.sh中配置JAVA_HOME：</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  hadoop-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">set</span> JAVA_HOME <span class="keyword">in</span> this <span class="keyword">file</span>, so that it <span class="keyword">is</span> correctly defined <span class="keyword">on</span></span><br><span class="line"># The java <span class="keyword">implementation</span> <span class="keyword">to</span> use.</span><br><span class="line"><span class="keyword">export</span> JAVA_HOME=/home/hadoop/bigdataspace/jdk1.<span class="number">8.0</span>_60</span><br></pre></td></tr></table></figure><h3 id="2-gt-在yarn-env-sh中配置JAVA-HOME："><a href="#2-gt-在yarn-env-sh中配置JAVA-HOME：" class="headerlink" title="2&gt;    在yarn-env.sh中配置JAVA_HOME："></a>2&gt;    在yarn-env.sh中配置JAVA_HOME：</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  yarn-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># some Java parameters</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/home/hadoop/bigdataspace/jdk1.8.0_60</span><br></pre></td></tr></table></figure><h3 id="3-gt-在slaves中配置slave节点的ip或者host"><a href="#3-gt-在slaves中配置slave节点的ip或者host" class="headerlink" title="3&gt;    在slaves中配置slave节点的ip或者host"></a>3&gt;    在slaves中配置slave节点的ip或者host</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  slaves</span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line">slave3</span><br></pre></td></tr></table></figure><h3 id="4-gt-修改core-site-xml"><a href="#4-gt-修改core-site-xml" class="headerlink" title="4&gt;    修改core-site.xml"></a>4&gt;    修改core-site.xml</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hadoop-2.6.0-cdh5.5.0/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5-gt-修改hdfs-site-xml"><a href="#5-gt-修改hdfs-site-xml" class="headerlink" title="5&gt;    修改hdfs-site.xml"></a>5&gt;    修改hdfs-site.xml</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/data/hadoop-2.6.0-cdh5.5.0/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/data/hadoop-2.6.0-cdh5.5.0/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="6-gt-修改mapred-site-xml"><a href="#6-gt-修改mapred-site-xml" class="headerlink" title="6&gt;    修改mapred-site.xml"></a>6&gt;    修改mapred-site.xml</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="7-gt-修改yarn-site-xml"><a href="#7-gt-修改yarn-site-xml" class="headerlink" title="7&gt;    修改yarn-site.xml"></a>7&gt;    修改yarn-site.xml</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop</span>]$ vi  yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><em>因为CDH版本缺少hadoop的native库，因此需要引入，否则会报错，解决方法：</em><br><a href="http://www.cnblogs.com/huaxiaoyao/p/5046374.html" target="_blank" rel="noopener">http://www.cnblogs.com/huaxiaoyao/p/5046374.html</a><br>本次安装具体采取的解决方法：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> cd ~/bigdataspace</span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$ wget  http://archive.cloudera.com/cdh5/redhat/<span class="number">6</span>/x86_64/cdh/<span class="number">5.5</span>.<span class="number">0</span>/RPMS/x86_64/hadoop-<span class="number">2.6</span>.<span class="number">0</span>+cdh5.<span class="number">5.0</span>+<span class="number">921</span>-<span class="number">1</span>.cdh5.<span class="number">5.0</span>.p0.<span class="number">15</span>.el6.x86_64.rpm</span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$  rpm2cpio *.rpm | cpio -div</span><br></pre></td></tr></table></figure></p><p>在bigdataspace文件夹下<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp -r ./usr/<span class="class"><span class="keyword">lib</span>/<span class="title">hadoop</span>/<span class="title">lib</span>/<span class="title">native</span>/  ~/<span class="title">bigdataspace</span>/<span class="title">hadoop</span>-2.6.0-<span class="title">cdh5</span>.5.0/<span class="title">lib</span>/<span class="title">native</span>/</span></span><br></pre></td></tr></table></figure></p><p>删除解压后得到的文件：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/etc/</span><br><span class="line">[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/usr/</span><br><span class="line">[hadoop@master bigdataspace]$ rm -r ~/bigdataspace/var<span class="comment">//</span></span><br><span class="line">$  rm  ~/ bigdataspace/hadoop<span class="number">-2.6</span><span class="number">.0</span>+cdh5<span class="number">.5</span><span class="number">.0</span>+<span class="number">921</span><span class="number">-1.</span>cdh5<span class="number">.5</span><span class="number">.0</span>.p0<span class="number">.15</span>.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></p><h2 id="5-使用scp命令分发配置好的hadoop到各个子节点"><a href="#5-使用scp命令分发配置好的hadoop到各个子节点" class="headerlink" title="5.使用scp命令分发配置好的hadoop到各个子节点"></a>5.使用scp命令分发配置好的hadoop到各个子节点</h2><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span> scp  –r  ~<span class="regexp">/bigdataspace/hadoop</span>-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">5.0</span>/  hadoop<span class="variable">@slave1</span><span class="symbol">:~/bigdataspace/</span></span><br><span class="line"><span class="variable">$ </span> scp  –r  ~<span class="regexp">/bigdataspace/hadoop</span>-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">5.0</span>/  hadoop<span class="variable">@slave2</span><span class="symbol">:~/bigdataspace/</span></span><br><span class="line"><span class="variable">$ </span> scp  –r  ~<span class="regexp">/bigdataspace/hadoop</span>-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">5.0</span>/  hadoop<span class="variable">@slave3</span><span class="symbol">:~/bigdataspace/</span></span><br></pre></td></tr></table></figure><p>(每台机器)修改环境变量配置文件:<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$ sudo vi /etc/profile</span><br></pre></td></tr></table></figure></p><p>(在配置文件末尾加上如下配置)<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_HOME</span>=/home/hadoop/bigdataspace/hadoop-2.6.0-cdh5.5.0</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$JAVA_HOME</span>/bin:$HADOOP_HOME/bin:$PATH</span><br></pre></td></tr></table></figure></p><p>让环境变量设置生效：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">bigdataspace</span>]$ source  /etc/profile</span><br></pre></td></tr></table></figure></p><h2 id="6-启动并验证Hadoop"><a href="#6-启动并验证Hadoop" class="headerlink" title="6.启动并验证Hadoop"></a>6.启动并验证Hadoop</h2><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">~]$</span> cd  ~/bigdataspace/hadoop-<span class="number">2.6</span>.<span class="number">0</span>-cdh5.<span class="number">5.0</span>   <span class="comment">#进入hadoop目录</span></span><br><span class="line">  [hadoop@<span class="keyword">master</span> <span class="title">hadoop-2</span>.<span class="number">6.0</span>-cdh5.<span class="number">5.0</span>]$ ./bin/hdfs namenode –format <span class="comment">#格式化namenode</span></span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop-2</span>.<span class="number">6.0</span>-cdh5.<span class="number">5.0</span>]$ ./sbin/<span class="literal">start</span>-dfs.sh     <span class="comment">#启动dfs</span></span><br><span class="line">[hadoop@<span class="keyword">master</span> <span class="title">hadoop-2</span>.<span class="number">6.0</span>-cdh5.<span class="number">5.0</span>]$ ./sbin/<span class="literal">start</span>-yarn.sh    <span class="comment">#启动yarn</span></span><br></pre></td></tr></table></figure><p>可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop<span class="number">-2.6</span><span class="number">.0</span>-cdh5<span class="number">.5</span><span class="number">.0</span>]$ jps</span><br><span class="line"><span class="number">3407</span> SecondaryNameNode</span><br><span class="line"><span class="number">3218</span> NameNode</span><br><span class="line"><span class="number">3552</span> ResourceManager</span><br><span class="line"><span class="number">3910</span> Jps</span><br></pre></td></tr></table></figure></p><p>在 slave1 上应该有以下几个进程<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 ~]$ jps</span><br><span class="line"><span class="number">2072</span> NodeManager</span><br><span class="line"><span class="number">2213</span> Jps</span><br><span class="line"><span class="number">1962</span> DataNode</span><br></pre></td></tr></table></figure></p><p><em>或者在浏览器中输入 <a href="http://master:8088" target="_blank" rel="noopener">http://master:8088</a> ，应该有 hadoop 的管理界面出来了,并通过<a href="http://master:8088/cluster/nodes能看到" target="_blank" rel="noopener">http://master:8088/cluster/nodes能看到</a> slave1、slave2、slave3节点</em></p><h2 id="7-启动Hadoop自带的jobhistoryserver"><a href="#7-启动Hadoop自带的jobhistoryserver" class="headerlink" title="7.启动Hadoop自带的jobhistoryserver"></a>7.启动Hadoop自带的jobhistoryserver</h2><p>[hadoop@master ~]$ cd  ~/bigdataspace/hadoop-2.6.0-cdh5.5.0   #进入hadoop目录<br>[hadoop@master hadoop-2.6.0-cdh5.5.0]$ sbin/mr-jobhistory-daemon.sh  start historyserver<br>(mapred-site.xml配置文件有对jobhistory的相关配置)<br>[hadoop@master hadoop-2.6.0-cdh5.5.0]$ jps<br>5314 Jps<br>19994 JobHistoryServer<br>19068 NameNode<br>19422 ResourceManager<br>19263 SecondaryNameNode</p><p>参考：<br><a href="http://blog.csdn.net/liubei_whut/article/details/42397985" target="_blank" rel="noopener">http://blog.csdn.net/liubei_whut/article/details/42397985</a></p><h2 id="8-停止hadoop集群的问题"><a href="#8-停止hadoop集群的问题" class="headerlink" title="8.停止hadoop集群的问题"></a>8.停止hadoop集群的问题</h2><p>Linux运行一段时间后，/tmp下的文件夹下面会清空一些文件，hadoop的停止脚本stop-all.sh是需要根据/tmp下面的pid文件关闭对应的进程，当/tmp下的文件被自动清理后可能会出出先的错误：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$   ./sbin/<span class="keyword">stop</span>-all.sh</span><br><span class="line">Stopping namenodes <span class="keyword">on</span> [<span class="keyword">master</span>]</span><br><span class="line"><span class="keyword">master</span>: <span class="keyword">no</span> namenode <span class="keyword">to</span> <span class="keyword">stop</span></span><br><span class="line">slave1: <span class="keyword">no</span> datanode <span class="keyword">to</span> <span class="keyword">stop</span></span><br><span class="line">slave2: <span class="keyword">no</span> datanode <span class="keyword">to</span> <span class="keyword">stop</span></span><br><span class="line">slave3: <span class="keyword">no</span> datanode <span class="keyword">to</span> <span class="keyword">stop</span></span><br><span class="line">Stopping secondary namenodes [<span class="keyword">master</span>]</span><br><span class="line"><span class="keyword">master</span>: <span class="keyword">no</span> secondarynamenode <span class="keyword">to</span> <span class="keyword">stop</span></span><br><span class="line">……</span><br></pre></td></tr></table></figure></p><p><em>方法1：这时需要在/tmp文件夹下手动创建恢复这些pid文件</em><br>master节点（每个文件中保存对应的进程id）：<br>hadoop-hadoop-namenode.pid<br>hadoop-hadoop-secondarynamenode.pid<br>yarn-hadoop-resourcemanager.pid<br>slave节点（每个文件中保存对应的进程id）：<br>hadoop-hadoop-datanode.pid<br>yarn-hadoop-nodemanager.pid<br><em>方法2：使用kill -9逐个关闭相应的进程id</em></p><p>从根本上解决的方法：<br>（首先使用了方法1或方法2关闭了hadoop集群）<br>1.修改配置文件hadoop-env.sh:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_PID_DIR</span>=/data/hadoop-2.6.0-cdh5.5.0/pids</span><br><span class="line"><span class="comment">#export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_SECURE_DN_PID_DIR</span>=/data/hadoop-2.6.0-cdh5.5.0/pids</span><br></pre></td></tr></table></figure></p><p>2.修改配置文件yarn-env.sh:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export YARN_PID_DIR=/data/hadoop<span class="number">-2.6</span><span class="number">.0</span>-cdh5<span class="number">.5</span><span class="number">.0</span>/pids</span><br></pre></td></tr></table></figure></p><p>3.创建文件夹pids：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  mkdir /data/hadoop<span class="number">-2.6</span><span class="number">.0</span>-cdh5<span class="number">.5</span><span class="number">.0</span>/pids（发现会自动创建pids文件，因此不需要创建）</span><br></pre></td></tr></table></figure></p><p>这2个步骤需要在各个节点都执行.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-ssh免密码登录设置&quot;&gt;&lt;a href=&quot;#1-ssh免密码登录设置&quot; class=&quot;headerlink&quot; title=&quot;1.ssh免密码登录设置&quot;&gt;&lt;/a&gt;1.ssh免密码登录设置&lt;/h2&gt;&lt;figure class=&quot;highlight lsl&quot;&gt;&lt;ta
      
    
    </summary>
    
      <category term="大数据" scheme="https://blog.monbuilder.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://blog.monbuilder.top/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>搭建大数据平台系列(0)-机器准备</title>
    <link href="https://blog.monbuilder.top/2018/07/20/conf-os/"/>
    <id>https://blog.monbuilder.top/2018/07/20/conf-os/</id>
    <published>2018-07-20T05:16:05.000Z</published>
    <updated>2018-12-14T06:14:20.376Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前期规划"><a href="#0-前期规划" class="headerlink" title="0. 前期规划"></a>0. 前期规划</h2><p>假设现在有四台机器，各自ip如下:<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.201</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.202</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.203</span></span><br><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.1</span><span class="selector-class">.204</span></span><br></pre></td></tr></table></figure></p><p>安装下面统一要求进行重装系统：</p><ol><li>每台机器的系统为：CentOS-6.5-x86_64</li><li><p>每台机器的磁盘分区为：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/boot      : 系统引导分区</span><br><span class="line">/          : 系统安装区</span><br><span class="line">/data       : 系统数据分区</span><br><span class="line">（swap分区尚未建立，待机器内存不足需求时再建立）</span><br></pre></td></tr></table></figure></li><li><p>每台机器上安装的都是Mininal Desktop版本</p></li><li>每台机器的主机名(hostname)分别为：master、slave1、slave2、slave3</li><li>每台机器的root密码都是:master</li><li>每台机器上建立的第一个非root用户都为:hadoop，密码为:hadoop</li><li>非root用户在centos中获取sudo权限的命令：<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave3 ~]$ su -   #切换到root用户，需要输入root密码</span><br><span class="line">[root@slave3 ~]# visudo -f /etc/sudoers</span><br><span class="line">## Allow root to run <span class="built_in">any</span> commands anywhere</span><br><span class="line">root    <span class="built_in">ALL</span>=(<span class="built_in">ALL</span>)       <span class="built_in">ALL</span></span><br><span class="line">在上面这条文字下加上如下：</span><br><span class="line">hadoop  <span class="built_in">ALL</span>=(<span class="built_in">ALL</span>)       <span class="built_in">ALL</span></span><br></pre></td></tr></table></figure></li></ol><p>这样配置后，hadoop用户可以使用sudo命令了</p><ol start="8"><li>修改每台机器的/etc/hosts文件，如：<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slave3 ~]$ sudo vi /etc/hosts</span><br><span class="line"><span class="number">192.168</span><span class="number">.1</span><span class="number">.201</span> master</span><br><span class="line"><span class="number">192.168</span><span class="number">.1</span><span class="number">.202</span> slave1</span><br><span class="line"><span class="number">192.168</span><span class="number">.1</span><span class="number">.203</span> slave2</span><br><span class="line"><span class="number">192.168</span><span class="number">.1</span><span class="number">.204</span> slave3</span><br></pre></td></tr></table></figure></li></ol><p>9.防火墙设置</p><ul><li>因为4台机器组成的大数据平台小集群需要互相访问对方的多个端口，需要在防火墙中打开这些端口的访问（如果未打开的话），当然最方便的就是把每台机器上的防火墙关闭了。<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master  ~]$  sudo <span class="built_in"> service </span> iptables  start     #打开防火墙</span><br><span class="line">[hadoop@master  ~]$  sudo <span class="built_in"> service </span> iptables  status    #查看防火墙</span><br><span class="line">[hadoop@master  ~]$  sudo <span class="built_in"> service </span> iptables  stop      #关闭防火墙</span><br><span class="line">(注意：防火墙操作时需要root权限的，非root用户不使用sudo的话，不会报错，但操作无结果)</span><br><span class="line">修改防火墙设置(红色为新增配置：开放2000-6000,7000以上的端口):</span><br><span class="line">[hadoop@master ~]$ sudo  vim  /etc/sysconfig/iptables</span><br><span class="line"><span class="comment"># Firewall configuration written by system-config-firewall</span></span><br><span class="line"><span class="comment"># Manual customization of this file is not recommended.</span></span><br><span class="line"><span class="number">*f</span>ilter</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line">-A INPUT -p icmp -j ACCEPT</span><br><span class="line">-A INPUT -i lo -j ACCEPT</span><br><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT</span><br><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 2000:6000 -j ACCEPT</span><br><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 7000: -j ACCEPT</span><br><span class="line">-A INPUT -j REJECT --reject-with icmp-host-prohibited</span><br><span class="line">-A FORWARD -j REJECT --reject-with icmp-host-prohibited</span><br><span class="line">COMMIT</span><br><span class="line"></span><br><span class="line">保存上面的文件后，需要重启防火墙，让配置生效！</span><br><span class="line">[hadoop@master  ~]$  sudo <span class="built_in"> service </span> iptables  restart     #重启防火墙</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-前期规划&quot;&gt;&lt;a href=&quot;#0-前期规划&quot; class=&quot;headerlink&quot; title=&quot;0. 前期规划&quot;&gt;&lt;/a&gt;0. 前期规划&lt;/h2&gt;&lt;p&gt;假设现在有四台机器，各自ip如下:&lt;br&gt;&lt;figure class=&quot;highlight css&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="https://blog.monbuilder.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="https://blog.monbuilder.top/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github Page搭建个人博客</title>
    <link href="https://blog.monbuilder.top/2018/06/22/build-blog/"/>
    <id>https://blog.monbuilder.top/2018/06/22/build-blog/</id>
    <published>2018-06-22T09:03:09.000Z</published>
    <updated>2018-12-14T06:14:20.390Z</updated>
    
    <content type="html"><![CDATA[<p>什么是Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。堪称在座各位喜欢Markdown的优雅人士博客建站神器哟！</p><h2 id="1-Quick-Start"><a href="#1-Quick-Start" class="headerlink" title="1. Quick Start"></a>1. Quick Start</h2><h3 id="1-1-创建存放Github-Pages的仓库"><a href="#1-1-创建存放Github-Pages的仓库" class="headerlink" title="1.1 创建存放Github Pages的仓库"></a>1.1 创建存放Github Pages的仓库</h3><p>Github Pages 是面向用户、组织和项目开放的公共静态页面搭建托管服务，站点可以被免费托管在 Github 上，你可以选择使用 Github Pages 提供的域名 github.io 或者自定义域名来发布站点。</p><p>需要Github账号，请登录<a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a> 注册。<br>登录了自己的github账号后，可以安装下图一样，创建自己的GitHub Pages仓库名[参考<a href="https://pages.github.com/" target="_blank" rel="noopener">https://pages.github.com/</a> ]，<code>[PS] 仓库名repository name需要约定为: 你的账号名.github.io</code><br><img src="/2018/06/22/build-blog/img/create_pages_project.png" alt="create_page"><br>创建好博客项目仓库后，可以通过git命名下载到本地，并编辑一下README.md从本地提交到GitHub，这样做主要是使本地文件与Github关联起来，方便后面hexo deploy,直接部署博客内容到GitHub进行更新。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/yourGithubName/yourGithubName.github.io</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim README.md</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> REAMME.md上可以简单写一些博客介绍啥的</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.email <span class="string">"you@example.com"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.name <span class="string">"Your Name"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git add ./</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">'test'</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git push -u origin master</span></span><br><span class="line">Username for 'https://github.com': Builder34</span><br><span class="line">Password for 'https://Builder34@github.com':</span><br></pre></td></tr></table></figure></p><h3 id="1-2-Hexo安装"><a href="#1-2-Hexo安装" class="headerlink" title="1.2 Hexo安装"></a>1.2 Hexo安装</h3><p>安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序：</p><ul><li>Node.js (请看<a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">https://nodejs.org/zh-cn/</a>)</li><li>Git （请看<a href="https://git-scm.com/downloads）" target="_blank" rel="noopener">https://git-scm.com/downloads）</a><br>安装好上面2个程序后，可以进行hexo的安装了：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> npm install -g hexo-cli</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="1-3-Hexo初始化"><a href="#1-3-Hexo初始化" class="headerlink" title="1.3 Hexo初始化"></a>1.3 Hexo初始化</h3><p>安装 Hexo 完成后，我们可以在本地新建一个文件夹如：builder34.github.io(这个目录是我们Github Pages博客项目的目录),假如我的文件夹路径为/home/test/builder34.github.io，建站初始化命令可以如下:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /home/<span class="built_in">test</span>/builder34.github.io</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo init ./</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm install</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo generate</span></span><br></pre></td></tr></table></figure></p><p>下面介绍几个常用的hexo命令(括号里面的命令为缩写形式，效果一样)：</p><pre><code>1. hexo generate(hexo g) #生成静态文件，会在当前目录下生成一个新的叫做public的文件夹2. hexo new &quot;postTitle&quot; #新建博客文章3. hexo new page &quot;pageTitle&quot; #新建1个页面4. hexo server(hexo s) #启动本地web服务预览(加参数--debug,用于调试，如：hexo s --debug)5. hexo deploy(hexo d) #部署播客到远端（比如Github,coding,heroku等平台）</code></pre><p>在命令行中输入<code>hexo s --debug</code>后，运行成功后，可以在浏览器中输入：<a href="http://localhost:4000看到自己新建的博客了。" target="_blank" rel="noopener">http://localhost:4000看到自己新建的博客了。</a></p><h3 id="1-4-更改主题"><a href="#1-4-更改主题" class="headerlink" title="1.4 更改主题"></a>1.4 更改主题</h3><p>一般我们初始化博客的文件夹后，文件结构大概如下：<br><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ ll</span><br><span class="line">total 1352</span><br><span class="line">-rw-r--r--   <span class="number"> 1 </span>builder34  staff    32B <span class="number"> 4 </span>14 01:34 README.md</span><br><span class="line">-rw-r--r--   <span class="number"> 1 </span>builder34  staff   2.3K <span class="number"> 6 </span>25 10:40 _config.yml</span><br><span class="line">-rw-r--r--   <span class="number"> 1 </span>builder34  staff    32K <span class="number"> 6 </span>26 15:50 db.json</span><br><span class="line">-rw-r--r--   <span class="number"> 1 </span>builder34  staff   458K <span class="number"> 6 </span>26 15:56 debug.log</span><br><span class="line">drwxr-xr-x <span class="number"> 293 </span>builder34  staff   9.2K <span class="number"> 6 </span>25 10:42 node_modules</span><br><span class="line">-rw-r--r--   <span class="number"> 1 </span>builder34  staff   110K <span class="number"> 6 </span>22 23:59 package-lock.json</span><br><span class="line">-rw-r--r--   <span class="number"> 1 </span>builder34  staff   564B <span class="number"> 6 </span>22 23:59 package.json</span><br><span class="line">drwxr-xr-x  <span class="number"> 14 </span>builder34  staff   448B <span class="number"> 6 </span>25 10:40 public</span><br><span class="line">drwxr-xr-x   <span class="number"> 5 </span>builder34  staff   160B <span class="number"> 4 </span>17 23:12 scaffolds</span><br><span class="line">drwxr-xr-x   <span class="number"> 3 </span>builder34  staff    96B <span class="number"> 6 </span>25 10:57 source</span><br><span class="line">drwxr-xr-x   <span class="number"> 6 </span>builder34  staff   192B <span class="number"> 6 </span>25 11:33 themes</span><br></pre></td></tr></table></figure></p><p>themes文件夹是我们博客主题的存放地方，下面我推荐一个主题：BlueLake<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> themes/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/chaooo/hexo-theme-BlueLake.git ./BlueLake</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm install hexo-renderer-jade@0.3.0 --save</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm install hexo-renderer-stylus --save</span></span><br><span class="line"></span><br><span class="line">(该主题更细致的配置，请登录上面这个github网址，阅读README.md进行定制化配置）</span><br></pre></td></tr></table></figure></p><p>在Hexo配置文件（$your_blog_path/_config.yml）中把主题设置修改为BlueLake。<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">theme:</span> BlueLake</span><br></pre></td></tr></table></figure></p><p>完成配置后，执行下面语句，重新打开<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a> 可以看到博客以一个新的主题展现了<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo g</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo s --debug</span></span><br></pre></td></tr></table></figure></p><h3 id="1-5-hexo部署到Github"><a href="#1-5-hexo部署到Github" class="headerlink" title="1.5 hexo部署到Github"></a>1.5 hexo部署到Github</h3><p>配置$your_blog_path/_config.yml文件的Deployment：<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># Deployment</span></span><br><span class="line"><span class="meta">## Docs: https:<span class="comment">//hexo.io/docs/deployment.html</span></span></span><br><span class="line"><span class="symbol">deploy:</span></span><br><span class="line"><span class="symbol">  type:</span> git</span><br><span class="line"><span class="symbol">  repo:</span> https:<span class="comment">//github.com/your_githubName/your_githubName.github.io.git</span></span><br></pre></td></tr></table></figure></p><p>通过下面的命名进行博客静态页面的生成，以及部署到远端Github Pages<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">删除静态文件,即 public 文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo clean</span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成静态文件,即 public 文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo generate</span></span><br><span class="line"><span class="meta">#</span><span class="bash">部署到远程站点</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo deploy</span></span><br><span class="line"><span class="meta">#</span><span class="bash">也可以使用组合命令(替代上面2条命令)：生成静态命令并部署到远程站点</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo deploy -g</span></span><br></pre></td></tr></table></figure></p><p><img src="/2018/06/22/build-blog/img/git_error.png" alt="git_error"></p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用 hexo <span class="keyword">deploy</span> 命名部署到github失败，报上面的错误时，安装下面的插件即可解决:</span><br><span class="line">$ npm install hexo-deployer-git <span class="params">--save</span></span><br></pre></td></tr></table></figure><p>至此，Hexo+Github Pages构建个人博客网站已经基本完成了。可以通过网页访问自己的博客地址如：<a href="https://builder34.github.io" target="_blank" rel="noopener">https://builder34.github.io</a></p><h2 id="2-设置博客自定义域名"><a href="#2-设置博客自定义域名" class="headerlink" title="2.设置博客自定义域名"></a>2.设置博客自定义域名</h2><p>进入自己博客的repository仓库，通过类似如下的页面进行设置：<br><img src="/2018/06/22/build-blog/img/github_pages_settings_btn.png" alt="pages_settings"><br>进入了settings页面后，往下拉直到看到Github Pages模块：<br><img src="/2018/06/22/build-blog/img/github_pages_settings.png" alt="pages_settings"></p><p>所填的自定义域名是需要自己已经在万网上注册的了，并且如果勾选了 Enforce HTTPS 的话，你的域名是需要ssl证书的哟。</p><h2 id="3-注意事项"><a href="#3-注意事项" class="headerlink" title="3.注意事项"></a>3.注意事项</h2><h3 id="3-1-上传README-md并防止被渲染成文章"><a href="#3-1-上传README-md并防止被渲染成文章" class="headerlink" title="3.1 上传README.md并防止被渲染成文章"></a>3.1 上传README.md并防止被渲染成文章</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">在博客根目录下，新建或编辑你的README.md文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim README.md</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv README.md ./sources</span></span><br><span class="line"><span class="meta">#</span><span class="bash">修改_config.yml文件,设置不渲染的文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim _config.yml</span></span><br><span class="line">skip_render: README.md</span><br></pre></td></tr></table></figure><h3 id="3-2-每次hexo-deploy后Github-Pages自定义域名会被重置的问题"><a href="#3-2-每次hexo-deploy后Github-Pages自定义域名会被重置的问题" class="headerlink" title="3.2 每次hexo deploy后Github Pages自定义域名会被重置的问题"></a>3.2 每次<code>hexo deploy</code>后Github Pages自定义域名会被重置的问题</h3><p>需要在sources目录下新建CNAME文件(注意为全大写无后缀的文件哦),文件内容为你需要映射到的自定义域名：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim CNAME</span></span><br><span class="line">blog.monbuilder.top</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv CNAME ./sources</span></span><br></pre></td></tr></table></figure></p><h2 id="4-写文章"><a href="#4-写文章" class="headerlink" title="4.写文章"></a>4.写文章</h2><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo <span class="keyword">new</span> interview-java2018</span><br><span class="line">INFO  Created: ~<span class="regexp">/Documents/</span>workspace<span class="regexp">/builder/</span>blog<span class="regexp">/builder34.github.io/</span><span class="keyword">source</span><span class="regexp">/_posts/i</span>nterview-java2018.md</span><br></pre></td></tr></table></figure><p>使用上面的命令创建文章，找到对应的md文件，使用你喜欢的markdown编辑器，尽情输出吧…<br>下面两个命令是，生成静态页面，再部署，然后就可以在你的博客网站上看到你新发布的文章了，哈哈！<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo g</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo d</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;什么是Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。堪称在座各位喜欢Markdown的优雅人士博客建站神器哟！&lt;/p&gt;
&lt;h2 id=&quot;1-Quick-Start&quot;
      
    
    </summary>
    
      <category term="hexo博客" scheme="https://blog.monbuilder.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://blog.monbuilder.top/tags/hexo/"/>
    
  </entry>
  
</feed>
